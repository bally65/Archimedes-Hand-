创建机械臂环境...
Using cuda device
开始训练...
提示: 按 Ctrl+C 可以中途停止训练并保存最后一次模型数据
Eval num_timesteps=5000, episode_reward=-662.84 +/- 151.76
Episode length: 435.20 +/- 154.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 435      |
|    mean_reward     | -663     |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
保存与最佳模型对应的VecNormalize参数
Eval num_timesteps=10000, episode_reward=-649.15 +/- 413.87
Episode length: 337.20 +/- 294.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 337      |
|    mean_reward     | -649     |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
New best mean reward!
保存与最佳模型对应的VecNormalize参数
Eval num_timesteps=15000, episode_reward=-1419.95 +/- 802.42
Episode length: 63.00 +/- 1.10
----------------------------------
| eval/              |           |
|    mean_ep_length  | 63        |
|    mean_reward     | -1.42e+03 |
| time/              |           |
|    total_timesteps | 15000     |
| train/             |           |
|    actor_loss      | 0.0281    |
|    critic_loss     | 0.0101    |
|    learning_rate   | 0.0003    |
|    n_updates       | 4999      |
----------------------------------
Eval num_timesteps=20000, episode_reward=-2269.69 +/- 1465.09
Episode length: 700.00 +/- 1150.02
----------------------------------
| eval/              |           |
|    mean_ep_length  | 700       |
|    mean_reward     | -2.27e+03 |
| time/              |           |
|    total_timesteps | 20000     |
| train/             |           |
|    actor_loss      | 0.105     |
|    critic_loss     | 0.00931   |
|    learning_rate   | 0.0003    |
|    n_updates       | 9999      |
----------------------------------
Eval num_timesteps=25000, episode_reward=-3408.69 +/- 1408.58
Episode length: 1329.00 +/- 1364.44
----------------------------------
| eval/              |           |
|    mean_ep_length  | 1.33e+03  |
|    mean_reward     | -3.41e+03 |
| time/              |           |
|    total_timesteps | 25000     |
| train/             |           |
|    actor_loss      | 0.0873    |
|    critic_loss     | 0.00344   |
|    learning_rate   | 0.0003    |
|    n_updates       | 14999     |
----------------------------------
Eval num_timesteps=30000, episode_reward=1062.55 +/- 277.01
Episode length: 3000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 3e+03    |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 30000    |
| train/             |          |
|    actor_loss      | 0.0529   |
|    critic_loss     | 0.00303  |
|    learning_rate   | 0.0003   |
|    n_updates       | 19999    |
---------------------------------
New best mean reward!
保存与最佳模型对应的VecNormalize参数
Eval num_timesteps=35000, episode_reward=1338.64 +/- 533.58
Episode length: 3000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 3e+03    |
|    mean_reward     | 1.34e+03 |
| time/              |          |
|    total_timesteps | 35000    |
| train/             |          |
|    actor_loss      | 0.0835   |
|    critic_loss     | 0.00146  |
|    learning_rate   | 0.0003   |
|    n_updates       | 24999    |
---------------------------------
New best mean reward!
保存与最佳模型对应的VecNormalize参数
Eval num_timesteps=40000, episode_reward=771.85 +/- 281.75
Episode length: 3000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 3e+03    |
|    mean_reward     | 772      |
| time/              |          |
|    total_timesteps | 40000    |
| train/             |          |
|    actor_loss      | 0.0899   |
|    critic_loss     | 0.00303  |
|    learning_rate   | 0.0003   |
|    n_updates       | 29999    |
---------------------------------
Eval num_timesteps=45000, episode_reward=8018.15 +/- 8623.60
Episode length: 2400.80 +/- 781.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2.4e+03  |
|    mean_reward     | 8.02e+03 |
| time/              |          |
|    total_timesteps | 45000    |
| train/             |          |
|    actor_loss      | 0.00288  |
|    critic_loss     | 0.0076   |
|    learning_rate   | 0.0003   |
|    n_updates       | 34999    |
---------------------------------
New best mean reward!
保存与最佳模型对应的VecNormalize参数
Eval num_timesteps=50000, episode_reward=7571.43 +/- 7859.95
Episode length: 2578.60 +/- 529.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2.58e+03 |
|    mean_reward     | 7.57e+03 |
| time/              |          |
|    total_timesteps | 50000    |
| train/             |          |
|    actor_loss      | 0.00501  |
|    critic_loss     | 0.00223  |
|    learning_rate   | 0.0003   |
|    n_updates       | 39999    |
---------------------------------
Eval num_timesteps=55000, episode_reward=2354.52 +/- 489.16
Episode length: 3000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 3e+03    |
|    mean_reward     | 2.35e+03 |
| time/              |          |
|    total_timesteps | 55000    |
| train/             |          |
|    actor_loss      | -0.135   |
|    critic_loss     | 0.0105   |
|    learning_rate   | 0.0003   |
|    n_updates       | 44999    |
---------------------------------
Eval num_timesteps=60000, episode_reward=1093.44 +/- 674.64
Episode length: 3000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 3e+03    |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 60000    |
| train/             |          |
|    actor_loss      | -0.145   |
|    critic_loss     | 0.00885  |
|    learning_rate   | 0.0003   |
|    n_updates       | 49999    |
---------------------------------
Eval num_timesteps=65000, episode_reward=-1469.52 +/- 779.95
Episode length: 210.80 +/- 345.66
----------------------------------
| eval/              |           |
|    mean_ep_length  | 211       |
|    mean_reward     | -1.47e+03 |
| time/              |           |
|    total_timesteps | 65000     |
| train/             |           |
|    actor_loss      | -0.137    |
|    critic_loss     | 0.0302    |
|    learning_rate   | 0.0003    |
|    n_updates       | 54999     |
----------------------------------
Eval num_timesteps=70000, episode_reward=-2832.43 +/- 1938.41
Episode length: 32.60 +/- 2.33
----------------------------------
| eval/              |           |
|    mean_ep_length  | 32.6      |
|    mean_reward     | -2.83e+03 |
| time/              |           |
|    total_timesteps | 70000     |
| train/             |           |
|    actor_loss      | -0.159    |
|    critic_loss     | 0.0172    |
|    learning_rate   | 0.0003    |
|    n_updates       | 59999     |
----------------------------------
Eval num_timesteps=75000, episode_reward=-2034.11 +/- 877.61
Episode length: 46.60 +/- 22.99
----------------------------------
| eval/              |           |
|    mean_ep_length  | 46.6      |
|    mean_reward     | -2.03e+03 |
| time/              |           |
|    total_timesteps | 75000     |
| train/             |           |
|    actor_loss      | -0.126    |
|    critic_loss     | 0.0174    |
|    learning_rate   | 0.0003    |
|    n_updates       | 64999     |
----------------------------------
Eval num_timesteps=80000, episode_reward=-758.47 +/- 1355.05
Episode length: 1585.60 +/- 1252.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.59e+03 |
|    mean_reward     | -758     |
| time/              |          |
|    total_timesteps | 80000    |
| train/             |          |
|    actor_loss      | -0.0789  |
|    critic_loss     | 0.0209   |
|    learning_rate   | 0.0003   |
|    n_updates       | 69999    |
---------------------------------
Eval num_timesteps=85000, episode_reward=-1696.66 +/- 1085.92
Episode length: 1248.80 +/- 1430.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.25e+03 |
|    mean_reward     | -1.7e+03 |
| time/              |          |
|    total_timesteps | 85000    |
| train/             |          |
|    actor_loss      | -0.031   |
|    critic_loss     | 0.0143   |
|    learning_rate   | 0.0003   |
|    n_updates       | 74999    |
---------------------------------
Eval num_timesteps=90000, episode_reward=-1530.91 +/- 990.78
Episode length: 650.80 +/- 1174.93
----------------------------------
| eval/              |           |
|    mean_ep_length  | 651       |
|    mean_reward     | -1.53e+03 |
| time/              |           |
|    total_timesteps | 90000     |
| train/             |           |
|    actor_loss      | -0.0465   |
|    critic_loss     | 0.0164    |
|    learning_rate   | 0.0003    |
|    n_updates       | 79999     |
----------------------------------
Eval num_timesteps=95000, episode_reward=287.70 +/- 589.67
Episode length: 3000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 3e+03    |
|    mean_reward     | 288      |
| time/              |          |
|    total_timesteps | 95000    |
| train/             |          |
|    actor_loss      | 0.0624   |
|    critic_loss     | 0.0241   |
|    learning_rate   | 0.0003   |
|    n_updates       | 84999    |
---------------------------------
Eval num_timesteps=100000, episode_reward=-935.48 +/- 1049.53
Episode length: 2659.80 +/- 680.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2.66e+03 |
|    mean_reward     | -935     |
| time/              |          |
|    total_timesteps | 100000   |
| train/             |          |
|    actor_loss      | 0.0232   |
|    critic_loss     | 0.0246   |
|    learning_rate   | 0.0003   |
|    n_updates       | 89999    |
---------------------------------
Eval num_timesteps=105000, episode_reward=4328.17 +/- 8249.88
Episode length: 1568.00 +/- 1178.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.57e+03 |
|    mean_reward     | 4.33e+03 |
| time/              |          |
|    total_timesteps | 105000   |
| train/             |          |
|    actor_loss      | 0.0682   |
|    critic_loss     | 0.0161   |
|    learning_rate   | 0.0003   |
|    n_updates       | 94999    |
---------------------------------
Eval num_timesteps=110000, episode_reward=-741.54 +/- 2581.40
Episode length: 2048.00 +/- 1166.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2.05e+03 |
|    mean_reward     | -742     |
| time/              |          |
|    total_timesteps | 110000   |
| train/             |          |
|    actor_loss      | 0.0747   |
|    critic_loss     | 0.0161   |
|    learning_rate   | 0.0003   |
|    n_updates       | 99999    |
---------------------------------
Eval num_timesteps=115000, episode_reward=-647.78 +/- 2504.19
Episode length: 2093.00 +/- 1112.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2.09e+03 |
|    mean_reward     | -648     |
| time/              |          |
|    total_timesteps | 115000   |
| train/             |          |
|    actor_loss      | 0.122    |
|    critic_loss     | 0.0217   |
|    learning_rate   | 0.0003   |
|    n_updates       | 104999   |
---------------------------------
Eval num_timesteps=120000, episode_reward=-2487.71 +/- 2279.00
Episode length: 1654.80 +/- 678.20
----------------------------------
| eval/              |           |
|    mean_ep_length  | 1.65e+03  |
|    mean_reward     | -2.49e+03 |
| time/              |           |
|    total_timesteps | 120000    |
| train/             |           |
|    actor_loss      | -0.0475   |
|    critic_loss     | 0.021     |
|    learning_rate   | 0.0003    |
|    n_updates       | 109999    |
----------------------------------
Eval num_timesteps=125000, episode_reward=-1867.91 +/- 2503.71
Episode length: 1615.80 +/- 871.37
----------------------------------
| eval/              |           |
|    mean_ep_length  | 1.62e+03  |
|    mean_reward     | -1.87e+03 |
| time/              |           |
|    total_timesteps | 125000    |
| train/             |           |
|    actor_loss      | -0.145    |
|    critic_loss     | 0.0172    |
|    learning_rate   | 0.0003    |
|    n_updates       | 114999    |
----------------------------------
Eval num_timesteps=130000, episode_reward=-2244.28 +/- 996.73
Episode length: 803.60 +/- 296.85
----------------------------------
| eval/              |           |
|    mean_ep_length  | 804       |
|    mean_reward     | -2.24e+03 |
| time/              |           |
|    total_timesteps | 130000    |
| train/             |           |
|    actor_loss      | -0.0595   |
|    critic_loss     | 0.0389    |
|    learning_rate   | 0.0003    |
|    n_updates       | 119999    |
----------------------------------
Eval num_timesteps=135000, episode_reward=-3953.18 +/- 1670.38
Episode length: 2075.60 +/- 844.39
----------------------------------
| eval/              |           |
|    mean_ep_length  | 2.08e+03  |
|    mean_reward     | -3.95e+03 |
| time/              |           |
|    total_timesteps | 135000    |
| train/             |           |
|    actor_loss      | 0.0313    |
|    critic_loss     | 0.0183    |
|    learning_rate   | 0.0003    |
|    n_updates       | 124999    |
----------------------------------
Eval num_timesteps=140000, episode_reward=-1000.15 +/- 471.16
Episode length: 746.40 +/- 882.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 746      |
|    mean_reward     | -1e+03   |
| time/              |          |
|    total_timesteps | 140000   |
| train/             |          |
|    actor_loss      | -0.0563  |
|    critic_loss     | 0.0281   |
|    learning_rate   | 0.0003   |
|    n_updates       | 129999   |
---------------------------------
Eval num_timesteps=145000, episode_reward=-708.25 +/- 776.73
Episode length: 1267.40 +/- 877.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.27e+03 |
|    mean_reward     | -708     |
| time/              |          |
|    total_timesteps | 145000   |
| train/             |          |
|    actor_loss      | -0.24    |
|    critic_loss     | 0.0328   |
|    learning_rate   | 0.0003   |
|    n_updates       | 134999   |
---------------------------------
Eval num_timesteps=150000, episode_reward=-1229.08 +/- 216.90
Episode length: 624.20 +/- 53.84
----------------------------------
| eval/              |           |
|    mean_ep_length  | 624       |
|    mean_reward     | -1.23e+03 |
| time/              |           |
|    total_timesteps | 150000    |
| train/             |           |
|    actor_loss      | -0.233    |
|    critic_loss     | 0.0349    |
|    learning_rate   | 0.0003    |
|    n_updates       | 139999    |
----------------------------------
Eval num_timesteps=155000, episode_reward=-1475.83 +/- 693.04
Episode length: 564.20 +/- 381.16
----------------------------------
| eval/              |           |
|    mean_ep_length  | 564       |
|    mean_reward     | -1.48e+03 |
| time/              |           |
|    total_timesteps | 155000    |
| train/             |           |
|    actor_loss      | -0.358    |
|    critic_loss     | 0.0216    |
|    learning_rate   | 0.0003    |
|    n_updates       | 144999    |
----------------------------------
Eval num_timesteps=160000, episode_reward=-1667.72 +/- 541.02
Episode length: 1519.40 +/- 260.63
----------------------------------
| eval/              |           |
|    mean_ep_length  | 1.52e+03  |
|    mean_reward     | -1.67e+03 |
| time/              |           |
|    total_timesteps | 160000    |
| train/             |           |
|    actor_loss      | -0.256    |
|    critic_loss     | 0.0248    |
|    learning_rate   | 0.0003    |
|    n_updates       | 149999    |
----------------------------------
Eval num_timesteps=165000, episode_reward=313.94 +/- 635.27
Episode length: 3000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 3e+03    |
|    mean_reward     | 314      |
| time/              |          |
|    total_timesteps | 165000   |
| train/             |          |
|    actor_loss      | -0.448   |
|    critic_loss     | 0.0256   |
|    learning_rate   | 0.0003   |
|    n_updates       | 154999   |
---------------------------------
Eval num_timesteps=170000, episode_reward=138.25 +/- 1209.30
Episode length: 3000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 3e+03    |
|    mean_reward     | 138      |
| time/              |          |
|    total_timesteps | 170000   |
| train/             |          |
|    actor_loss      | -0.43    |
|    critic_loss     | 0.0263   |
|    learning_rate   | 0.0003   |
|    n_updates       | 159999   |
---------------------------------
Eval num_timesteps=175000, episode_reward=86.47 +/- 698.91
Episode length: 3000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 3e+03    |
|    mean_reward     | 86.5     |
| time/              |          |
|    total_timesteps | 175000   |
| train/             |          |
|    actor_loss      | -0.485   |
|    critic_loss     | 0.028    |
|    learning_rate   | 0.0003   |
|    n_updates       | 164999   |
---------------------------------
Eval num_timesteps=180000, episode_reward=-697.27 +/- 1885.68
Episode length: 1998.60 +/- 1226.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -697     |
| time/              |          |
|    total_timesteps | 180000   |
| train/             |          |
|    actor_loss      | -0.516   |
|    critic_loss     | 0.052    |
|    learning_rate   | 0.0003   |
|    n_updates       | 169999   |
---------------------------------
Eval num_timesteps=185000, episode_reward=2243.88 +/- 8443.67
Episode length: 1636.60 +/- 1136.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.64e+03 |
|    mean_reward     | 2.24e+03 |
| time/              |          |
|    total_timesteps | 185000   |
| train/             |          |
|    actor_loss      | -0.529   |
|    critic_loss     | 0.0169   |
|    learning_rate   | 0.0003   |
|    n_updates       | 174999   |
---------------------------------
Eval num_timesteps=190000, episode_reward=-164.39 +/- 268.17
Episode length: 3000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 3e+03    |
|    mean_reward     | -164     |
| time/              |          |
|    total_timesteps | 190000   |
| train/             |          |
|    actor_loss      | -0.592   |
|    critic_loss     | 0.0235   |
|    learning_rate   | 0.0003   |
|    n_updates       | 179999   |
---------------------------------
Eval num_timesteps=195000, episode_reward=191.08 +/- 482.93
Episode length: 3000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 3e+03    |
|    mean_reward     | 191      |
| time/              |          |
|    total_timesteps | 195000   |
| train/             |          |
|    actor_loss      | -0.774   |
|    critic_loss     | 0.0394   |
|    learning_rate   | 0.0003   |
|    n_updates       | 184999   |
---------------------------------
Eval num_timesteps=200000, episode_reward=13868.77 +/- 12036.47
Episode length: 959.80 +/- 1163.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 960      |
|    mean_reward     | 1.39e+04 |
| time/              |          |
|    total_timesteps | 200000   |
| train/             |          |
|    actor_loss      | -0.738   |
|    critic_loss     | 0.028    |
|    learning_rate   | 0.0003   |
|    n_updates       | 189999   |
---------------------------------
New best mean reward!
保存与最佳模型对应的VecNormalize参数
Eval num_timesteps=205000, episode_reward=3919.05 +/- 8762.56
Episode length: 1956.40 +/- 1281.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.96e+03 |
|    mean_reward     | 3.92e+03 |
| time/              |          |
|    total_timesteps | 205000   |
| train/             |          |
|    actor_loss      | -0.907   |
|    critic_loss     | 0.0201   |
|    learning_rate   | 0.0003   |
|    n_updates       | 194999   |
---------------------------------
Eval num_timesteps=210000, episode_reward=3482.50 +/- 10243.66
Episode length: 2407.40 +/- 1185.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2.41e+03 |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 210000   |
| train/             |          |
|    actor_loss      | -0.902   |
|    critic_loss     | 0.022    |
|    learning_rate   | 0.0003   |
|    n_updates       | 199999   |
---------------------------------
Eval num_timesteps=215000, episode_reward=-2872.63 +/- 834.87
Episode length: 2358.80 +/- 896.58
----------------------------------
| eval/              |           |
|    mean_ep_length  | 2.36e+03  |
|    mean_reward     | -2.87e+03 |
| time/              |           |
|    total_timesteps | 215000    |
| train/             |           |
|    actor_loss      | -1.06     |
|    critic_loss     | 0.0162    |
|    learning_rate   | 0.0003    |
|    n_updates       | 204999    |
----------------------------------
Eval num_timesteps=220000, episode_reward=-3893.09 +/- 1544.40
Episode length: 3000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 3e+03     |
|    mean_reward     | -3.89e+03 |
| time/              |           |
|    total_timesteps | 220000    |
| train/             |           |
|    actor_loss      | -0.963    |
|    critic_loss     | 0.0241    |
|    learning_rate   | 0.0003    |
|    n_updates       | 209999    |
----------------------------------
Eval num_timesteps=225000, episode_reward=2053.85 +/- 10886.86
Episode length: 2066.60 +/- 1196.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2.07e+03 |
|    mean_reward     | 2.05e+03 |
| time/              |          |
|    total_timesteps | 225000   |
| train/             |          |
|    actor_loss      | -1.07    |
|    critic_loss     | 0.0253   |
|    learning_rate   | 0.0003   |
|    n_updates       | 214999   |
---------------------------------
Eval num_timesteps=230000, episode_reward=9615.28 +/- 11541.11
Episode length: 1834.00 +/- 1428.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.83e+03 |
|    mean_reward     | 9.62e+03 |
| time/              |          |
|    total_timesteps | 230000   |
| train/             |          |
|    actor_loss      | -1       |
|    critic_loss     | 0.0201   |
|    learning_rate   | 0.0003   |
|    n_updates       | 219999   |
---------------------------------
Eval num_timesteps=235000, episode_reward=3468.67 +/- 10300.64
Episode length: 2422.00 +/- 1156.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2.42e+03 |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 235000   |
| train/             |          |
|    actor_loss      | -0.944   |
|    critic_loss     | 0.0252   |
|    learning_rate   | 0.0003   |
|    n_updates       | 224999   |
---------------------------------
Eval num_timesteps=240000, episode_reward=2582.94 +/- 9376.60
Episode length: 2528.40 +/- 943.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2.53e+03 |
|    mean_reward     | 2.58e+03 |
| time/              |          |
|    total_timesteps | 240000   |
| train/             |          |
|    actor_loss      | -0.985   |
|    critic_loss     | 0.0346   |
|    learning_rate   | 0.0003   |
|    n_updates       | 229999   |
---------------------------------
Eval num_timesteps=245000, episode_reward=19287.75 +/- 8592.68
Episode length: 695.80 +/- 1153.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 696      |
|    mean_reward     | 1.93e+04 |
| time/              |          |
|    total_timesteps | 245000   |
| train/             |          |
|    actor_loss      | -1.1     |
|    critic_loss     | 0.0268   |
|    learning_rate   | 0.0003   |
|    n_updates       | 234999   |
---------------------------------
New best mean reward!
保存与最佳模型对应的VecNormalize参数
Eval num_timesteps=250000, episode_reward=14177.79 +/- 11406.73
Episode length: 1283.40 +/- 1403.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.28e+03 |
|    mean_reward     | 1.42e+04 |
| time/              |          |
|    total_timesteps | 250000   |
| train/             |          |
|    actor_loss      | -1       |
|    critic_loss     | 0.0239   |
|    learning_rate   | 0.0003   |
|    n_updates       | 239999   |
---------------------------------
Eval num_timesteps=255000, episode_reward=2900.30 +/- 10060.78
Episode length: 2476.80 +/- 1046.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2.48e+03 |
|    mean_reward     | 2.9e+03  |
| time/              |          |
|    total_timesteps | 255000   |
| train/             |          |
|    actor_loss      | -0.983   |
|    critic_loss     | 0.0231   |
|    learning_rate   | 0.0003   |
|    n_updates       | 244999   |
---------------------------------
Eval num_timesteps=260000, episode_reward=9631.58 +/- 10979.57
Episode length: 1894.00 +/- 1358.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.89e+03 |
|    mean_reward     | 9.63e+03 |
| time/              |          |
|    total_timesteps | 260000   |
| train/             |          |
|    actor_loss      | -1.04    |
|    critic_loss     | 0.0256   |
|    learning_rate   | 0.0003   |
|    n_updates       | 249999   |
---------------------------------
Eval num_timesteps=265000, episode_reward=8713.59 +/- 9799.41
Episode length: 2179.60 +/- 1143.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2.18e+03 |
|    mean_reward     | 8.71e+03 |
| time/              |          |
|    total_timesteps | 265000   |
| train/             |          |
|    actor_loss      | -0.945   |
|    critic_loss     | 0.0322   |
|    learning_rate   | 0.0003   |
|    n_updates       | 254999   |
---------------------------------
Eval num_timesteps=270000, episode_reward=4812.33 +/- 9688.86
Episode length: 2411.20 +/- 1177.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2.41e+03 |
|    mean_reward     | 4.81e+03 |
| time/              |          |
|    total_timesteps | 270000   |
| train/             |          |
|    actor_loss      | -0.986   |
|    critic_loss     | 0.0238   |
|    learning_rate   | 0.0003   |
|    n_updates       | 259999   |
---------------------------------
Eval num_timesteps=275000, episode_reward=4271.11 +/- 10125.19
Episode length: 2412.20 +/- 1175.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2.41e+03 |
|    mean_reward     | 4.27e+03 |
| time/              |          |
|    total_timesteps | 275000   |
| train/             |          |
|    actor_loss      | -0.985   |
|    critic_loss     | 0.0314   |
|    learning_rate   | 0.0003   |
|    n_updates       | 264999   |
---------------------------------
Eval num_timesteps=280000, episode_reward=13262.04 +/- 11510.42
Episode length: 1405.60 +/- 1309.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.41e+03 |
|    mean_reward     | 1.33e+04 |
| time/              |          |
|    total_timesteps | 280000   |
| train/             |          |
|    actor_loss      | -0.91    |
|    critic_loss     | 0.0149   |
|    learning_rate   | 0.0003   |
|    n_updates       | 269999   |
---------------------------------
Eval num_timesteps=285000, episode_reward=7979.93 +/- 12564.38
Episode length: 1863.20 +/- 1394.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.86e+03 |
|    mean_reward     | 7.98e+03 |
| time/              |          |
|    total_timesteps | 285000   |
| train/             |          |
|    actor_loss      | -0.89    |
|    critic_loss     | 0.0171   |
|    learning_rate   | 0.0003   |
|    n_updates       | 274999   |
---------------------------------
Eval num_timesteps=290000, episode_reward=8729.80 +/- 12232.71
Episode length: 1837.60 +/- 1423.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.84e+03 |
|    mean_reward     | 8.73e+03 |
| time/              |          |
|    total_timesteps | 290000   |
| train/             |          |
|    actor_loss      | -0.908   |
|    critic_loss     | 0.0242   |
|    learning_rate   | 0.0003   |
|    n_updates       | 279999   |
---------------------------------
Eval num_timesteps=295000, episode_reward=1038.24 +/- 11480.58
Episode length: 2412.40 +/- 1175.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2.41e+03 |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 295000   |
| train/             |          |
|    actor_loss      | -0.932   |
|    critic_loss     | 0.0176   |
|    learning_rate   | 0.0003   |
|    n_updates       | 284999   |
---------------------------------
Eval num_timesteps=300000, episode_reward=-2549.53 +/- 1157.54
Episode length: 3000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 3e+03     |
|    mean_reward     | -2.55e+03 |
| time/              |           |
|    total_timesteps | 300000    |
| train/             |           |
|    actor_loss      | -0.774    |
|    critic_loss     | 0.0128    |
|    learning_rate   | 0.0003    |
|    n_updates       | 289999    |
----------------------------------
Eval num_timesteps=305000, episode_reward=-2036.01 +/- 5379.43
Episode length: 2969.60 +/- 60.80
----------------------------------
| eval/              |           |
|    mean_ep_length  | 2.97e+03  |
|    mean_reward     | -2.04e+03 |
| time/              |           |
|    total_timesteps | 305000    |
| train/             |           |
|    actor_loss      | -0.924    |
|    critic_loss     | 0.012     |
|    learning_rate   | 0.0003    |
|    n_updates       | 294999    |
----------------------------------
Eval num_timesteps=310000, episode_reward=13592.99 +/- 12564.66
Episode length: 1234.80 +/- 1441.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.23e+03 |
|    mean_reward     | 1.36e+04 |
| time/              |          |
|    total_timesteps | 310000   |
| train/             |          |
|    actor_loss      | -1.02    |
|    critic_loss     | 0.0106   |
|    learning_rate   | 0.0003   |
|    n_updates       | 299999   |
---------------------------------
Eval num_timesteps=315000, episode_reward=-574.76 +/- 978.10
Episode length: 3000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 3e+03    |
|    mean_reward     | -575     |
| time/              |          |
|    total_timesteps | 315000   |
| train/             |          |
|    actor_loss      | -0.99    |
|    critic_loss     | 0.0144   |
|    learning_rate   | 0.0003   |
|    n_updates       | 304999   |
---------------------------------
Eval num_timesteps=320000, episode_reward=-209.52 +/- 1577.67
Episode length: 3000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 3e+03    |
|    mean_reward     | -210     |
| time/              |          |
|    total_timesteps | 320000   |
| train/             |          |
|    actor_loss      | -0.877   |
|    critic_loss     | 0.0212   |
|    learning_rate   | 0.0003   |
|    n_updates       | 309999   |
---------------------------------
Eval num_timesteps=325000, episode_reward=3811.89 +/- 10230.97
Episode length: 2407.00 +/- 1186.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2.41e+03 |
|    mean_reward     | 3.81e+03 |
| time/              |          |
|    total_timesteps | 325000   |
| train/             |          |
|    actor_loss      | -0.946   |
|    critic_loss     | 0.0158   |
|    learning_rate   | 0.0003   |
|    n_updates       | 314999   |
---------------------------------
Eval num_timesteps=330000, episode_reward=1329.15 +/- 988.73
Episode length: 3000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 3e+03    |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 330000   |
| train/             |          |
|    actor_loss      | -0.934   |
|    critic_loss     | 0.0148   |
|    learning_rate   | 0.0003   |
|    n_updates       | 319999   |
---------------------------------
Eval num_timesteps=335000, episode_reward=5745.47 +/- 14908.56
Episode length: 1809.00 +/- 1458.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.81e+03 |
|    mean_reward     | 5.75e+03 |
| time/              |          |
|    total_timesteps | 335000   |
| train/             |          |
|    actor_loss      | -0.965   |
|    critic_loss     | 0.00805  |
|    learning_rate   | 0.0003   |
|    n_updates       | 324999   |
---------------------------------
Eval num_timesteps=340000, episode_reward=13929.13 +/- 11137.08
Episode length: 1347.60 +/- 1361.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.35e+03 |
|    mean_reward     | 1.39e+04 |
| time/              |          |
|    total_timesteps | 340000   |
| train/             |          |
|    actor_loss      | -0.909   |
|    critic_loss     | 0.00955  |
|    learning_rate   | 0.0003   |
|    n_updates       | 329999   |
---------------------------------
Eval num_timesteps=345000, episode_reward=9360.82 +/- 11891.55
Episode length: 1817.40 +/- 1448.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.82e+03 |
|    mean_reward     | 9.36e+03 |
| time/              |          |
|    total_timesteps | 345000   |
| train/             |          |
|    actor_loss      | -0.813   |
|    critic_loss     | 0.0131   |
|    learning_rate   | 0.0003   |
|    n_updates       | 334999   |
---------------------------------
Eval num_timesteps=350000, episode_reward=4984.22 +/- 9549.55
Episode length: 2416.00 +/- 1168.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2.42e+03 |
|    mean_reward     | 4.98e+03 |
| time/              |          |
|    total_timesteps | 350000   |
| train/             |          |
|    actor_loss      | -1.08    |
|    critic_loss     | 0.0223   |
|    learning_rate   | 0.0003   |
|    n_updates       | 339999   |
---------------------------------
Eval num_timesteps=355000, episode_reward=12115.34 +/- 12342.37
Episode length: 1455.20 +/- 1302.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.46e+03 |
|    mean_reward     | 1.21e+04 |
| time/              |          |
|    total_timesteps | 355000   |
| train/             |          |
|    actor_loss      | -0.828   |
|    critic_loss     | 0.00938  |
|    learning_rate   | 0.0003   |
|    n_updates       | 344999   |
---------------------------------
Eval num_timesteps=360000, episode_reward=1895.33 +/- 1258.87
Episode length: 3000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 3e+03    |
|    mean_reward     | 1.9e+03  |
| time/              |          |
|    total_timesteps | 360000   |
| train/             |          |
|    actor_loss      | -1.02    |
|    critic_loss     | 0.282    |
|    learning_rate   | 0.0003   |
|    n_updates       | 349999   |
---------------------------------
Eval num_timesteps=365000, episode_reward=12988.46 +/- 11226.37
Episode length: 1467.40 +/- 1286.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.47e+03 |
|    mean_reward     | 1.3e+04  |
| time/              |          |
|    total_timesteps | 365000   |
| train/             |          |
|    actor_loss      | -0.821   |
|    critic_loss     | 0.0105   |
|    learning_rate   | 0.0003   |
|    n_updates       | 354999   |
---------------------------------
Eval num_timesteps=370000, episode_reward=11291.21 +/- 9641.47
Episode length: 1905.20 +/- 1343.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.91e+03 |
|    mean_reward     | 1.13e+04 |
| time/              |          |
|    total_timesteps | 370000   |
| train/             |          |
|    actor_loss      | -0.974   |
|    critic_loss     | 0.0307   |
|    learning_rate   | 0.0003   |
|    n_updates       | 359999   |
---------------------------------
Eval num_timesteps=375000, episode_reward=10473.92 +/- 9399.35
Episode length: 2050.60 +/- 1221.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2.05e+03 |
|    mean_reward     | 1.05e+04 |
| time/              |          |
|    total_timesteps | 375000   |
| train/             |          |
|    actor_loss      | -0.973   |
|    critic_loss     | 0.0182   |
|    learning_rate   | 0.0003   |
|    n_updates       | 364999   |
---------------------------------
Eval num_timesteps=380000, episode_reward=9567.22 +/- 11422.28
Episode length: 1870.60 +/- 1383.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.87e+03 |
|    mean_reward     | 9.57e+03 |
| time/              |          |
|    total_timesteps | 380000   |
| train/             |          |
|    actor_loss      | -0.903   |
|    critic_loss     | 0.0107   |
|    learning_rate   | 0.0003   |
|    n_updates       | 369999   |
---------------------------------
Eval num_timesteps=385000, episode_reward=11740.56 +/- 9637.56
Episode length: 1877.20 +/- 1376.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.88e+03 |
|    mean_reward     | 1.17e+04 |
| time/              |          |
|    total_timesteps | 385000   |
| train/             |          |
|    actor_loss      | -1.01    |
|    critic_loss     | 0.0106   |
|    learning_rate   | 0.0003   |
|    n_updates       | 374999   |
---------------------------------
Eval num_timesteps=390000, episode_reward=12425.62 +/- 11619.43
Episode length: 1548.80 +/- 1308.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.55e+03 |
|    mean_reward     | 1.24e+04 |
| time/              |          |
|    total_timesteps | 390000   |
| train/             |          |
|    actor_loss      | -0.906   |
|    critic_loss     | 0.0358   |
|    learning_rate   | 0.0003   |
|    n_updates       | 379999   |
---------------------------------
Eval num_timesteps=395000, episode_reward=9822.79 +/- 10453.22
Episode length: 1968.40 +/- 1286.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.97e+03 |
|    mean_reward     | 9.82e+03 |
| time/              |          |
|    total_timesteps | 395000   |
| train/             |          |
|    actor_loss      | -1.12    |
|    critic_loss     | 0.0188   |
|    learning_rate   | 0.0003   |
|    n_updates       | 384999   |
---------------------------------
Eval num_timesteps=400000, episode_reward=18513.02 +/- 10466.57
Episode length: 666.60 +/- 1168.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 667      |
|    mean_reward     | 1.85e+04 |
| time/              |          |
|    total_timesteps | 400000   |
| train/             |          |
|    actor_loss      | -1.06    |
|    critic_loss     | 0.0173   |
|    learning_rate   | 0.0003   |
|    n_updates       | 389999   |
---------------------------------
Eval num_timesteps=405000, episode_reward=11317.14 +/- 14875.30
Episode length: 1411.80 +/- 1328.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.41e+03 |
|    mean_reward     | 1.13e+04 |
| time/              |          |
|    total_timesteps | 405000   |
| train/             |          |
|    actor_loss      | -1.16    |
|    critic_loss     | 0.0176   |
|    learning_rate   | 0.0003   |
|    n_updates       | 394999   |
---------------------------------
Eval num_timesteps=410000, episode_reward=11379.87 +/- 9615.22
Episode length: 1898.20 +/- 1349.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.9e+03  |
|    mean_reward     | 1.14e+04 |
| time/              |          |
|    total_timesteps | 410000   |
| train/             |          |
|    actor_loss      | -1.07    |
|    critic_loss     | 0.0178   |
|    learning_rate   | 0.0003   |
|    n_updates       | 399999   |
---------------------------------
Eval num_timesteps=415000, episode_reward=4495.10 +/- 9494.89
Episode length: 2440.00 +/- 1120.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2.44e+03 |
|    mean_reward     | 4.5e+03  |
| time/              |          |
|    total_timesteps | 415000   |
| train/             |          |
|    actor_loss      | -1.17    |
|    critic_loss     | 0.0662   |
|    learning_rate   | 0.0003   |
|    n_updates       | 404999   |
---------------------------------
Eval num_timesteps=420000, episode_reward=6149.17 +/- 6122.33
Episode length: 2838.40 +/- 323.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2.84e+03 |
|    mean_reward     | 6.15e+03 |
| time/              |          |
|    total_timesteps | 420000   |
| train/             |          |
|    actor_loss      | -1.07    |
|    critic_loss     | 0.0121   |
|    learning_rate   | 0.0003   |
|    n_updates       | 409999   |
---------------------------------
Eval num_timesteps=425000, episode_reward=11694.30 +/- 9933.78
Episode length: 1832.00 +/- 1430.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.83e+03 |
|    mean_reward     | 1.17e+04 |
| time/              |          |
|    total_timesteps | 425000   |
| train/             |          |
|    actor_loss      | -1.2     |
|    critic_loss     | 0.0137   |
|    learning_rate   | 0.0003   |
|    n_updates       | 414999   |
---------------------------------
Eval num_timesteps=430000, episode_reward=21821.60 +/- 3914.20
Episode length: 442.40 +/- 719.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 442      |
|    mean_reward     | 2.18e+04 |
| time/              |          |
|    total_timesteps | 430000   |
| train/             |          |
|    actor_loss      | -1.17    |
|    critic_loss     | 0.0217   |
|    learning_rate   | 0.0003   |
|    n_updates       | 419999   |
---------------------------------
New best mean reward!
保存与最佳模型对应的VecNormalize参数
Eval num_timesteps=435000, episode_reward=15493.32 +/- 10202.57
Episode length: 1228.20 +/- 1446.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.23e+03 |
|    mean_reward     | 1.55e+04 |
| time/              |          |
|    total_timesteps | 435000   |
| train/             |          |
|    actor_loss      | -1.02    |
|    critic_loss     | 0.0152   |
|    learning_rate   | 0.0003   |
|    n_updates       | 424999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 791      |
|    ep_rew_mean     | 7.89e+03 |
| time/              |          |
|    episodes        | 1000     |
|    fps             | 169      |
|    time_elapsed    | 2573     |
|    total_timesteps | 435274   |
| train/             |          |
|    actor_loss      | -1.18    |
|    critic_loss     | 0.0199   |
|    learning_rate   | 0.0003   |
|    n_updates       | 425273   |
---------------------------------
Eval num_timesteps=440000, episode_reward=12500.47 +/- 14157.99
Episode length: 1211.60 +/- 1165.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.21e+03 |
|    mean_reward     | 1.25e+04 |
| time/              |          |
|    total_timesteps | 440000   |
| train/             |          |
|    actor_loss      | -1.33    |
|    critic_loss     | 0.0273   |
|    learning_rate   | 0.0003   |
|    n_updates       | 429999   |
---------------------------------
Eval num_timesteps=445000, episode_reward=19702.38 +/- 8725.48
Episode length: 660.00 +/- 1172.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 660      |
|    mean_reward     | 1.97e+04 |
| time/              |          |
|    total_timesteps | 445000   |
| train/             |          |
|    actor_loss      | -1.24    |
|    critic_loss     | 0.0181   |
|    learning_rate   | 0.0003   |
|    n_updates       | 434999   |
---------------------------------
Eval num_timesteps=450000, episode_reward=18763.58 +/- 7901.27
Episode length: 876.40 +/- 1077.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 876      |
|    mean_reward     | 1.88e+04 |
| time/              |          |
|    total_timesteps | 450000   |
| train/             |          |
|    actor_loss      | -1.25    |
|    critic_loss     | 0.0122   |
|    learning_rate   | 0.0003   |
|    n_updates       | 439999   |
---------------------------------
Eval num_timesteps=455000, episode_reward=23893.71 +/- 165.42
Episode length: 39.60 +/- 27.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39.6     |
|    mean_reward     | 2.39e+04 |
| time/              |          |
|    total_timesteps | 455000   |
| train/             |          |
|    actor_loss      | -1.51    |
|    critic_loss     | 0.0176   |
|    learning_rate   | 0.0003   |
|    n_updates       | 444999   |
---------------------------------
New best mean reward!
保存与最佳模型对应的VecNormalize参数
Eval num_timesteps=460000, episode_reward=339.66 +/- 11663.07
Episode length: 2417.00 +/- 1166.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2.42e+03 |
|    mean_reward     | 340      |
| time/              |          |
|    total_timesteps | 460000   |
| train/             |          |
|    actor_loss      | -1.49    |
|    critic_loss     | 0.0186   |
|    learning_rate   | 0.0003   |
|    n_updates       | 449999   |
---------------------------------
Eval num_timesteps=465000, episode_reward=18251.93 +/- 11907.96
Episode length: 636.80 +/- 1181.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 637      |
|    mean_reward     | 1.83e+04 |
| time/              |          |
|    total_timesteps | 465000   |
| train/             |          |
|    actor_loss      | -1.39    |
|    critic_loss     | 0.0343   |
|    learning_rate   | 0.0003   |
|    n_updates       | 454999   |
---------------------------------
Eval num_timesteps=470000, episode_reward=19602.90 +/- 8154.27
Episode length: 670.40 +/- 1167.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 670      |
|    mean_reward     | 1.96e+04 |
| time/              |          |
|    total_timesteps | 470000   |
| train/             |          |
|    actor_loss      | -1.48    |
|    critic_loss     | 0.0236   |
|    learning_rate   | 0.0003   |
|    n_updates       | 459999   |
---------------------------------
Eval num_timesteps=475000, episode_reward=16746.61 +/- 13320.81
Episode length: 688.80 +/- 1156.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 689      |
|    mean_reward     | 1.67e+04 |
| time/              |          |
|    total_timesteps | 475000   |
| train/             |          |
|    actor_loss      | -1.71    |
|    critic_loss     | 0.0142   |
|    learning_rate   | 0.0003   |
|    n_updates       | 464999   |
---------------------------------
Eval num_timesteps=480000, episode_reward=23977.56 +/- 677.36
Episode length: 71.20 +/- 31.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 71.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 480000   |
| train/             |          |
|    actor_loss      | -1.6     |
|    critic_loss     | 0.0174   |
|    learning_rate   | 0.0003   |
|    n_updates       | 469999   |
---------------------------------
New best mean reward!
保存与最佳模型对应的VecNormalize参数
Eval num_timesteps=485000, episode_reward=9971.34 +/- 16998.37
Episode length: 1220.40 +/- 1206.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.22e+03 |
|    mean_reward     | 9.97e+03 |
| time/              |          |
|    total_timesteps | 485000   |
| train/             |          |
|    actor_loss      | -1.59    |
|    critic_loss     | 0.0227   |
|    learning_rate   | 0.0003   |
|    n_updates       | 474999   |
---------------------------------
Eval num_timesteps=490000, episode_reward=18406.44 +/- 10203.74
Episode length: 676.40 +/- 1163.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 676      |
|    mean_reward     | 1.84e+04 |
| time/              |          |
|    total_timesteps | 490000   |
| train/             |          |
|    actor_loss      | -1.75    |
|    critic_loss     | 0.0193   |
|    learning_rate   | 0.0003   |
|    n_updates       | 479999   |
---------------------------------
Eval num_timesteps=495000, episode_reward=22365.88 +/- 1783.52
Episode length: 271.00 +/- 267.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 271      |
|    mean_reward     | 2.24e+04 |
| time/              |          |
|    total_timesteps | 495000   |
| train/             |          |
|    actor_loss      | -1.56    |
|    critic_loss     | 0.0285   |
|    learning_rate   | 0.0003   |
|    n_updates       | 484999   |
---------------------------------
Eval num_timesteps=500000, episode_reward=23405.26 +/- 473.04
Episode length: 132.80 +/- 78.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 133      |
|    mean_reward     | 2.34e+04 |
| time/              |          |
|    total_timesteps | 500000   |
| train/             |          |
|    actor_loss      | -1.78    |
|    critic_loss     | 0.0321   |
|    learning_rate   | 0.0003   |
|    n_updates       | 489999   |
---------------------------------
Eval num_timesteps=505000, episode_reward=17340.58 +/- 8952.27
Episode length: 1047.60 +/- 1144.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.05e+03 |
|    mean_reward     | 1.73e+04 |
| time/              |          |
|    total_timesteps | 505000   |
| train/             |          |
|    actor_loss      | -1.79    |
|    critic_loss     | 0.0413   |
|    learning_rate   | 0.0003   |
|    n_updates       | 494999   |
---------------------------------
Eval num_timesteps=510000, episode_reward=23870.29 +/- 119.06
Episode length: 53.60 +/- 27.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.6     |
|    mean_reward     | 2.39e+04 |
| time/              |          |
|    total_timesteps | 510000   |
| train/             |          |
|    actor_loss      | -1.85    |
|    critic_loss     | 0.0219   |
|    learning_rate   | 0.0003   |
|    n_updates       | 499999   |
---------------------------------
Eval num_timesteps=515000, episode_reward=23878.72 +/- 144.65
Episode length: 43.80 +/- 26.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43.8     |
|    mean_reward     | 2.39e+04 |
| time/              |          |
|    total_timesteps | 515000   |
| train/             |          |
|    actor_loss      | -1.9     |
|    critic_loss     | 0.0204   |
|    learning_rate   | 0.0003   |
|    n_updates       | 504999   |
---------------------------------
Eval num_timesteps=520000, episode_reward=17393.21 +/- 9522.28
Episode length: 848.60 +/- 1148.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 849      |
|    mean_reward     | 1.74e+04 |
| time/              |          |
|    total_timesteps | 520000   |
| train/             |          |
|    actor_loss      | -2.01    |
|    critic_loss     | 0.0191   |
|    learning_rate   | 0.0003   |
|    n_updates       | 509999   |
---------------------------------
Eval num_timesteps=525000, episode_reward=23888.11 +/- 111.78
Episode length: 36.80 +/- 19.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | 2.39e+04 |
| time/              |          |
|    total_timesteps | 525000   |
| train/             |          |
|    actor_loss      | -2.08    |
|    critic_loss     | 0.0254   |
|    learning_rate   | 0.0003   |
|    n_updates       | 514999   |
---------------------------------
Eval num_timesteps=530000, episode_reward=23945.87 +/- 47.92
Episode length: 25.40 +/- 3.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.4     |
|    mean_reward     | 2.39e+04 |
| time/              |          |
|    total_timesteps | 530000   |
| train/             |          |
|    actor_loss      | -2.01    |
|    critic_loss     | 0.0292   |
|    learning_rate   | 0.0003   |
|    n_updates       | 519999   |
---------------------------------
Eval num_timesteps=535000, episode_reward=20900.23 +/- 7464.51
Episode length: 536.20 +/- 999.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 536      |
|    mean_reward     | 2.09e+04 |
| time/              |          |
|    total_timesteps | 535000   |
| train/             |          |
|    actor_loss      | -2.29    |
|    critic_loss     | 0.159    |
|    learning_rate   | 0.0003   |
|    n_updates       | 524999   |
---------------------------------
Eval num_timesteps=540000, episode_reward=23961.05 +/- 71.94
Episode length: 28.60 +/- 10.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 28.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 540000   |
| train/             |          |
|    actor_loss      | -2.33    |
|    critic_loss     | 0.0622   |
|    learning_rate   | 0.0003   |
|    n_updates       | 529999   |
---------------------------------
Eval num_timesteps=545000, episode_reward=17145.65 +/- 13388.01
Episode length: 637.20 +/- 1181.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 637      |
|    mean_reward     | 1.71e+04 |
| time/              |          |
|    total_timesteps | 545000   |
| train/             |          |
|    actor_loss      | -2.44    |
|    critic_loss     | 0.0251   |
|    learning_rate   | 0.0003   |
|    n_updates       | 534999   |
---------------------------------
Eval num_timesteps=550000, episode_reward=24006.55 +/- 801.67
Episode length: 58.40 +/- 55.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 550000   |
| train/             |          |
|    actor_loss      | -2.28    |
|    critic_loss     | 0.025    |
|    learning_rate   | 0.0003   |
|    n_updates       | 539999   |
---------------------------------
New best mean reward!
保存与最佳模型对应的VecNormalize参数
Eval num_timesteps=555000, episode_reward=23023.00 +/- 1858.61
Episode length: 252.20 +/- 438.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 252      |
|    mean_reward     | 2.3e+04  |
| time/              |          |
|    total_timesteps | 555000   |
| train/             |          |
|    actor_loss      | -2.46    |
|    critic_loss     | 0.0231   |
|    learning_rate   | 0.0003   |
|    n_updates       | 544999   |
---------------------------------
Eval num_timesteps=560000, episode_reward=17242.23 +/- 10807.21
Episode length: 778.20 +/- 1125.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 778      |
|    mean_reward     | 1.72e+04 |
| time/              |          |
|    total_timesteps | 560000   |
| train/             |          |
|    actor_loss      | -2.5     |
|    critic_loss     | 0.0252   |
|    learning_rate   | 0.0003   |
|    n_updates       | 549999   |
---------------------------------
Eval num_timesteps=565000, episode_reward=17183.89 +/- 13152.06
Episode length: 640.40 +/- 1179.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 640      |
|    mean_reward     | 1.72e+04 |
| time/              |          |
|    total_timesteps | 565000   |
| train/             |          |
|    actor_loss      | -3.1     |
|    critic_loss     | 0.0236   |
|    learning_rate   | 0.0003   |
|    n_updates       | 554999   |
---------------------------------
Eval num_timesteps=570000, episode_reward=23783.41 +/- 168.26
Episode length: 50.20 +/- 23.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | 2.38e+04 |
| time/              |          |
|    total_timesteps | 570000   |
| train/             |          |
|    actor_loss      | -2.71    |
|    critic_loss     | 0.0312   |
|    learning_rate   | 0.0003   |
|    n_updates       | 559999   |
---------------------------------
Eval num_timesteps=575000, episode_reward=22151.75 +/- 2425.73
Episode length: 289.60 +/- 379.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 290      |
|    mean_reward     | 2.22e+04 |
| time/              |          |
|    total_timesteps | 575000   |
| train/             |          |
|    actor_loss      | -2.88    |
|    critic_loss     | 0.0264   |
|    learning_rate   | 0.0003   |
|    n_updates       | 564999   |
---------------------------------
Eval num_timesteps=580000, episode_reward=23835.57 +/- 90.73
Episode length: 47.40 +/- 15.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.4     |
|    mean_reward     | 2.38e+04 |
| time/              |          |
|    total_timesteps | 580000   |
| train/             |          |
|    actor_loss      | -3.09    |
|    critic_loss     | 0.0255   |
|    learning_rate   | 0.0003   |
|    n_updates       | 569999   |
---------------------------------
Eval num_timesteps=585000, episode_reward=23652.17 +/- 414.50
Episode length: 62.20 +/- 52.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62.2     |
|    mean_reward     | 2.37e+04 |
| time/              |          |
|    total_timesteps | 585000   |
| train/             |          |
|    actor_loss      | -2.95    |
|    critic_loss     | 0.0266   |
|    learning_rate   | 0.0003   |
|    n_updates       | 574999   |
---------------------------------
Eval num_timesteps=590000, episode_reward=23695.53 +/- 168.64
Episode length: 81.40 +/- 35.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 81.4     |
|    mean_reward     | 2.37e+04 |
| time/              |          |
|    total_timesteps | 590000   |
| train/             |          |
|    actor_loss      | -2.87    |
|    critic_loss     | 0.0241   |
|    learning_rate   | 0.0003   |
|    n_updates       | 579999   |
---------------------------------
Eval num_timesteps=595000, episode_reward=24205.75 +/- 571.92
Episode length: 34.80 +/- 11.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 2.42e+04 |
| time/              |          |
|    total_timesteps | 595000   |
| train/             |          |
|    actor_loss      | -3.27    |
|    critic_loss     | 0.0298   |
|    learning_rate   | 0.0003   |
|    n_updates       | 584999   |
---------------------------------
New best mean reward!
保存与最佳模型对应的VecNormalize参数
Eval num_timesteps=600000, episode_reward=23910.08 +/- 88.15
Episode length: 29.60 +/- 12.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.6     |
|    mean_reward     | 2.39e+04 |
| time/              |          |
|    total_timesteps | 600000   |
| train/             |          |
|    actor_loss      | -3.16    |
|    critic_loss     | 0.0287   |
|    learning_rate   | 0.0003   |
|    n_updates       | 589999   |
---------------------------------
Eval num_timesteps=605000, episode_reward=23890.84 +/- 121.29
Episode length: 34.60 +/- 12.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 2.39e+04 |
| time/              |          |
|    total_timesteps | 605000   |
| train/             |          |
|    actor_loss      | -3.16    |
|    critic_loss     | 0.0308   |
|    learning_rate   | 0.0003   |
|    n_updates       | 594999   |
---------------------------------
Eval num_timesteps=610000, episode_reward=23898.34 +/- 122.29
Episode length: 30.40 +/- 16.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.4     |
|    mean_reward     | 2.39e+04 |
| time/              |          |
|    total_timesteps | 610000   |
| train/             |          |
|    actor_loss      | -3.28    |
|    critic_loss     | 0.03     |
|    learning_rate   | 0.0003   |
|    n_updates       | 599999   |
---------------------------------
Eval num_timesteps=615000, episode_reward=17197.56 +/- 13019.94
Episode length: 645.60 +/- 1177.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 646      |
|    mean_reward     | 1.72e+04 |
| time/              |          |
|    total_timesteps | 615000   |
| train/             |          |
|    actor_loss      | -3.1     |
|    critic_loss     | 0.191    |
|    learning_rate   | 0.0003   |
|    n_updates       | 604999   |
---------------------------------
Eval num_timesteps=620000, episode_reward=23942.91 +/- 78.84
Episode length: 23.60 +/- 11.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | 2.39e+04 |
| time/              |          |
|    total_timesteps | 620000   |
| train/             |          |
|    actor_loss      | -3.4     |
|    critic_loss     | 0.0259   |
|    learning_rate   | 0.0003   |
|    n_updates       | 609999   |
---------------------------------
Eval num_timesteps=625000, episode_reward=23865.45 +/- 78.34
Episode length: 39.60 +/- 12.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39.6     |
|    mean_reward     | 2.39e+04 |
| time/              |          |
|    total_timesteps | 625000   |
| train/             |          |
|    actor_loss      | -3.29    |
|    critic_loss     | 0.0356   |
|    learning_rate   | 0.0003   |
|    n_updates       | 614999   |
---------------------------------
Eval num_timesteps=630000, episode_reward=24238.70 +/- 596.85
Episode length: 28.40 +/- 9.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 28.4     |
|    mean_reward     | 2.42e+04 |
| time/              |          |
|    total_timesteps | 630000   |
| train/             |          |
|    actor_loss      | -3.48    |
|    critic_loss     | 0.0259   |
|    learning_rate   | 0.0003   |
|    n_updates       | 619999   |
---------------------------------
New best mean reward!
保存与最佳模型对应的VecNormalize参数
Eval num_timesteps=635000, episode_reward=23838.10 +/- 85.38
Episode length: 45.00 +/- 17.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45       |
|    mean_reward     | 2.38e+04 |
| time/              |          |
|    total_timesteps | 635000   |
| train/             |          |
|    actor_loss      | -3.21    |
|    critic_loss     | 0.0219   |
|    learning_rate   | 0.0003   |
|    n_updates       | 624999   |
---------------------------------
Eval num_timesteps=640000, episode_reward=23956.69 +/- 55.80
Episode length: 22.00 +/- 3.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 640000   |
| train/             |          |
|    actor_loss      | -3.16    |
|    critic_loss     | 0.0248   |
|    learning_rate   | 0.0003   |
|    n_updates       | 629999   |
---------------------------------
Eval num_timesteps=645000, episode_reward=23856.26 +/- 57.28
Episode length: 34.40 +/- 7.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 2.39e+04 |
| time/              |          |
|    total_timesteps | 645000   |
| train/             |          |
|    actor_loss      | -3.63    |
|    critic_loss     | 0.0329   |
|    learning_rate   | 0.0003   |
|    n_updates       | 634999   |
---------------------------------
Eval num_timesteps=650000, episode_reward=23871.03 +/- 83.83
Episode length: 33.80 +/- 15.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 2.39e+04 |
| time/              |          |
|    total_timesteps | 650000   |
| train/             |          |
|    actor_loss      | -3.41    |
|    critic_loss     | 0.0501   |
|    learning_rate   | 0.0003   |
|    n_updates       | 639999   |
---------------------------------
Eval num_timesteps=655000, episode_reward=23801.40 +/- 99.74
Episode length: 39.60 +/- 10.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39.6     |
|    mean_reward     | 2.38e+04 |
| time/              |          |
|    total_timesteps | 655000   |
| train/             |          |
|    actor_loss      | -3.79    |
|    critic_loss     | 0.0314   |
|    learning_rate   | 0.0003   |
|    n_updates       | 644999   |
---------------------------------
Eval num_timesteps=660000, episode_reward=23855.96 +/- 40.88
Episode length: 42.80 +/- 9.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.8     |
|    mean_reward     | 2.39e+04 |
| time/              |          |
|    total_timesteps | 660000   |
| train/             |          |
|    actor_loss      | -3.57    |
|    critic_loss     | 0.029    |
|    learning_rate   | 0.0003   |
|    n_updates       | 649999   |
---------------------------------
Eval num_timesteps=665000, episode_reward=24516.60 +/- 773.61
Episode length: 31.60 +/- 12.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31.6     |
|    mean_reward     | 2.45e+04 |
| time/              |          |
|    total_timesteps | 665000   |
| train/             |          |
|    actor_loss      | -3.6     |
|    critic_loss     | 0.0368   |
|    learning_rate   | 0.0003   |
|    n_updates       | 654999   |
---------------------------------
New best mean reward!
保存与最佳模型对应的VecNormalize参数
Eval num_timesteps=670000, episode_reward=24201.32 +/- 640.86
Episode length: 37.80 +/- 22.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.8     |
|    mean_reward     | 2.42e+04 |
| time/              |          |
|    total_timesteps | 670000   |
| train/             |          |
|    actor_loss      | -3.72    |
|    critic_loss     | 0.0563   |
|    learning_rate   | 0.0003   |
|    n_updates       | 659999   |
---------------------------------
Eval num_timesteps=675000, episode_reward=23937.80 +/- 52.83
Episode length: 26.40 +/- 5.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.4     |
|    mean_reward     | 2.39e+04 |
| time/              |          |
|    total_timesteps | 675000   |
| train/             |          |
|    actor_loss      | -3.66    |
|    critic_loss     | 0.0235   |
|    learning_rate   | 0.0003   |
|    n_updates       | 664999   |
---------------------------------
Eval num_timesteps=680000, episode_reward=23824.58 +/- 266.52
Episode length: 44.60 +/- 32.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.6     |
|    mean_reward     | 2.38e+04 |
| time/              |          |
|    total_timesteps | 680000   |
| train/             |          |
|    actor_loss      | -3.71    |
|    critic_loss     | 0.0376   |
|    learning_rate   | 0.0003   |
|    n_updates       | 669999   |
---------------------------------
Eval num_timesteps=685000, episode_reward=23937.42 +/- 69.83
Episode length: 28.60 +/- 9.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 28.6     |
|    mean_reward     | 2.39e+04 |
| time/              |          |
|    total_timesteps | 685000   |
| train/             |          |
|    actor_loss      | -3.73    |
|    critic_loss     | 0.0256   |
|    learning_rate   | 0.0003   |
|    n_updates       | 674999   |
---------------------------------
Eval num_timesteps=690000, episode_reward=23959.97 +/- 47.87
Episode length: 25.20 +/- 6.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 690000   |
| train/             |          |
|    actor_loss      | -3.7     |
|    critic_loss     | 0.04     |
|    learning_rate   | 0.0003   |
|    n_updates       | 679999   |
---------------------------------
Eval num_timesteps=695000, episode_reward=23902.83 +/- 92.39
Episode length: 33.40 +/- 10.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.4     |
|    mean_reward     | 2.39e+04 |
| time/              |          |
|    total_timesteps | 695000   |
| train/             |          |
|    actor_loss      | -3.34    |
|    critic_loss     | 0.0283   |
|    learning_rate   | 0.0003   |
|    n_updates       | 684999   |
---------------------------------
Eval num_timesteps=700000, episode_reward=23906.64 +/- 53.91
Episode length: 36.00 +/- 6.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 2.39e+04 |
| time/              |          |
|    total_timesteps | 700000   |
| train/             |          |
|    actor_loss      | -3.55    |
|    critic_loss     | 0.0355   |
|    learning_rate   | 0.0003   |
|    n_updates       | 689999   |
---------------------------------
Eval num_timesteps=705000, episode_reward=23953.71 +/- 63.32
Episode length: 26.00 +/- 4.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 705000   |
| train/             |          |
|    actor_loss      | -3.72    |
|    critic_loss     | 0.0283   |
|    learning_rate   | 0.0003   |
|    n_updates       | 694999   |
---------------------------------
Eval num_timesteps=710000, episode_reward=16575.52 +/- 14599.63
Episode length: 626.40 +/- 1186.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 626      |
|    mean_reward     | 1.66e+04 |
| time/              |          |
|    total_timesteps | 710000   |
| train/             |          |
|    actor_loss      | -3.76    |
|    critic_loss     | 0.0444   |
|    learning_rate   | 0.0003   |
|    n_updates       | 699999   |
---------------------------------
Eval num_timesteps=715000, episode_reward=24546.95 +/- 727.36
Episode length: 23.00 +/- 3.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23       |
|    mean_reward     | 2.45e+04 |
| time/              |          |
|    total_timesteps | 715000   |
| train/             |          |
|    actor_loss      | -3.78    |
|    critic_loss     | 0.0569   |
|    learning_rate   | 0.0003   |
|    n_updates       | 704999   |
---------------------------------
New best mean reward!
保存与最佳模型对应的VecNormalize参数
Eval num_timesteps=720000, episode_reward=23967.96 +/- 26.26
Episode length: 25.20 +/- 3.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 720000   |
| train/             |          |
|    actor_loss      | -4.06    |
|    critic_loss     | 0.0396   |
|    learning_rate   | 0.0003   |
|    n_updates       | 709999   |
---------------------------------
Eval num_timesteps=725000, episode_reward=12853.24 +/- 22228.10
Episode length: 618.20 +/- 1190.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 618      |
|    mean_reward     | 1.29e+04 |
| time/              |          |
|    total_timesteps | 725000   |
| train/             |          |
|    actor_loss      | -4.25    |
|    critic_loss     | 0.0441   |
|    learning_rate   | 0.0003   |
|    n_updates       | 714999   |
---------------------------------
Eval num_timesteps=730000, episode_reward=23466.41 +/- 411.35
Episode length: 92.00 +/- 56.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92       |
|    mean_reward     | 2.35e+04 |
| time/              |          |
|    total_timesteps | 730000   |
| train/             |          |
|    actor_loss      | -4.15    |
|    critic_loss     | 0.0326   |
|    learning_rate   | 0.0003   |
|    n_updates       | 719999   |
---------------------------------
Eval num_timesteps=735000, episode_reward=23975.48 +/- 22.37
Episode length: 24.60 +/- 3.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 735000   |
| train/             |          |
|    actor_loss      | -4.09    |
|    critic_loss     | 0.0427   |
|    learning_rate   | 0.0003   |
|    n_updates       | 724999   |
---------------------------------
Eval num_timesteps=740000, episode_reward=17602.76 +/- 12537.03
Episode length: 631.40 +/- 1184.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 631      |
|    mean_reward     | 1.76e+04 |
| time/              |          |
|    total_timesteps | 740000   |
| train/             |          |
|    actor_loss      | -4.14    |
|    critic_loss     | 0.0512   |
|    learning_rate   | 0.0003   |
|    n_updates       | 729999   |
---------------------------------
Eval num_timesteps=745000, episode_reward=23931.71 +/- 33.58
Episode length: 27.00 +/- 2.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27       |
|    mean_reward     | 2.39e+04 |
| time/              |          |
|    total_timesteps | 745000   |
| train/             |          |
|    actor_loss      | -4.82    |
|    critic_loss     | 0.0383   |
|    learning_rate   | 0.0003   |
|    n_updates       | 734999   |
---------------------------------
Eval num_timesteps=750000, episode_reward=23959.81 +/- 32.86
Episode length: 24.60 +/- 2.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 750000   |
| train/             |          |
|    actor_loss      | -4.44    |
|    critic_loss     | 0.0437   |
|    learning_rate   | 0.0003   |
|    n_updates       | 739999   |
---------------------------------
Eval num_timesteps=755000, episode_reward=23916.52 +/- 43.55
Episode length: 36.60 +/- 10.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 2.39e+04 |
| time/              |          |
|    total_timesteps | 755000   |
| train/             |          |
|    actor_loss      | -4.72    |
|    critic_loss     | 0.0356   |
|    learning_rate   | 0.0003   |
|    n_updates       | 744999   |
---------------------------------
Eval num_timesteps=760000, episode_reward=23966.78 +/- 36.06
Episode length: 32.40 +/- 8.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 760000   |
| train/             |          |
|    actor_loss      | -4.91    |
|    critic_loss     | 0.0335   |
|    learning_rate   | 0.0003   |
|    n_updates       | 749999   |
---------------------------------
Eval num_timesteps=765000, episode_reward=23933.18 +/- 15.38
Episode length: 27.40 +/- 4.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.4     |
|    mean_reward     | 2.39e+04 |
| time/              |          |
|    total_timesteps | 765000   |
| train/             |          |
|    actor_loss      | -5.03    |
|    critic_loss     | 0.0304   |
|    learning_rate   | 0.0003   |
|    n_updates       | 754999   |
---------------------------------
Eval num_timesteps=770000, episode_reward=23952.72 +/- 14.77
Episode length: 28.60 +/- 1.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 28.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 770000   |
| train/             |          |
|    actor_loss      | -4.73    |
|    critic_loss     | 0.0274   |
|    learning_rate   | 0.0003   |
|    n_updates       | 759999   |
---------------------------------
Eval num_timesteps=775000, episode_reward=23977.02 +/- 23.34
Episode length: 26.80 +/- 2.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 775000   |
| train/             |          |
|    actor_loss      | -4.97    |
|    critic_loss     | 0.0371   |
|    learning_rate   | 0.0003   |
|    n_updates       | 764999   |
---------------------------------
Eval num_timesteps=780000, episode_reward=23969.38 +/- 25.40
Episode length: 26.20 +/- 3.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 780000   |
| train/             |          |
|    actor_loss      | -5       |
|    critic_loss     | 0.0334   |
|    learning_rate   | 0.0003   |
|    n_updates       | 769999   |
---------------------------------
Eval num_timesteps=785000, episode_reward=23987.99 +/- 29.43
Episode length: 22.80 +/- 4.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 785000   |
| train/             |          |
|    actor_loss      | -5.17    |
|    critic_loss     | 0.0309   |
|    learning_rate   | 0.0003   |
|    n_updates       | 774999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 194      |
|    ep_rew_mean     | 2.15e+04 |
| time/              |          |
|    episodes        | 2000     |
|    fps             | 179      |
|    time_elapsed    | 4386     |
|    total_timesteps | 788893   |
| train/             |          |
|    actor_loss      | -5.43    |
|    critic_loss     | 0.0347   |
|    learning_rate   | 0.0003   |
|    n_updates       | 778892   |
---------------------------------
Eval num_timesteps=790000, episode_reward=23983.34 +/- 19.83
Episode length: 22.20 +/- 2.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 790000   |
| train/             |          |
|    actor_loss      | -5.2     |
|    critic_loss     | 0.0305   |
|    learning_rate   | 0.0003   |
|    n_updates       | 779999   |
---------------------------------
Eval num_timesteps=795000, episode_reward=23702.54 +/- 600.18
Episode length: 63.00 +/- 78.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 63       |
|    mean_reward     | 2.37e+04 |
| time/              |          |
|    total_timesteps | 795000   |
| train/             |          |
|    actor_loss      | -5.51    |
|    critic_loss     | 0.0265   |
|    learning_rate   | 0.0003   |
|    n_updates       | 784999   |
---------------------------------
Eval num_timesteps=800000, episode_reward=23954.20 +/- 53.57
Episode length: 28.00 +/- 11.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 28       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 800000   |
| train/             |          |
|    actor_loss      | -5.51    |
|    critic_loss     | 0.0496   |
|    learning_rate   | 0.0003   |
|    n_updates       | 789999   |
---------------------------------
Eval num_timesteps=805000, episode_reward=23963.07 +/- 39.93
Episode length: 26.20 +/- 12.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 805000   |
| train/             |          |
|    actor_loss      | -5.08    |
|    critic_loss     | 0.032    |
|    learning_rate   | 0.0003   |
|    n_updates       | 794999   |
---------------------------------
Eval num_timesteps=810000, episode_reward=23962.25 +/- 22.65
Episode length: 26.80 +/- 5.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 810000   |
| train/             |          |
|    actor_loss      | -5.4     |
|    critic_loss     | 0.0329   |
|    learning_rate   | 0.0003   |
|    n_updates       | 799999   |
---------------------------------
Eval num_timesteps=815000, episode_reward=23994.69 +/- 19.86
Episode length: 24.20 +/- 5.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 815000   |
| train/             |          |
|    actor_loss      | -5.13    |
|    critic_loss     | 0.026    |
|    learning_rate   | 0.0003   |
|    n_updates       | 804999   |
---------------------------------
Eval num_timesteps=820000, episode_reward=24297.40 +/- 601.65
Episode length: 26.60 +/- 2.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.6     |
|    mean_reward     | 2.43e+04 |
| time/              |          |
|    total_timesteps | 820000   |
| train/             |          |
|    actor_loss      | -5.34    |
|    critic_loss     | 0.0259   |
|    learning_rate   | 0.0003   |
|    n_updates       | 809999   |
---------------------------------
Eval num_timesteps=825000, episode_reward=23966.66 +/- 38.06
Episode length: 25.60 +/- 5.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 825000   |
| train/             |          |
|    actor_loss      | -5.5     |
|    critic_loss     | 0.0303   |
|    learning_rate   | 0.0003   |
|    n_updates       | 814999   |
---------------------------------
Eval num_timesteps=830000, episode_reward=23963.12 +/- 24.33
Episode length: 26.60 +/- 4.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 830000   |
| train/             |          |
|    actor_loss      | -5.1     |
|    critic_loss     | 0.02     |
|    learning_rate   | 0.0003   |
|    n_updates       | 819999   |
---------------------------------
Eval num_timesteps=835000, episode_reward=23998.01 +/- 18.38
Episode length: 25.00 +/- 3.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 835000   |
| train/             |          |
|    actor_loss      | -5.83    |
|    critic_loss     | 0.0482   |
|    learning_rate   | 0.0003   |
|    n_updates       | 824999   |
---------------------------------
Eval num_timesteps=840000, episode_reward=23984.35 +/- 21.59
Episode length: 26.00 +/- 5.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 840000   |
| train/             |          |
|    actor_loss      | -6.06    |
|    critic_loss     | 0.022    |
|    learning_rate   | 0.0003   |
|    n_updates       | 829999   |
---------------------------------
Eval num_timesteps=845000, episode_reward=23992.76 +/- 27.69
Episode length: 27.00 +/- 3.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 845000   |
| train/             |          |
|    actor_loss      | -5.98    |
|    critic_loss     | 0.0239   |
|    learning_rate   | 0.0003   |
|    n_updates       | 834999   |
---------------------------------
Eval num_timesteps=850000, episode_reward=23994.58 +/- 26.28
Episode length: 24.40 +/- 2.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 850000   |
| train/             |          |
|    actor_loss      | -6.23    |
|    critic_loss     | 0.033    |
|    learning_rate   | 0.0003   |
|    n_updates       | 839999   |
---------------------------------
Eval num_timesteps=855000, episode_reward=24290.26 +/- 591.18
Episode length: 25.80 +/- 4.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.8     |
|    mean_reward     | 2.43e+04 |
| time/              |          |
|    total_timesteps | 855000   |
| train/             |          |
|    actor_loss      | -6.19    |
|    critic_loss     | 0.028    |
|    learning_rate   | 0.0003   |
|    n_updates       | 844999   |
---------------------------------
Eval num_timesteps=860000, episode_reward=23990.74 +/- 28.63
Episode length: 23.80 +/- 9.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 860000   |
| train/             |          |
|    actor_loss      | -6.52    |
|    critic_loss     | 0.0287   |
|    learning_rate   | 0.0003   |
|    n_updates       | 849999   |
---------------------------------
Eval num_timesteps=865000, episode_reward=24301.71 +/- 590.12
Episode length: 23.60 +/- 3.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | 2.43e+04 |
| time/              |          |
|    total_timesteps | 865000   |
| train/             |          |
|    actor_loss      | -5.97    |
|    critic_loss     | 0.0268   |
|    learning_rate   | 0.0003   |
|    n_updates       | 854999   |
---------------------------------
Eval num_timesteps=870000, episode_reward=24010.40 +/- 25.51
Episode length: 23.60 +/- 4.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 870000   |
| train/             |          |
|    actor_loss      | -5.69    |
|    critic_loss     | 0.0333   |
|    learning_rate   | 0.0003   |
|    n_updates       | 859999   |
---------------------------------
Eval num_timesteps=875000, episode_reward=23990.35 +/- 18.53
Episode length: 25.40 +/- 3.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 875000   |
| train/             |          |
|    actor_loss      | -6.15    |
|    critic_loss     | 0.0271   |
|    learning_rate   | 0.0003   |
|    n_updates       | 864999   |
---------------------------------
Eval num_timesteps=880000, episode_reward=23995.86 +/- 16.50
Episode length: 25.80 +/- 3.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 880000   |
| train/             |          |
|    actor_loss      | -5.74    |
|    critic_loss     | 0.0289   |
|    learning_rate   | 0.0003   |
|    n_updates       | 869999   |
---------------------------------
Eval num_timesteps=885000, episode_reward=23986.09 +/- 32.77
Episode length: 24.00 +/- 6.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 885000   |
| train/             |          |
|    actor_loss      | -6.27    |
|    critic_loss     | 0.0329   |
|    learning_rate   | 0.0003   |
|    n_updates       | 874999   |
---------------------------------
Eval num_timesteps=890000, episode_reward=23982.99 +/- 5.63
Episode length: 25.40 +/- 1.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 890000   |
| train/             |          |
|    actor_loss      | -5.94    |
|    critic_loss     | 0.0353   |
|    learning_rate   | 0.0003   |
|    n_updates       | 879999   |
---------------------------------
Eval num_timesteps=895000, episode_reward=23999.91 +/- 42.51
Episode length: 21.00 +/- 5.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 895000   |
| train/             |          |
|    actor_loss      | -6.29    |
|    critic_loss     | 0.0342   |
|    learning_rate   | 0.0003   |
|    n_updates       | 884999   |
---------------------------------
Eval num_timesteps=900000, episode_reward=23943.51 +/- 16.81
Episode length: 28.40 +/- 2.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 28.4     |
|    mean_reward     | 2.39e+04 |
| time/              |          |
|    total_timesteps | 900000   |
| train/             |          |
|    actor_loss      | -6.3     |
|    critic_loss     | 0.0343   |
|    learning_rate   | 0.0003   |
|    n_updates       | 889999   |
---------------------------------
Eval num_timesteps=905000, episode_reward=23968.06 +/- 37.18
Episode length: 25.40 +/- 3.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 905000   |
| train/             |          |
|    actor_loss      | -6.04    |
|    critic_loss     | 0.0384   |
|    learning_rate   | 0.0003   |
|    n_updates       | 894999   |
---------------------------------
Eval num_timesteps=910000, episode_reward=24258.23 +/- 597.96
Episode length: 26.60 +/- 4.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.6     |
|    mean_reward     | 2.43e+04 |
| time/              |          |
|    total_timesteps | 910000   |
| train/             |          |
|    actor_loss      | -6.46    |
|    critic_loss     | 0.0254   |
|    learning_rate   | 0.0003   |
|    n_updates       | 899999   |
---------------------------------
Eval num_timesteps=915000, episode_reward=23963.68 +/- 32.26
Episode length: 27.20 +/- 4.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 915000   |
| train/             |          |
|    actor_loss      | -6.56    |
|    critic_loss     | 0.0248   |
|    learning_rate   | 0.0003   |
|    n_updates       | 904999   |
---------------------------------
Eval num_timesteps=920000, episode_reward=23992.21 +/- 27.02
Episode length: 20.60 +/- 5.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 920000   |
| train/             |          |
|    actor_loss      | -6.27    |
|    critic_loss     | 0.0247   |
|    learning_rate   | 0.0003   |
|    n_updates       | 909999   |
---------------------------------
Eval num_timesteps=925000, episode_reward=23978.82 +/- 17.63
Episode length: 24.20 +/- 2.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 925000   |
| train/             |          |
|    actor_loss      | -6.5     |
|    critic_loss     | 0.0221   |
|    learning_rate   | 0.0003   |
|    n_updates       | 914999   |
---------------------------------
Eval num_timesteps=930000, episode_reward=24298.46 +/- 613.85
Episode length: 24.20 +/- 2.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | 2.43e+04 |
| time/              |          |
|    total_timesteps | 930000   |
| train/             |          |
|    actor_loss      | -6.8     |
|    critic_loss     | 0.0246   |
|    learning_rate   | 0.0003   |
|    n_updates       | 919999   |
---------------------------------
Eval num_timesteps=935000, episode_reward=24001.04 +/- 13.93
Episode length: 23.40 +/- 3.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 935000   |
| train/             |          |
|    actor_loss      | -6.56    |
|    critic_loss     | 0.0241   |
|    learning_rate   | 0.0003   |
|    n_updates       | 924999   |
---------------------------------
Eval num_timesteps=940000, episode_reward=23995.87 +/- 23.20
Episode length: 21.40 +/- 3.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 940000   |
| train/             |          |
|    actor_loss      | -6.58    |
|    critic_loss     | 0.0309   |
|    learning_rate   | 0.0003   |
|    n_updates       | 929999   |
---------------------------------
Eval num_timesteps=945000, episode_reward=23978.93 +/- 13.68
Episode length: 25.20 +/- 2.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 945000   |
| train/             |          |
|    actor_loss      | -6.75    |
|    critic_loss     | 0.0216   |
|    learning_rate   | 0.0003   |
|    n_updates       | 934999   |
---------------------------------
Eval num_timesteps=950000, episode_reward=23954.38 +/- 58.26
Episode length: 30.80 +/- 8.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 950000   |
| train/             |          |
|    actor_loss      | -6.43    |
|    critic_loss     | 0.0211   |
|    learning_rate   | 0.0003   |
|    n_updates       | 939999   |
---------------------------------
Eval num_timesteps=955000, episode_reward=23978.66 +/- 14.26
Episode length: 25.20 +/- 3.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 955000   |
| train/             |          |
|    actor_loss      | -6.7     |
|    critic_loss     | 0.0176   |
|    learning_rate   | 0.0003   |
|    n_updates       | 944999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 173      |
|    ep_rew_mean     | 2.24e+04 |
| time/              |          |
|    episodes        | 3000     |
|    fps             | 182      |
|    time_elapsed    | 5242     |
|    total_timesteps | 958966   |
| train/             |          |
|    actor_loss      | -6.86    |
|    critic_loss     | 0.0294   |
|    learning_rate   | 0.0003   |
|    n_updates       | 948965   |
---------------------------------
Eval num_timesteps=960000, episode_reward=23951.03 +/- 18.69
Episode length: 31.40 +/- 3.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 960000   |
| train/             |          |
|    actor_loss      | -6.74    |
|    critic_loss     | 0.0274   |
|    learning_rate   | 0.0003   |
|    n_updates       | 949999   |
---------------------------------
Eval num_timesteps=965000, episode_reward=24007.80 +/- 15.85
Episode length: 22.80 +/- 5.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 965000   |
| train/             |          |
|    actor_loss      | -6.89    |
|    critic_loss     | 0.0201   |
|    learning_rate   | 0.0003   |
|    n_updates       | 954999   |
---------------------------------
Eval num_timesteps=970000, episode_reward=23984.56 +/- 20.75
Episode length: 24.00 +/- 3.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 970000   |
| train/             |          |
|    actor_loss      | -7.11    |
|    critic_loss     | 0.0312   |
|    learning_rate   | 0.0003   |
|    n_updates       | 959999   |
---------------------------------
Eval num_timesteps=975000, episode_reward=23973.17 +/- 19.46
Episode length: 27.20 +/- 3.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 975000   |
| train/             |          |
|    actor_loss      | -6.84    |
|    critic_loss     | 0.0336   |
|    learning_rate   | 0.0003   |
|    n_updates       | 964999   |
---------------------------------
Eval num_timesteps=980000, episode_reward=24278.09 +/- 595.88
Episode length: 24.40 +/- 2.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | 2.43e+04 |
| time/              |          |
|    total_timesteps | 980000   |
| train/             |          |
|    actor_loss      | -6.56    |
|    critic_loss     | 0.0366   |
|    learning_rate   | 0.0003   |
|    n_updates       | 969999   |
---------------------------------
Eval num_timesteps=985000, episode_reward=23984.67 +/- 29.50
Episode length: 24.60 +/- 6.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 985000   |
| train/             |          |
|    actor_loss      | -6.91    |
|    critic_loss     | 0.0187   |
|    learning_rate   | 0.0003   |
|    n_updates       | 974999   |
---------------------------------
Eval num_timesteps=990000, episode_reward=23991.70 +/- 34.39
Episode length: 22.80 +/- 6.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 990000   |
| train/             |          |
|    actor_loss      | -6.46    |
|    critic_loss     | 0.0201   |
|    learning_rate   | 0.0003   |
|    n_updates       | 979999   |
---------------------------------
Eval num_timesteps=995000, episode_reward=23998.94 +/- 32.67
Episode length: 24.00 +/- 6.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 995000   |
| train/             |          |
|    actor_loss      | -6.99    |
|    critic_loss     | 0.019    |
|    learning_rate   | 0.0003   |
|    n_updates       | 984999   |
---------------------------------
Eval num_timesteps=1000000, episode_reward=24005.54 +/- 32.74
Episode length: 19.20 +/- 4.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1000000  |
| train/             |          |
|    actor_loss      | -6.75    |
|    critic_loss     | 0.0404   |
|    learning_rate   | 0.0003   |
|    n_updates       | 989999   |
---------------------------------
Eval num_timesteps=1005000, episode_reward=24010.03 +/- 17.93
Episode length: 18.40 +/- 4.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1005000  |
| train/             |          |
|    actor_loss      | -7.15    |
|    critic_loss     | 0.0267   |
|    learning_rate   | 0.0003   |
|    n_updates       | 994999   |
---------------------------------
Eval num_timesteps=1010000, episode_reward=24003.72 +/- 14.56
Episode length: 22.80 +/- 2.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1010000  |
| train/             |          |
|    actor_loss      | -6.85    |
|    critic_loss     | 0.0258   |
|    learning_rate   | 0.0003   |
|    n_updates       | 999999   |
---------------------------------
Eval num_timesteps=1015000, episode_reward=23994.83 +/- 27.48
Episode length: 23.60 +/- 4.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1015000  |
| train/             |          |
|    actor_loss      | -6.8     |
|    critic_loss     | 0.0183   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1004999  |
---------------------------------
Eval num_timesteps=1020000, episode_reward=24007.56 +/- 10.11
Episode length: 20.20 +/- 1.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1020000  |
| train/             |          |
|    actor_loss      | -6.82    |
|    critic_loss     | 0.0205   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1009999  |
---------------------------------
Eval num_timesteps=1025000, episode_reward=23954.83 +/- 24.32
Episode length: 28.60 +/- 2.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 28.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1025000  |
| train/             |          |
|    actor_loss      | -6.51    |
|    critic_loss     | 0.0308   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1014999  |
---------------------------------
Eval num_timesteps=1030000, episode_reward=24010.82 +/- 18.36
Episode length: 21.80 +/- 5.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1030000  |
| train/             |          |
|    actor_loss      | -6.48    |
|    critic_loss     | 0.0299   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1019999  |
---------------------------------
Eval num_timesteps=1035000, episode_reward=24025.03 +/- 24.17
Episode length: 17.60 +/- 7.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1035000  |
| train/             |          |
|    actor_loss      | -6.85    |
|    critic_loss     | 0.0226   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1024999  |
---------------------------------
Eval num_timesteps=1040000, episode_reward=23995.23 +/- 24.66
Episode length: 23.60 +/- 5.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1040000  |
| train/             |          |
|    actor_loss      | -6.94    |
|    critic_loss     | 0.0398   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1029999  |
---------------------------------
Eval num_timesteps=1045000, episode_reward=23969.45 +/- 38.70
Episode length: 27.80 +/- 5.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1045000  |
| train/             |          |
|    actor_loss      | -7.12    |
|    critic_loss     | 0.022    |
|    learning_rate   | 0.0003   |
|    n_updates       | 1034999  |
---------------------------------
Eval num_timesteps=1050000, episode_reward=23993.33 +/- 23.61
Episode length: 24.00 +/- 7.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1050000  |
| train/             |          |
|    actor_loss      | -6.85    |
|    critic_loss     | 0.0316   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1039999  |
---------------------------------
Eval num_timesteps=1055000, episode_reward=23997.97 +/- 24.69
Episode length: 20.80 +/- 6.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1055000  |
| train/             |          |
|    actor_loss      | -6.86    |
|    critic_loss     | 0.02     |
|    learning_rate   | 0.0003   |
|    n_updates       | 1044999  |
---------------------------------
Eval num_timesteps=1060000, episode_reward=24010.39 +/- 26.29
Episode length: 17.60 +/- 6.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1060000  |
| train/             |          |
|    actor_loss      | -6.94    |
|    critic_loss     | 0.0239   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1049999  |
---------------------------------
Eval num_timesteps=1065000, episode_reward=24298.01 +/- 611.54
Episode length: 24.40 +/- 3.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | 2.43e+04 |
| time/              |          |
|    total_timesteps | 1065000  |
| train/             |          |
|    actor_loss      | -7.08    |
|    critic_loss     | 0.0209   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1054999  |
---------------------------------
Eval num_timesteps=1070000, episode_reward=23977.77 +/- 18.85
Episode length: 26.40 +/- 3.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1070000  |
| train/             |          |
|    actor_loss      | -6.68    |
|    critic_loss     | 0.021    |
|    learning_rate   | 0.0003   |
|    n_updates       | 1059999  |
---------------------------------
Eval num_timesteps=1075000, episode_reward=23984.78 +/- 20.13
Episode length: 28.40 +/- 3.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 28.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1075000  |
| train/             |          |
|    actor_loss      | -7.14    |
|    critic_loss     | 0.02     |
|    learning_rate   | 0.0003   |
|    n_updates       | 1064999  |
---------------------------------
Eval num_timesteps=1080000, episode_reward=23978.95 +/- 26.04
Episode length: 25.20 +/- 3.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1080000  |
| train/             |          |
|    actor_loss      | -7.45    |
|    critic_loss     | 0.0215   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1069999  |
---------------------------------
Eval num_timesteps=1085000, episode_reward=24006.58 +/- 35.87
Episode length: 18.60 +/- 6.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1085000  |
| train/             |          |
|    actor_loss      | -7.3     |
|    critic_loss     | 0.0255   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1074999  |
---------------------------------
Eval num_timesteps=1090000, episode_reward=24281.43 +/- 591.54
Episode length: 25.40 +/- 3.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.4     |
|    mean_reward     | 2.43e+04 |
| time/              |          |
|    total_timesteps | 1090000  |
| train/             |          |
|    actor_loss      | -7.22    |
|    critic_loss     | 0.0156   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1079999  |
---------------------------------
Eval num_timesteps=1095000, episode_reward=24000.20 +/- 25.39
Episode length: 24.00 +/- 4.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1095000  |
| train/             |          |
|    actor_loss      | -7.2     |
|    critic_loss     | 0.0348   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1084999  |
---------------------------------
Eval num_timesteps=1100000, episode_reward=23993.65 +/- 22.08
Episode length: 23.20 +/- 2.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1100000  |
| train/             |          |
|    actor_loss      | -7.08    |
|    critic_loss     | 0.0214   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1089999  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 145      |
|    ep_rew_mean     | 2.21e+04 |
| time/              |          |
|    episodes        | 4000     |
|    fps             | 184      |
|    time_elapsed    | 5972     |
|    total_timesteps | 1104260  |
| train/             |          |
|    actor_loss      | -7.08    |
|    critic_loss     | 0.0263   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1094259  |
---------------------------------
Eval num_timesteps=1105000, episode_reward=23987.24 +/- 17.92
Episode length: 26.20 +/- 3.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1105000  |
| train/             |          |
|    actor_loss      | -7.01    |
|    critic_loss     | 0.0301   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1094999  |
---------------------------------
Eval num_timesteps=1110000, episode_reward=23987.14 +/- 20.73
Episode length: 26.80 +/- 2.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1110000  |
| train/             |          |
|    actor_loss      | -7.37    |
|    critic_loss     | 0.0195   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1099999  |
---------------------------------
Eval num_timesteps=1115000, episode_reward=23982.55 +/- 38.31
Episode length: 25.20 +/- 4.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1115000  |
| train/             |          |
|    actor_loss      | -7.56    |
|    critic_loss     | 0.0219   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1104999  |
---------------------------------
Eval num_timesteps=1120000, episode_reward=23997.57 +/- 13.97
Episode length: 25.60 +/- 3.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1120000  |
| train/             |          |
|    actor_loss      | -7.35    |
|    critic_loss     | 0.0358   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1109999  |
---------------------------------
Eval num_timesteps=1125000, episode_reward=23983.91 +/- 32.50
Episode length: 25.80 +/- 3.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1125000  |
| train/             |          |
|    actor_loss      | -7.43    |
|    critic_loss     | 0.0131   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1114999  |
---------------------------------
Eval num_timesteps=1130000, episode_reward=23974.80 +/- 28.08
Episode length: 27.00 +/- 4.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1130000  |
| train/             |          |
|    actor_loss      | -7.54    |
|    critic_loss     | 0.0237   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1119999  |
---------------------------------
Eval num_timesteps=1135000, episode_reward=23988.68 +/- 12.19
Episode length: 25.20 +/- 3.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1135000  |
| train/             |          |
|    actor_loss      | -7.04    |
|    critic_loss     | 0.0181   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1124999  |
---------------------------------
Eval num_timesteps=1140000, episode_reward=23991.37 +/- 18.04
Episode length: 24.00 +/- 2.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1140000  |
| train/             |          |
|    actor_loss      | -7       |
|    critic_loss     | 0.0228   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1129999  |
---------------------------------
Eval num_timesteps=1145000, episode_reward=23984.72 +/- 11.18
Episode length: 24.40 +/- 2.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1145000  |
| train/             |          |
|    actor_loss      | -7.54    |
|    critic_loss     | 0.0181   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1134999  |
---------------------------------
Eval num_timesteps=1150000, episode_reward=23993.78 +/- 21.76
Episode length: 25.00 +/- 3.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1150000  |
| train/             |          |
|    actor_loss      | -7.39    |
|    critic_loss     | 0.0376   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1139999  |
---------------------------------
Eval num_timesteps=1155000, episode_reward=23991.48 +/- 16.82
Episode length: 23.40 +/- 2.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1155000  |
| train/             |          |
|    actor_loss      | -7.29    |
|    critic_loss     | 0.0221   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1144999  |
---------------------------------
Eval num_timesteps=1160000, episode_reward=23995.03 +/- 26.21
Episode length: 24.20 +/- 4.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1160000  |
| train/             |          |
|    actor_loss      | -7.65    |
|    critic_loss     | 0.0156   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1149999  |
---------------------------------
Eval num_timesteps=1165000, episode_reward=24006.69 +/- 23.53
Episode length: 24.20 +/- 4.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1165000  |
| train/             |          |
|    actor_loss      | -7.27    |
|    critic_loss     | 0.0161   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1154999  |
---------------------------------
Eval num_timesteps=1170000, episode_reward=23971.00 +/- 28.23
Episode length: 28.40 +/- 3.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 28.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1170000  |
| train/             |          |
|    actor_loss      | -7.6     |
|    critic_loss     | 0.0164   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1159999  |
---------------------------------
Eval num_timesteps=1175000, episode_reward=23956.27 +/- 18.10
Episode length: 29.60 +/- 2.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1175000  |
| train/             |          |
|    actor_loss      | -7.39    |
|    critic_loss     | 0.0322   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1164999  |
---------------------------------
Eval num_timesteps=1180000, episode_reward=23975.48 +/- 19.12
Episode length: 28.00 +/- 4.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 28       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1180000  |
| train/             |          |
|    actor_loss      | -7.43    |
|    critic_loss     | 0.0145   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1169999  |
---------------------------------
Eval num_timesteps=1185000, episode_reward=23990.90 +/- 33.90
Episode length: 27.60 +/- 4.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1185000  |
| train/             |          |
|    actor_loss      | -7.76    |
|    critic_loss     | 0.0255   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1174999  |
---------------------------------
Eval num_timesteps=1190000, episode_reward=23998.96 +/- 26.34
Episode length: 21.60 +/- 6.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1190000  |
| train/             |          |
|    actor_loss      | -7.54    |
|    critic_loss     | 0.0188   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1179999  |
---------------------------------
Eval num_timesteps=1195000, episode_reward=23984.42 +/- 48.37
Episode length: 28.00 +/- 9.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 28       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1195000  |
| train/             |          |
|    actor_loss      | -7.27    |
|    critic_loss     | 0.0186   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1184999  |
---------------------------------
Eval num_timesteps=1200000, episode_reward=24002.48 +/- 18.43
Episode length: 23.40 +/- 3.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1200000  |
| train/             |          |
|    actor_loss      | -7.47    |
|    critic_loss     | 0.018    |
|    learning_rate   | 0.0003   |
|    n_updates       | 1189999  |
---------------------------------
Eval num_timesteps=1205000, episode_reward=24009.03 +/- 16.00
Episode length: 23.80 +/- 4.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1205000  |
| train/             |          |
|    actor_loss      | -7.48    |
|    critic_loss     | 0.0266   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1194999  |
---------------------------------
Eval num_timesteps=1210000, episode_reward=23992.04 +/- 38.73
Episode length: 23.40 +/- 7.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1210000  |
| train/             |          |
|    actor_loss      | -7.93    |
|    critic_loss     | 0.0164   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1199999  |
---------------------------------
Eval num_timesteps=1215000, episode_reward=24016.11 +/- 9.97
Episode length: 23.00 +/- 3.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1215000  |
| train/             |          |
|    actor_loss      | -7.56    |
|    critic_loss     | 0.0326   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1204999  |
---------------------------------
Eval num_timesteps=1220000, episode_reward=23993.65 +/- 18.94
Episode length: 26.00 +/- 3.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1220000  |
| train/             |          |
|    actor_loss      | -7.48    |
|    critic_loss     | 0.0112   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1209999  |
---------------------------------
Eval num_timesteps=1225000, episode_reward=24007.48 +/- 14.87
Episode length: 24.20 +/- 4.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1225000  |
| train/             |          |
|    actor_loss      | -7.87    |
|    critic_loss     | 0.141    |
|    learning_rate   | 0.0003   |
|    n_updates       | 1214999  |
---------------------------------
Eval num_timesteps=1230000, episode_reward=23998.46 +/- 16.04
Episode length: 25.80 +/- 2.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1230000  |
| train/             |          |
|    actor_loss      | -7.72    |
|    critic_loss     | 0.0242   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1219999  |
---------------------------------
Eval num_timesteps=1235000, episode_reward=23988.66 +/- 24.00
Episode length: 27.20 +/- 2.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1235000  |
| train/             |          |
|    actor_loss      | -7.54    |
|    critic_loss     | 0.0257   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1224999  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 118      |
|    ep_rew_mean     | 2.27e+04 |
| time/              |          |
|    episodes        | 5000     |
|    fps             | 186      |
|    time_elapsed    | 6632     |
|    total_timesteps | 1235186  |
| train/             |          |
|    actor_loss      | -7.42    |
|    critic_loss     | 0.0147   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1225185  |
---------------------------------
Eval num_timesteps=1240000, episode_reward=24011.62 +/- 25.58
Episode length: 21.00 +/- 5.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1240000  |
| train/             |          |
|    actor_loss      | -7.63    |
|    critic_loss     | 0.0117   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1229999  |
---------------------------------
Eval num_timesteps=1245000, episode_reward=23990.08 +/- 12.27
Episode length: 28.00 +/- 2.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 28       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1245000  |
| train/             |          |
|    actor_loss      | -7.76    |
|    critic_loss     | 0.0192   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1234999  |
---------------------------------
Eval num_timesteps=1250000, episode_reward=24009.75 +/- 7.49
Episode length: 22.80 +/- 0.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1250000  |
| train/             |          |
|    actor_loss      | -7.33    |
|    critic_loss     | 0.024    |
|    learning_rate   | 0.0003   |
|    n_updates       | 1239999  |
---------------------------------
Eval num_timesteps=1255000, episode_reward=24009.56 +/- 15.35
Episode length: 24.40 +/- 3.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1255000  |
| train/             |          |
|    actor_loss      | -7.64    |
|    critic_loss     | 0.0158   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1244999  |
---------------------------------
Eval num_timesteps=1260000, episode_reward=24016.94 +/- 20.17
Episode length: 22.40 +/- 3.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1260000  |
| train/             |          |
|    actor_loss      | -7.51    |
|    critic_loss     | 0.0171   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1249999  |
---------------------------------
Eval num_timesteps=1265000, episode_reward=23980.43 +/- 15.90
Episode length: 25.20 +/- 3.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1265000  |
| train/             |          |
|    actor_loss      | -7.61    |
|    critic_loss     | 0.0157   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1254999  |
---------------------------------
Eval num_timesteps=1270000, episode_reward=23984.58 +/- 21.86
Episode length: 30.20 +/- 5.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1270000  |
| train/             |          |
|    actor_loss      | -7.63    |
|    critic_loss     | 0.0178   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1259999  |
---------------------------------
Eval num_timesteps=1275000, episode_reward=24000.28 +/- 26.03
Episode length: 22.00 +/- 4.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1275000  |
| train/             |          |
|    actor_loss      | -7.69    |
|    critic_loss     | 0.0128   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1264999  |
---------------------------------
Eval num_timesteps=1280000, episode_reward=23989.20 +/- 29.29
Episode length: 23.40 +/- 3.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1280000  |
| train/             |          |
|    actor_loss      | -7.34    |
|    critic_loss     | 0.0459   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1269999  |
---------------------------------
Eval num_timesteps=1285000, episode_reward=23996.09 +/- 21.76
Episode length: 22.20 +/- 2.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1285000  |
| train/             |          |
|    actor_loss      | -7.69    |
|    critic_loss     | 0.0172   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1274999  |
---------------------------------
Eval num_timesteps=1290000, episode_reward=24006.02 +/- 24.59
Episode length: 24.20 +/- 4.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1290000  |
| train/             |          |
|    actor_loss      | -7.59    |
|    critic_loss     | 0.0128   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1279999  |
---------------------------------
Eval num_timesteps=1295000, episode_reward=24002.73 +/- 27.44
Episode length: 22.80 +/- 6.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1295000  |
| train/             |          |
|    actor_loss      | -7.68    |
|    critic_loss     | 0.0137   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1284999  |
---------------------------------
Eval num_timesteps=1300000, episode_reward=24010.90 +/- 8.10
Episode length: 23.60 +/- 2.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1300000  |
| train/             |          |
|    actor_loss      | -7.3     |
|    critic_loss     | 0.0209   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1289999  |
---------------------------------
Eval num_timesteps=1305000, episode_reward=23999.80 +/- 13.34
Episode length: 25.00 +/- 3.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1305000  |
| train/             |          |
|    actor_loss      | -7.81    |
|    critic_loss     | 0.024    |
|    learning_rate   | 0.0003   |
|    n_updates       | 1294999  |
---------------------------------
Eval num_timesteps=1310000, episode_reward=23987.06 +/- 38.92
Episode length: 21.40 +/- 5.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1310000  |
| train/             |          |
|    actor_loss      | -7.68    |
|    critic_loss     | 0.0181   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1299999  |
---------------------------------
Eval num_timesteps=1315000, episode_reward=24022.67 +/- 32.19
Episode length: 18.20 +/- 7.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1315000  |
| train/             |          |
|    actor_loss      | -7.44    |
|    critic_loss     | 0.053    |
|    learning_rate   | 0.0003   |
|    n_updates       | 1304999  |
---------------------------------
Eval num_timesteps=1320000, episode_reward=24008.00 +/- 17.23
Episode length: 22.00 +/- 2.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1320000  |
| train/             |          |
|    actor_loss      | -7.76    |
|    critic_loss     | 0.0275   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1309999  |
---------------------------------
Eval num_timesteps=1325000, episode_reward=23995.12 +/- 18.70
Episode length: 23.20 +/- 5.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1325000  |
| train/             |          |
|    actor_loss      | -7.87    |
|    critic_loss     | 0.013    |
|    learning_rate   | 0.0003   |
|    n_updates       | 1314999  |
---------------------------------
Eval num_timesteps=1330000, episode_reward=23982.95 +/- 19.32
Episode length: 24.20 +/- 5.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1330000  |
| train/             |          |
|    actor_loss      | -7.68    |
|    critic_loss     | 0.0108   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1319999  |
---------------------------------
Eval num_timesteps=1335000, episode_reward=23979.61 +/- 22.46
Episode length: 26.60 +/- 2.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1335000  |
| train/             |          |
|    actor_loss      | -7.6     |
|    critic_loss     | 0.0177   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1324999  |
---------------------------------
Eval num_timesteps=1340000, episode_reward=23995.93 +/- 8.74
Episode length: 24.20 +/- 2.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1340000  |
| train/             |          |
|    actor_loss      | -7.82    |
|    critic_loss     | 0.0144   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1329999  |
---------------------------------
Eval num_timesteps=1345000, episode_reward=24010.34 +/- 27.95
Episode length: 19.60 +/- 6.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1345000  |
| train/             |          |
|    actor_loss      | -7.67    |
|    critic_loss     | 0.0123   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1334999  |
---------------------------------
Eval num_timesteps=1350000, episode_reward=23984.74 +/- 16.66
Episode length: 23.20 +/- 3.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1350000  |
| train/             |          |
|    actor_loss      | -7.57    |
|    critic_loss     | 0.0326   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1339999  |
---------------------------------
Eval num_timesteps=1355000, episode_reward=24001.55 +/- 15.64
Episode length: 19.40 +/- 3.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1355000  |
| train/             |          |
|    actor_loss      | -7.88    |
|    critic_loss     | 0.013    |
|    learning_rate   | 0.0003   |
|    n_updates       | 1344999  |
---------------------------------
Eval num_timesteps=1360000, episode_reward=23999.30 +/- 25.55
Episode length: 21.40 +/- 5.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1360000  |
| train/             |          |
|    actor_loss      | -7.99    |
|    critic_loss     | 0.0106   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1349999  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 125      |
|    ep_rew_mean     | 2.28e+04 |
| time/              |          |
|    episodes        | 6000     |
|    fps             | 187      |
|    time_elapsed    | 7269     |
|    total_timesteps | 1362904  |
| train/             |          |
|    actor_loss      | -7.8     |
|    critic_loss     | 0.0174   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1352903  |
---------------------------------
Eval num_timesteps=1365000, episode_reward=24007.98 +/- 13.47
Episode length: 22.40 +/- 3.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1365000  |
| train/             |          |
|    actor_loss      | -7.71    |
|    critic_loss     | 0.016    |
|    learning_rate   | 0.0003   |
|    n_updates       | 1354999  |
---------------------------------
Eval num_timesteps=1370000, episode_reward=23993.56 +/- 20.52
Episode length: 24.40 +/- 5.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1370000  |
| train/             |          |
|    actor_loss      | -7.84    |
|    critic_loss     | 0.0214   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1359999  |
---------------------------------
Eval num_timesteps=1375000, episode_reward=23986.29 +/- 22.48
Episode length: 21.20 +/- 3.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1375000  |
| train/             |          |
|    actor_loss      | -7.69    |
|    critic_loss     | 0.0119   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1364999  |
---------------------------------
Eval num_timesteps=1380000, episode_reward=23974.91 +/- 26.62
Episode length: 25.80 +/- 2.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1380000  |
| train/             |          |
|    actor_loss      | -7.86    |
|    critic_loss     | 0.0163   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1369999  |
---------------------------------
Eval num_timesteps=1385000, episode_reward=23975.72 +/- 56.42
Episode length: 28.60 +/- 14.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 28.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1385000  |
| train/             |          |
|    actor_loss      | -7.85    |
|    critic_loss     | 0.0146   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1374999  |
---------------------------------
Eval num_timesteps=1390000, episode_reward=23989.24 +/- 31.16
Episode length: 21.20 +/- 5.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1390000  |
| train/             |          |
|    actor_loss      | -8.15    |
|    critic_loss     | 0.0131   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1379999  |
---------------------------------
Eval num_timesteps=1395000, episode_reward=24006.64 +/- 28.12
Episode length: 21.80 +/- 5.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1395000  |
| train/             |          |
|    actor_loss      | -7.89    |
|    critic_loss     | 0.0171   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1384999  |
---------------------------------
Eval num_timesteps=1400000, episode_reward=24003.99 +/- 24.98
Episode length: 21.20 +/- 3.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1400000  |
| train/             |          |
|    actor_loss      | -7.77    |
|    critic_loss     | 0.0147   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1389999  |
---------------------------------
Eval num_timesteps=1405000, episode_reward=24002.44 +/- 25.31
Episode length: 24.00 +/- 1.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1405000  |
| train/             |          |
|    actor_loss      | -7.55    |
|    critic_loss     | 0.0157   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1394999  |
---------------------------------
Eval num_timesteps=1410000, episode_reward=24299.47 +/- 607.13
Episode length: 24.00 +/- 4.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | 2.43e+04 |
| time/              |          |
|    total_timesteps | 1410000  |
| train/             |          |
|    actor_loss      | -8.11    |
|    critic_loss     | 0.0158   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1399999  |
---------------------------------
Eval num_timesteps=1415000, episode_reward=24003.15 +/- 26.24
Episode length: 20.80 +/- 6.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1415000  |
| train/             |          |
|    actor_loss      | -7.74    |
|    critic_loss     | 0.0248   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1404999  |
---------------------------------
Eval num_timesteps=1420000, episode_reward=23997.74 +/- 24.83
Episode length: 22.60 +/- 4.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1420000  |
| train/             |          |
|    actor_loss      | -7.96    |
|    critic_loss     | 0.0125   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1409999  |
---------------------------------
Eval num_timesteps=1425000, episode_reward=24020.82 +/- 10.77
Episode length: 18.20 +/- 4.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1425000  |
| train/             |          |
|    actor_loss      | -7.7     |
|    critic_loss     | 0.0151   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1414999  |
---------------------------------
Eval num_timesteps=1430000, episode_reward=23993.14 +/- 30.29
Episode length: 21.80 +/- 3.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1430000  |
| train/             |          |
|    actor_loss      | -7.87    |
|    critic_loss     | 0.013    |
|    learning_rate   | 0.0003   |
|    n_updates       | 1419999  |
---------------------------------
Eval num_timesteps=1435000, episode_reward=23991.17 +/- 20.09
Episode length: 22.40 +/- 3.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1435000  |
| train/             |          |
|    actor_loss      | -7.58    |
|    critic_loss     | 0.0174   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1424999  |
---------------------------------
Eval num_timesteps=1440000, episode_reward=23989.18 +/- 36.61
Episode length: 23.00 +/- 2.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1440000  |
| train/             |          |
|    actor_loss      | -8.18    |
|    critic_loss     | 0.0139   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1429999  |
---------------------------------
Eval num_timesteps=1445000, episode_reward=24001.41 +/- 8.42
Episode length: 24.00 +/- 3.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1445000  |
| train/             |          |
|    actor_loss      | -7.56    |
|    critic_loss     | 0.0213   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1434999  |
---------------------------------
Eval num_timesteps=1450000, episode_reward=23988.56 +/- 23.75
Episode length: 21.20 +/- 2.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1450000  |
| train/             |          |
|    actor_loss      | -7.83    |
|    critic_loss     | 0.0133   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1439999  |
---------------------------------
Eval num_timesteps=1455000, episode_reward=24008.14 +/- 13.64
Episode length: 20.00 +/- 1.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1455000  |
| train/             |          |
|    actor_loss      | -7.8     |
|    critic_loss     | 0.0125   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1444999  |
---------------------------------
Eval num_timesteps=1460000, episode_reward=24297.19 +/- 600.59
Episode length: 22.80 +/- 3.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | 2.43e+04 |
| time/              |          |
|    total_timesteps | 1460000  |
| train/             |          |
|    actor_loss      | -7.91    |
|    critic_loss     | 0.0118   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1449999  |
---------------------------------
Eval num_timesteps=1465000, episode_reward=23998.32 +/- 26.44
Episode length: 21.00 +/- 6.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1465000  |
| train/             |          |
|    actor_loss      | -8.1     |
|    critic_loss     | 0.0173   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1454999  |
---------------------------------
Eval num_timesteps=1470000, episode_reward=24002.75 +/- 25.51
Episode length: 22.60 +/- 3.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1470000  |
| train/             |          |
|    actor_loss      | -7.6     |
|    critic_loss     | 0.0216   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1459999  |
---------------------------------
Eval num_timesteps=1475000, episode_reward=23991.59 +/- 13.17
Episode length: 23.60 +/- 3.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1475000  |
| train/             |          |
|    actor_loss      | -7.85    |
|    critic_loss     | 0.0168   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1464999  |
---------------------------------
Eval num_timesteps=1480000, episode_reward=24018.64 +/- 9.73
Episode length: 19.40 +/- 3.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1480000  |
| train/             |          |
|    actor_loss      | -7.88    |
|    critic_loss     | 0.0172   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1469999  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 136      |
|    ep_rew_mean     | 2.31e+04 |
| time/              |          |
|    episodes        | 7000     |
|    fps             | 188      |
|    time_elapsed    | 7876     |
|    total_timesteps | 1484902  |
| train/             |          |
|    actor_loss      | -7.9     |
|    critic_loss     | 0.0113   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1474901  |
---------------------------------
Eval num_timesteps=1485000, episode_reward=23987.88 +/- 25.25
Episode length: 25.20 +/- 2.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1485000  |
| train/             |          |
|    actor_loss      | -7.87    |
|    critic_loss     | 0.0148   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1474999  |
---------------------------------
Eval num_timesteps=1490000, episode_reward=23990.30 +/- 16.69
Episode length: 23.20 +/- 3.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1490000  |
| train/             |          |
|    actor_loss      | -7.7     |
|    critic_loss     | 0.021    |
|    learning_rate   | 0.0003   |
|    n_updates       | 1479999  |
---------------------------------
Eval num_timesteps=1495000, episode_reward=23989.06 +/- 29.16
Episode length: 23.20 +/- 4.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1495000  |
| train/             |          |
|    actor_loss      | -7.84    |
|    critic_loss     | 0.0231   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1484999  |
---------------------------------
Eval num_timesteps=1500000, episode_reward=24013.69 +/- 31.43
Episode length: 19.40 +/- 7.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1500000  |
| train/             |          |
|    actor_loss      | -8.01    |
|    critic_loss     | 0.0204   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1489999  |
---------------------------------
Eval num_timesteps=1505000, episode_reward=23988.04 +/- 39.80
Episode length: 21.80 +/- 7.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1505000  |
| train/             |          |
|    actor_loss      | -7.87    |
|    critic_loss     | 0.0162   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1494999  |
---------------------------------
Eval num_timesteps=1510000, episode_reward=23989.13 +/- 12.49
Episode length: 25.20 +/- 2.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1510000  |
| train/             |          |
|    actor_loss      | -7.87    |
|    critic_loss     | 0.0127   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1499999  |
---------------------------------
Eval num_timesteps=1515000, episode_reward=23999.62 +/- 23.32
Episode length: 23.60 +/- 2.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1515000  |
| train/             |          |
|    actor_loss      | -7.89    |
|    critic_loss     | 0.0143   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1504999  |
---------------------------------
Eval num_timesteps=1520000, episode_reward=23993.11 +/- 17.59
Episode length: 24.20 +/- 1.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1520000  |
| train/             |          |
|    actor_loss      | -7.78    |
|    critic_loss     | 0.013    |
|    learning_rate   | 0.0003   |
|    n_updates       | 1509999  |
---------------------------------
Eval num_timesteps=1525000, episode_reward=23996.28 +/- 19.40
Episode length: 22.80 +/- 3.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1525000  |
| train/             |          |
|    actor_loss      | -8.15    |
|    critic_loss     | 0.014    |
|    learning_rate   | 0.0003   |
|    n_updates       | 1514999  |
---------------------------------
Eval num_timesteps=1530000, episode_reward=23988.83 +/- 24.73
Episode length: 22.60 +/- 2.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1530000  |
| train/             |          |
|    actor_loss      | -8.12    |
|    critic_loss     | 0.0156   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1519999  |
---------------------------------
Eval num_timesteps=1535000, episode_reward=23987.31 +/- 23.85
Episode length: 24.80 +/- 4.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1535000  |
| train/             |          |
|    actor_loss      | -8.06    |
|    critic_loss     | 0.0133   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1524999  |
---------------------------------
Eval num_timesteps=1540000, episode_reward=23988.52 +/- 10.81
Episode length: 22.40 +/- 4.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1540000  |
| train/             |          |
|    actor_loss      | -8.13    |
|    critic_loss     | 0.0151   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1529999  |
---------------------------------
Eval num_timesteps=1545000, episode_reward=24009.44 +/- 21.14
Episode length: 21.20 +/- 5.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1545000  |
| train/             |          |
|    actor_loss      | -7.7     |
|    critic_loss     | 0.0146   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1534999  |
---------------------------------
Eval num_timesteps=1550000, episode_reward=24011.13 +/- 28.75
Episode length: 19.00 +/- 6.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1550000  |
| train/             |          |
|    actor_loss      | -7.82    |
|    critic_loss     | 0.0273   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1539999  |
---------------------------------
Eval num_timesteps=1555000, episode_reward=23982.01 +/- 20.15
Episode length: 23.20 +/- 3.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1555000  |
| train/             |          |
|    actor_loss      | -7.88    |
|    critic_loss     | 0.0108   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1544999  |
---------------------------------
Eval num_timesteps=1560000, episode_reward=24015.34 +/- 17.85
Episode length: 19.00 +/- 4.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1560000  |
| train/             |          |
|    actor_loss      | -7.78    |
|    critic_loss     | 0.0146   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1549999  |
---------------------------------
Eval num_timesteps=1565000, episode_reward=24327.19 +/- 590.62
Episode length: 16.60 +/- 6.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | 2.43e+04 |
| time/              |          |
|    total_timesteps | 1565000  |
| train/             |          |
|    actor_loss      | -7.88    |
|    critic_loss     | 0.0143   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1554999  |
---------------------------------
Eval num_timesteps=1570000, episode_reward=23990.92 +/- 8.44
Episode length: 23.40 +/- 1.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1570000  |
| train/             |          |
|    actor_loss      | -8.06    |
|    critic_loss     | 0.0226   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1559999  |
---------------------------------
Eval num_timesteps=1575000, episode_reward=24004.89 +/- 32.19
Episode length: 21.60 +/- 5.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1575000  |
| train/             |          |
|    actor_loss      | -7.8     |
|    critic_loss     | 0.0194   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1564999  |
---------------------------------
Eval num_timesteps=1580000, episode_reward=23985.04 +/- 13.07
Episode length: 24.60 +/- 4.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1580000  |
| train/             |          |
|    actor_loss      | -7.76    |
|    critic_loss     | 0.0162   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1569999  |
---------------------------------
Eval num_timesteps=1585000, episode_reward=24001.30 +/- 21.99
Episode length: 23.60 +/- 4.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1585000  |
| train/             |          |
|    actor_loss      | -7.71    |
|    critic_loss     | 0.0128   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1574999  |
---------------------------------
Eval num_timesteps=1590000, episode_reward=24008.80 +/- 17.89
Episode length: 21.60 +/- 5.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1590000  |
| train/             |          |
|    actor_loss      | -8.03    |
|    critic_loss     | 0.0155   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1579999  |
---------------------------------
Eval num_timesteps=1595000, episode_reward=23987.74 +/- 14.12
Episode length: 27.60 +/- 1.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1595000  |
| train/             |          |
|    actor_loss      | -8.11    |
|    critic_loss     | 0.0149   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1584999  |
---------------------------------
Eval num_timesteps=1600000, episode_reward=23997.58 +/- 16.41
Episode length: 24.20 +/- 3.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1600000  |
| train/             |          |
|    actor_loss      | -7.99    |
|    critic_loss     | 0.0142   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1589999  |
---------------------------------
Eval num_timesteps=1605000, episode_reward=24323.86 +/- 615.23
Episode length: 17.60 +/- 5.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.6     |
|    mean_reward     | 2.43e+04 |
| time/              |          |
|    total_timesteps | 1605000  |
| train/             |          |
|    actor_loss      | -7.73    |
|    critic_loss     | 0.015    |
|    learning_rate   | 0.0003   |
|    n_updates       | 1594999  |
---------------------------------
Eval num_timesteps=1610000, episode_reward=24003.66 +/- 24.69
Episode length: 22.60 +/- 5.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1610000  |
| train/             |          |
|    actor_loss      | -8.24    |
|    critic_loss     | 0.0117   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1599999  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 119      |
|    ep_rew_mean     | 2.25e+04 |
| time/              |          |
|    episodes        | 8000     |
|    fps             | 189      |
|    time_elapsed    | 8507     |
|    total_timesteps | 1610276  |
| train/             |          |
|    actor_loss      | -7.9     |
|    critic_loss     | 0.0143   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1600275  |
---------------------------------
Eval num_timesteps=1615000, episode_reward=23997.98 +/- 27.70
Episode length: 23.20 +/- 6.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1615000  |
| train/             |          |
|    actor_loss      | -7.98    |
|    critic_loss     | 0.015    |
|    learning_rate   | 0.0003   |
|    n_updates       | 1604999  |
---------------------------------
Eval num_timesteps=1620000, episode_reward=24012.97 +/- 12.27
Episode length: 22.20 +/- 3.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1620000  |
| train/             |          |
|    actor_loss      | -8.03    |
|    critic_loss     | 0.0198   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1609999  |
---------------------------------
Eval num_timesteps=1625000, episode_reward=24026.56 +/- 15.53
Episode length: 17.20 +/- 5.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1625000  |
| train/             |          |
|    actor_loss      | -7.91    |
|    critic_loss     | 0.0127   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1614999  |
---------------------------------
Eval num_timesteps=1630000, episode_reward=24013.50 +/- 17.53
Episode length: 19.00 +/- 4.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1630000  |
| train/             |          |
|    actor_loss      | -8.23    |
|    critic_loss     | 0.022    |
|    learning_rate   | 0.0003   |
|    n_updates       | 1619999  |
---------------------------------
Eval num_timesteps=1635000, episode_reward=24000.58 +/- 21.86
Episode length: 23.60 +/- 3.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1635000  |
| train/             |          |
|    actor_loss      | -8.1     |
|    critic_loss     | 0.0181   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1624999  |
---------------------------------
Eval num_timesteps=1640000, episode_reward=24006.53 +/- 10.28
Episode length: 23.20 +/- 2.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1640000  |
| train/             |          |
|    actor_loss      | -8       |
|    critic_loss     | 0.0211   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1629999  |
---------------------------------
Eval num_timesteps=1645000, episode_reward=24014.19 +/- 16.56
Episode length: 19.00 +/- 4.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1645000  |
| train/             |          |
|    actor_loss      | -7.93    |
|    critic_loss     | 0.0092   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1634999  |
---------------------------------
Eval num_timesteps=1650000, episode_reward=23995.64 +/- 21.38
Episode length: 22.20 +/- 3.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1650000  |
| train/             |          |
|    actor_loss      | -8.08    |
|    critic_loss     | 0.0146   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1639999  |
---------------------------------
Eval num_timesteps=1655000, episode_reward=24020.52 +/- 29.98
Episode length: 16.40 +/- 6.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1655000  |
| train/             |          |
|    actor_loss      | -8.09    |
|    critic_loss     | 0.0107   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1644999  |
---------------------------------
Eval num_timesteps=1660000, episode_reward=24024.13 +/- 22.63
Episode length: 19.80 +/- 5.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1660000  |
| train/             |          |
|    actor_loss      | -7.91    |
|    critic_loss     | 0.0129   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1649999  |
---------------------------------
Eval num_timesteps=1665000, episode_reward=24016.87 +/- 24.35
Episode length: 19.20 +/- 4.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1665000  |
| train/             |          |
|    actor_loss      | -8.09    |
|    critic_loss     | 0.0161   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1654999  |
---------------------------------
Eval num_timesteps=1670000, episode_reward=24003.17 +/- 4.91
Episode length: 25.20 +/- 3.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1670000  |
| train/             |          |
|    actor_loss      | -7.93    |
|    critic_loss     | 0.0128   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1659999  |
---------------------------------
Eval num_timesteps=1675000, episode_reward=23999.58 +/- 33.60
Episode length: 22.20 +/- 4.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1675000  |
| train/             |          |
|    actor_loss      | -8.1     |
|    critic_loss     | 0.00907  |
|    learning_rate   | 0.0003   |
|    n_updates       | 1664999  |
---------------------------------
Eval num_timesteps=1680000, episode_reward=24003.59 +/- 19.55
Episode length: 21.80 +/- 3.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1680000  |
| train/             |          |
|    actor_loss      | -7.97    |
|    critic_loss     | 0.0248   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1669999  |
---------------------------------
Eval num_timesteps=1685000, episode_reward=24003.29 +/- 20.49
Episode length: 22.20 +/- 3.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1685000  |
| train/             |          |
|    actor_loss      | -8.03    |
|    critic_loss     | 0.0158   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1674999  |
---------------------------------
Eval num_timesteps=1690000, episode_reward=24005.05 +/- 19.95
Episode length: 22.20 +/- 3.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1690000  |
| train/             |          |
|    actor_loss      | -7.92    |
|    critic_loss     | 0.0118   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1679999  |
---------------------------------
Eval num_timesteps=1695000, episode_reward=24013.97 +/- 20.22
Episode length: 19.60 +/- 3.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1695000  |
| train/             |          |
|    actor_loss      | -8.05    |
|    critic_loss     | 0.0133   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1684999  |
---------------------------------
Eval num_timesteps=1700000, episode_reward=24011.68 +/- 24.77
Episode length: 21.00 +/- 1.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1700000  |
| train/             |          |
|    actor_loss      | -8.05    |
|    critic_loss     | 0.00983  |
|    learning_rate   | 0.0003   |
|    n_updates       | 1689999  |
---------------------------------
Eval num_timesteps=1705000, episode_reward=24009.98 +/- 11.26
Episode length: 19.20 +/- 2.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1705000  |
| train/             |          |
|    actor_loss      | -8.17    |
|    critic_loss     | 0.0194   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1694999  |
---------------------------------
Eval num_timesteps=1710000, episode_reward=24004.71 +/- 18.11
Episode length: 24.20 +/- 4.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1710000  |
| train/             |          |
|    actor_loss      | -7.89    |
|    critic_loss     | 0.0117   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1699999  |
---------------------------------
Eval num_timesteps=1715000, episode_reward=24030.79 +/- 6.42
Episode length: 18.60 +/- 1.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1715000  |
| train/             |          |
|    actor_loss      | -7.97    |
|    critic_loss     | 0.0137   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1704999  |
---------------------------------
Eval num_timesteps=1720000, episode_reward=23998.13 +/- 24.89
Episode length: 22.60 +/- 3.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1720000  |
| train/             |          |
|    actor_loss      | -8.12    |
|    critic_loss     | 0.017    |
|    learning_rate   | 0.0003   |
|    n_updates       | 1709999  |
---------------------------------
Eval num_timesteps=1725000, episode_reward=23995.65 +/- 26.76
Episode length: 20.60 +/- 3.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1725000  |
| train/             |          |
|    actor_loss      | -8.25    |
|    critic_loss     | 0.0142   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1714999  |
---------------------------------
Eval num_timesteps=1730000, episode_reward=24011.72 +/- 7.68
Episode length: 24.00 +/- 3.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1730000  |
| train/             |          |
|    actor_loss      | -8.05    |
|    critic_loss     | 0.0194   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1719999  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 124      |
|    ep_rew_mean     | 2.3e+04  |
| time/              |          |
|    episodes        | 9000     |
|    fps             | 189      |
|    time_elapsed    | 9116     |
|    total_timesteps | 1731653  |
| train/             |          |
|    actor_loss      | -8       |
|    critic_loss     | 0.0186   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1721652  |
---------------------------------
Eval num_timesteps=1735000, episode_reward=24011.25 +/- 11.51
Episode length: 21.60 +/- 1.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1735000  |
| train/             |          |
|    actor_loss      | -8       |
|    critic_loss     | 0.0125   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1724999  |
---------------------------------
Eval num_timesteps=1740000, episode_reward=24014.54 +/- 22.08
Episode length: 20.60 +/- 5.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1740000  |
| train/             |          |
|    actor_loss      | -7.82    |
|    critic_loss     | 0.0145   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1729999  |
---------------------------------
Eval num_timesteps=1745000, episode_reward=24009.86 +/- 16.26
Episode length: 22.00 +/- 3.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1745000  |
| train/             |          |
|    actor_loss      | -8.15    |
|    critic_loss     | 0.0347   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1734999  |
---------------------------------
Eval num_timesteps=1750000, episode_reward=24010.78 +/- 11.21
Episode length: 21.20 +/- 3.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1750000  |
| train/             |          |
|    actor_loss      | -8.1     |
|    critic_loss     | 0.012    |
|    learning_rate   | 0.0003   |
|    n_updates       | 1739999  |
---------------------------------
Eval num_timesteps=1755000, episode_reward=24001.47 +/- 23.31
Episode length: 22.80 +/- 3.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1755000  |
| train/             |          |
|    actor_loss      | -8.13    |
|    critic_loss     | 0.0201   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1744999  |
---------------------------------
Eval num_timesteps=1760000, episode_reward=24008.66 +/- 20.44
Episode length: 20.00 +/- 3.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1760000  |
| train/             |          |
|    actor_loss      | -7.94    |
|    critic_loss     | 0.0101   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1749999  |
---------------------------------
Eval num_timesteps=1765000, episode_reward=23999.04 +/- 36.67
Episode length: 19.60 +/- 6.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1765000  |
| train/             |          |
|    actor_loss      | -8.4     |
|    critic_loss     | 0.0117   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1754999  |
---------------------------------
Eval num_timesteps=1770000, episode_reward=23978.06 +/- 20.80
Episode length: 24.20 +/- 2.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1770000  |
| train/             |          |
|    actor_loss      | -7.99    |
|    critic_loss     | 0.0123   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1759999  |
---------------------------------
Eval num_timesteps=1775000, episode_reward=24007.20 +/- 24.22
Episode length: 20.60 +/- 5.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1775000  |
| train/             |          |
|    actor_loss      | -8.02    |
|    critic_loss     | 0.0109   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1764999  |
---------------------------------
Eval num_timesteps=1780000, episode_reward=24003.11 +/- 24.48
Episode length: 22.40 +/- 4.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1780000  |
| train/             |          |
|    actor_loss      | -8.05    |
|    critic_loss     | 0.0193   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1769999  |
---------------------------------
Eval num_timesteps=1785000, episode_reward=24005.22 +/- 35.14
Episode length: 19.00 +/- 6.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1785000  |
| train/             |          |
|    actor_loss      | -8.06    |
|    critic_loss     | 0.00978  |
|    learning_rate   | 0.0003   |
|    n_updates       | 1774999  |
---------------------------------
Eval num_timesteps=1790000, episode_reward=24016.12 +/- 25.40
Episode length: 19.40 +/- 5.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1790000  |
| train/             |          |
|    actor_loss      | -8.06    |
|    critic_loss     | 0.0192   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1779999  |
---------------------------------
Eval num_timesteps=1795000, episode_reward=23970.95 +/- 30.00
Episode length: 26.40 +/- 3.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1795000  |
| train/             |          |
|    actor_loss      | -7.94    |
|    critic_loss     | 0.0147   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1784999  |
---------------------------------
Eval num_timesteps=1800000, episode_reward=24001.88 +/- 40.24
Episode length: 19.80 +/- 6.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1800000  |
| train/             |          |
|    actor_loss      | -8.18    |
|    critic_loss     | 0.0143   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1789999  |
---------------------------------
Eval num_timesteps=1805000, episode_reward=23993.47 +/- 19.31
Episode length: 23.60 +/- 3.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1805000  |
| train/             |          |
|    actor_loss      | -8.07    |
|    critic_loss     | 0.017    |
|    learning_rate   | 0.0003   |
|    n_updates       | 1794999  |
---------------------------------
Eval num_timesteps=1810000, episode_reward=23997.83 +/- 8.06
Episode length: 22.00 +/- 2.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1810000  |
| train/             |          |
|    actor_loss      | -8.04    |
|    critic_loss     | 0.0103   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1799999  |
---------------------------------
Eval num_timesteps=1815000, episode_reward=24000.74 +/- 27.70
Episode length: 22.40 +/- 3.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1815000  |
| train/             |          |
|    actor_loss      | -8.23    |
|    critic_loss     | 0.0133   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1804999  |
---------------------------------
Eval num_timesteps=1820000, episode_reward=24013.82 +/- 11.79
Episode length: 20.40 +/- 3.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1820000  |
| train/             |          |
|    actor_loss      | -8.09    |
|    critic_loss     | 0.0118   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1809999  |
---------------------------------
Eval num_timesteps=1825000, episode_reward=24014.05 +/- 8.93
Episode length: 19.20 +/- 3.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1825000  |
| train/             |          |
|    actor_loss      | -8.27    |
|    critic_loss     | 0.0146   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1814999  |
---------------------------------
Eval num_timesteps=1830000, episode_reward=24014.46 +/- 26.70
Episode length: 19.60 +/- 4.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1830000  |
| train/             |          |
|    actor_loss      | -8       |
|    critic_loss     | 0.0115   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1819999  |
---------------------------------
Eval num_timesteps=1835000, episode_reward=24004.50 +/- 14.64
Episode length: 23.60 +/- 2.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1835000  |
| train/             |          |
|    actor_loss      | -8       |
|    critic_loss     | 0.0278   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1824999  |
---------------------------------
Eval num_timesteps=1840000, episode_reward=24000.28 +/- 24.99
Episode length: 21.40 +/- 5.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1840000  |
| train/             |          |
|    actor_loss      | -8.11    |
|    critic_loss     | 0.0193   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1829999  |
---------------------------------
Eval num_timesteps=1845000, episode_reward=24005.59 +/- 20.80
Episode length: 21.80 +/- 4.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1845000  |
| train/             |          |
|    actor_loss      | -7.89    |
|    critic_loss     | 0.0116   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1834999  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 132      |
|    ep_rew_mean     | 2.27e+04 |
| time/              |          |
|    episodes        | 10000    |
|    fps             | 190      |
|    time_elapsed    | 9690     |
|    total_timesteps | 1849151  |
| train/             |          |
|    actor_loss      | -7.87    |
|    critic_loss     | 0.0204   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1839150  |
---------------------------------
Eval num_timesteps=1850000, episode_reward=24008.83 +/- 12.60
Episode length: 20.20 +/- 1.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1850000  |
| train/             |          |
|    actor_loss      | -8.23    |
|    critic_loss     | 0.0135   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1839999  |
---------------------------------
Eval num_timesteps=1855000, episode_reward=24007.62 +/- 21.27
Episode length: 22.00 +/- 2.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1855000  |
| train/             |          |
|    actor_loss      | -8.31    |
|    critic_loss     | 0.0105   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1844999  |
---------------------------------
Eval num_timesteps=1860000, episode_reward=24022.06 +/- 13.42
Episode length: 18.00 +/- 4.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1860000  |
| train/             |          |
|    actor_loss      | -8.36    |
|    critic_loss     | 0.0328   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1849999  |
---------------------------------
Eval num_timesteps=1865000, episode_reward=24016.26 +/- 28.24
Episode length: 18.40 +/- 5.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1865000  |
| train/             |          |
|    actor_loss      | -8.12    |
|    critic_loss     | 0.0147   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1854999  |
---------------------------------
Eval num_timesteps=1870000, episode_reward=24011.95 +/- 18.00
Episode length: 20.80 +/- 2.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1870000  |
| train/             |          |
|    actor_loss      | -8.01    |
|    critic_loss     | 0.0153   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1859999  |
---------------------------------
Eval num_timesteps=1875000, episode_reward=24007.67 +/- 23.37
Episode length: 20.60 +/- 5.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1875000  |
| train/             |          |
|    actor_loss      | -8.25    |
|    critic_loss     | 0.0234   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1864999  |
---------------------------------
Eval num_timesteps=1880000, episode_reward=23994.19 +/- 33.32
Episode length: 21.60 +/- 4.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1880000  |
| train/             |          |
|    actor_loss      | -8.07    |
|    critic_loss     | 0.00912  |
|    learning_rate   | 0.0003   |
|    n_updates       | 1869999  |
---------------------------------
Eval num_timesteps=1885000, episode_reward=23997.64 +/- 25.33
Episode length: 23.00 +/- 4.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1885000  |
| train/             |          |
|    actor_loss      | -8.1     |
|    critic_loss     | 0.014    |
|    learning_rate   | 0.0003   |
|    n_updates       | 1874999  |
---------------------------------
Eval num_timesteps=1890000, episode_reward=23983.29 +/- 10.05
Episode length: 22.20 +/- 4.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1890000  |
| train/             |          |
|    actor_loss      | -7.78    |
|    critic_loss     | 0.0169   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1879999  |
---------------------------------
Eval num_timesteps=1895000, episode_reward=24014.12 +/- 13.54
Episode length: 19.40 +/- 1.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1895000  |
| train/             |          |
|    actor_loss      | -8.1     |
|    critic_loss     | 0.0111   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1884999  |
---------------------------------
Eval num_timesteps=1900000, episode_reward=24307.60 +/- 598.66
Episode length: 21.00 +/- 3.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21       |
|    mean_reward     | 2.43e+04 |
| time/              |          |
|    total_timesteps | 1900000  |
| train/             |          |
|    actor_loss      | -8.24    |
|    critic_loss     | 0.0136   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1889999  |
---------------------------------
Eval num_timesteps=1905000, episode_reward=23997.80 +/- 11.72
Episode length: 23.00 +/- 1.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1905000  |
| train/             |          |
|    actor_loss      | -8.21    |
|    critic_loss     | 0.0136   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1894999  |
---------------------------------
Eval num_timesteps=1910000, episode_reward=24002.21 +/- 22.63
Episode length: 21.80 +/- 4.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1910000  |
| train/             |          |
|    actor_loss      | -8.13    |
|    critic_loss     | 0.0133   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1899999  |
---------------------------------
Eval num_timesteps=1915000, episode_reward=24010.02 +/- 22.76
Episode length: 21.20 +/- 4.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1915000  |
| train/             |          |
|    actor_loss      | -8.28    |
|    critic_loss     | 0.0102   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1904999  |
---------------------------------
Eval num_timesteps=1920000, episode_reward=24013.21 +/- 25.08
Episode length: 18.00 +/- 5.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1920000  |
| train/             |          |
|    actor_loss      | -8.18    |
|    critic_loss     | 0.0126   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1909999  |
---------------------------------
Eval num_timesteps=1925000, episode_reward=23992.27 +/- 14.32
Episode length: 23.40 +/- 1.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1925000  |
| train/             |          |
|    actor_loss      | -8.17    |
|    critic_loss     | 0.0127   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1914999  |
---------------------------------
Eval num_timesteps=1930000, episode_reward=24008.06 +/- 15.75
Episode length: 20.80 +/- 3.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1930000  |
| train/             |          |
|    actor_loss      | -8.45    |
|    critic_loss     | 0.00927  |
|    learning_rate   | 0.0003   |
|    n_updates       | 1919999  |
---------------------------------
Eval num_timesteps=1935000, episode_reward=24005.95 +/- 29.79
Episode length: 20.40 +/- 4.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1935000  |
| train/             |          |
|    actor_loss      | -8.22    |
|    critic_loss     | 0.00923  |
|    learning_rate   | 0.0003   |
|    n_updates       | 1924999  |
---------------------------------
Eval num_timesteps=1940000, episode_reward=24001.10 +/- 4.00
Episode length: 19.80 +/- 2.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1940000  |
| train/             |          |
|    actor_loss      | -8.33    |
|    critic_loss     | 0.00875  |
|    learning_rate   | 0.0003   |
|    n_updates       | 1929999  |
---------------------------------
Eval num_timesteps=1945000, episode_reward=24007.36 +/- 26.58
Episode length: 21.00 +/- 4.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1945000  |
| train/             |          |
|    actor_loss      | -8.2     |
|    critic_loss     | 0.0102   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1934999  |
---------------------------------
Eval num_timesteps=1950000, episode_reward=23975.13 +/- 44.24
Episode length: 27.80 +/- 11.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1950000  |
| train/             |          |
|    actor_loss      | -8.04    |
|    critic_loss     | 0.014    |
|    learning_rate   | 0.0003   |
|    n_updates       | 1939999  |
---------------------------------
Eval num_timesteps=1955000, episode_reward=24010.31 +/- 32.53
Episode length: 19.60 +/- 6.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1955000  |
| train/             |          |
|    actor_loss      | -8.36    |
|    critic_loss     | 0.0159   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1944999  |
---------------------------------
Eval num_timesteps=1960000, episode_reward=23982.55 +/- 29.82
Episode length: 22.80 +/- 6.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1960000  |
| train/             |          |
|    actor_loss      | -8.4     |
|    critic_loss     | 0.0094   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1949999  |
---------------------------------
Eval num_timesteps=1965000, episode_reward=24003.41 +/- 10.92
Episode length: 21.80 +/- 1.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1965000  |
| train/             |          |
|    actor_loss      | -8.24    |
|    critic_loss     | 0.0158   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1954999  |
---------------------------------
Eval num_timesteps=1970000, episode_reward=23992.43 +/- 18.10
Episode length: 23.60 +/- 3.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1970000  |
| train/             |          |
|    actor_loss      | -8.29    |
|    critic_loss     | 0.0125   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1959999  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 132      |
|    ep_rew_mean     | 2.28e+04 |
| time/              |          |
|    episodes        | 11000    |
|    fps             | 191      |
|    time_elapsed    | 10296    |
|    total_timesteps | 1970511  |
| train/             |          |
|    actor_loss      | -8.08    |
|    critic_loss     | 0.0113   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1960510  |
---------------------------------
Eval num_timesteps=1975000, episode_reward=24010.29 +/- 4.26
Episode length: 22.20 +/- 1.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1975000  |
| train/             |          |
|    actor_loss      | -8.18    |
|    critic_loss     | 0.0135   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1964999  |
---------------------------------
Eval num_timesteps=1980000, episode_reward=24015.92 +/- 13.50
Episode length: 21.20 +/- 2.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1980000  |
| train/             |          |
|    actor_loss      | -8.24    |
|    critic_loss     | 0.0163   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1969999  |
---------------------------------
Eval num_timesteps=1985000, episode_reward=24000.00 +/- 25.72
Episode length: 22.40 +/- 5.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1985000  |
| train/             |          |
|    actor_loss      | -8.21    |
|    critic_loss     | 0.0119   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1974999  |
---------------------------------
Eval num_timesteps=1990000, episode_reward=23996.34 +/- 22.46
Episode length: 23.20 +/- 5.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1990000  |
| train/             |          |
|    actor_loss      | -8.25    |
|    critic_loss     | 0.0194   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1979999  |
---------------------------------
Eval num_timesteps=1995000, episode_reward=23998.60 +/- 8.82
Episode length: 21.80 +/- 1.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 1995000  |
| train/             |          |
|    actor_loss      | -7.95    |
|    critic_loss     | 0.00819  |
|    learning_rate   | 0.0003   |
|    n_updates       | 1984999  |
---------------------------------
Eval num_timesteps=2000000, episode_reward=23982.32 +/- 19.58
Episode length: 24.20 +/- 3.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2000000  |
| train/             |          |
|    actor_loss      | -8.04    |
|    critic_loss     | 0.00963  |
|    learning_rate   | 0.0003   |
|    n_updates       | 1989999  |
---------------------------------
Eval num_timesteps=2005000, episode_reward=24006.90 +/- 18.87
Episode length: 20.40 +/- 2.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2005000  |
| train/             |          |
|    actor_loss      | -8.16    |
|    critic_loss     | 0.0097   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1994999  |
---------------------------------
Eval num_timesteps=2010000, episode_reward=23984.74 +/- 17.58
Episode length: 23.80 +/- 1.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2010000  |
| train/             |          |
|    actor_loss      | -8.42    |
|    critic_loss     | 0.0108   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1999999  |
---------------------------------
Eval num_timesteps=2015000, episode_reward=24006.48 +/- 14.22
Episode length: 21.20 +/- 4.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2015000  |
| train/             |          |
|    actor_loss      | -8.05    |
|    critic_loss     | 0.0102   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2004999  |
---------------------------------
Eval num_timesteps=2020000, episode_reward=23980.22 +/- 17.51
Episode length: 24.40 +/- 2.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2020000  |
| train/             |          |
|    actor_loss      | -8.34    |
|    critic_loss     | 0.0231   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2009999  |
---------------------------------
Eval num_timesteps=2025000, episode_reward=23997.19 +/- 11.60
Episode length: 22.00 +/- 3.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2025000  |
| train/             |          |
|    actor_loss      | -8.18    |
|    critic_loss     | 0.0105   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2014999  |
---------------------------------
Eval num_timesteps=2030000, episode_reward=24002.43 +/- 27.56
Episode length: 20.80 +/- 6.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2030000  |
| train/             |          |
|    actor_loss      | -8.18    |
|    critic_loss     | 0.0151   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2019999  |
---------------------------------
Eval num_timesteps=2035000, episode_reward=23994.98 +/- 16.48
Episode length: 22.20 +/- 1.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2035000  |
| train/             |          |
|    actor_loss      | -8.24    |
|    critic_loss     | 0.00917  |
|    learning_rate   | 0.0003   |
|    n_updates       | 2024999  |
---------------------------------
Eval num_timesteps=2040000, episode_reward=24005.08 +/- 13.80
Episode length: 22.60 +/- 1.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2040000  |
| train/             |          |
|    actor_loss      | -8.24    |
|    critic_loss     | 0.0104   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2029999  |
---------------------------------
Eval num_timesteps=2045000, episode_reward=23999.43 +/- 9.38
Episode length: 23.20 +/- 1.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2045000  |
| train/             |          |
|    actor_loss      | -8.4     |
|    critic_loss     | 0.0119   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2034999  |
---------------------------------
Eval num_timesteps=2050000, episode_reward=24021.20 +/- 27.02
Episode length: 18.80 +/- 3.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2050000  |
| train/             |          |
|    actor_loss      | -8.34    |
|    critic_loss     | 0.0141   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2039999  |
---------------------------------
Eval num_timesteps=2055000, episode_reward=24005.74 +/- 19.40
Episode length: 21.80 +/- 3.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2055000  |
| train/             |          |
|    actor_loss      | -8.12    |
|    critic_loss     | 0.00936  |
|    learning_rate   | 0.0003   |
|    n_updates       | 2044999  |
---------------------------------
Eval num_timesteps=2060000, episode_reward=24010.96 +/- 26.27
Episode length: 21.80 +/- 3.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2060000  |
| train/             |          |
|    actor_loss      | -8.13    |
|    critic_loss     | 0.0118   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2049999  |
---------------------------------
Eval num_timesteps=2065000, episode_reward=24000.49 +/- 12.91
Episode length: 21.80 +/- 3.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2065000  |
| train/             |          |
|    actor_loss      | -8       |
|    critic_loss     | 0.0165   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2054999  |
---------------------------------
Eval num_timesteps=2070000, episode_reward=23998.36 +/- 26.49
Episode length: 23.60 +/- 3.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2070000  |
| train/             |          |
|    actor_loss      | -8.19    |
|    critic_loss     | 0.0109   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2059999  |
---------------------------------
Eval num_timesteps=2075000, episode_reward=24010.11 +/- 27.11
Episode length: 20.60 +/- 5.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2075000  |
| train/             |          |
|    actor_loss      | -8.11    |
|    critic_loss     | 0.00908  |
|    learning_rate   | 0.0003   |
|    n_updates       | 2064999  |
---------------------------------
Eval num_timesteps=2080000, episode_reward=24024.95 +/- 14.69
Episode length: 18.00 +/- 3.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2080000  |
| train/             |          |
|    actor_loss      | -8.2     |
|    critic_loss     | 0.0109   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2069999  |
---------------------------------
Eval num_timesteps=2085000, episode_reward=24001.06 +/- 16.87
Episode length: 22.00 +/- 2.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2085000  |
| train/             |          |
|    actor_loss      | -8.38    |
|    critic_loss     | 0.0196   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2074999  |
---------------------------------
Eval num_timesteps=2090000, episode_reward=23993.19 +/- 27.06
Episode length: 24.20 +/- 2.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2090000  |
| train/             |          |
|    actor_loss      | -8.41    |
|    critic_loss     | 0.0107   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2079999  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 124      |
|    ep_rew_mean     | 2.29e+04 |
| time/              |          |
|    episodes        | 12000    |
|    fps             | 191      |
|    time_elapsed    | 10907    |
|    total_timesteps | 2093385  |
| train/             |          |
|    actor_loss      | -8.41    |
|    critic_loss     | 0.0133   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2083384  |
---------------------------------
Eval num_timesteps=2095000, episode_reward=23997.61 +/- 25.64
Episode length: 23.60 +/- 2.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2095000  |
| train/             |          |
|    actor_loss      | -8.25    |
|    critic_loss     | 0.0117   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2084999  |
---------------------------------
Eval num_timesteps=2100000, episode_reward=24000.10 +/- 20.76
Episode length: 22.60 +/- 3.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2100000  |
| train/             |          |
|    actor_loss      | -7.97    |
|    critic_loss     | 0.00994  |
|    learning_rate   | 0.0003   |
|    n_updates       | 2089999  |
---------------------------------
Eval num_timesteps=2105000, episode_reward=24014.17 +/- 21.42
Episode length: 21.40 +/- 5.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2105000  |
| train/             |          |
|    actor_loss      | -8.26    |
|    critic_loss     | 0.00908  |
|    learning_rate   | 0.0003   |
|    n_updates       | 2094999  |
---------------------------------
Eval num_timesteps=2110000, episode_reward=23998.42 +/- 26.62
Episode length: 24.60 +/- 4.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2110000  |
| train/             |          |
|    actor_loss      | -8.25    |
|    critic_loss     | 0.0132   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2099999  |
---------------------------------
Eval num_timesteps=2115000, episode_reward=24002.80 +/- 27.35
Episode length: 22.40 +/- 3.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2115000  |
| train/             |          |
|    actor_loss      | -8.29    |
|    critic_loss     | 0.0138   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2104999  |
---------------------------------
Eval num_timesteps=2120000, episode_reward=24011.29 +/- 21.70
Episode length: 19.00 +/- 4.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2120000  |
| train/             |          |
|    actor_loss      | -8.25    |
|    critic_loss     | 0.0105   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2109999  |
---------------------------------
Eval num_timesteps=2125000, episode_reward=24014.64 +/- 32.51
Episode length: 18.60 +/- 4.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2125000  |
| train/             |          |
|    actor_loss      | -8.14    |
|    critic_loss     | 0.0127   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2114999  |
---------------------------------
Eval num_timesteps=2130000, episode_reward=23994.89 +/- 19.31
Episode length: 22.20 +/- 1.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2130000  |
| train/             |          |
|    actor_loss      | -8.26    |
|    critic_loss     | 0.0132   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2119999  |
---------------------------------
Eval num_timesteps=2135000, episode_reward=24291.76 +/- 601.85
Episode length: 21.20 +/- 3.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.2     |
|    mean_reward     | 2.43e+04 |
| time/              |          |
|    total_timesteps | 2135000  |
| train/             |          |
|    actor_loss      | -8.46    |
|    critic_loss     | 0.011    |
|    learning_rate   | 0.0003   |
|    n_updates       | 2124999  |
---------------------------------
Eval num_timesteps=2140000, episode_reward=24022.00 +/- 13.63
Episode length: 19.20 +/- 1.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2140000  |
| train/             |          |
|    actor_loss      | -8.3     |
|    critic_loss     | 0.00942  |
|    learning_rate   | 0.0003   |
|    n_updates       | 2129999  |
---------------------------------
Eval num_timesteps=2145000, episode_reward=23999.61 +/- 15.51
Episode length: 21.60 +/- 3.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2145000  |
| train/             |          |
|    actor_loss      | -8.3     |
|    critic_loss     | 0.00987  |
|    learning_rate   | 0.0003   |
|    n_updates       | 2134999  |
---------------------------------
Eval num_timesteps=2150000, episode_reward=24000.75 +/- 32.94
Episode length: 21.40 +/- 6.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2150000  |
| train/             |          |
|    actor_loss      | -8.25    |
|    critic_loss     | 0.0166   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2139999  |
---------------------------------
Eval num_timesteps=2155000, episode_reward=24000.57 +/- 26.57
Episode length: 22.80 +/- 4.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2155000  |
| train/             |          |
|    actor_loss      | -8.36    |
|    critic_loss     | 0.00834  |
|    learning_rate   | 0.0003   |
|    n_updates       | 2144999  |
---------------------------------
Eval num_timesteps=2160000, episode_reward=24010.39 +/- 16.22
Episode length: 19.80 +/- 1.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2160000  |
| train/             |          |
|    actor_loss      | -8.4     |
|    critic_loss     | 0.0113   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2149999  |
---------------------------------
Eval num_timesteps=2165000, episode_reward=24020.18 +/- 8.32
Episode length: 19.40 +/- 2.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2165000  |
| train/             |          |
|    actor_loss      | -8.15    |
|    critic_loss     | 0.0242   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2154999  |
---------------------------------
Eval num_timesteps=2170000, episode_reward=23996.01 +/- 24.36
Episode length: 23.40 +/- 4.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2170000  |
| train/             |          |
|    actor_loss      | -8.16    |
|    critic_loss     | 0.0109   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2159999  |
---------------------------------
Eval num_timesteps=2175000, episode_reward=23984.48 +/- 20.70
Episode length: 22.60 +/- 4.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2175000  |
| train/             |          |
|    actor_loss      | -8.07    |
|    critic_loss     | 0.0121   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2164999  |
---------------------------------
Eval num_timesteps=2180000, episode_reward=23989.14 +/- 24.50
Episode length: 21.80 +/- 4.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2180000  |
| train/             |          |
|    actor_loss      | -8.19    |
|    critic_loss     | 0.0146   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2169999  |
---------------------------------
Eval num_timesteps=2185000, episode_reward=24004.48 +/- 22.31
Episode length: 20.20 +/- 3.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2185000  |
| train/             |          |
|    actor_loss      | -8.23    |
|    critic_loss     | 0.0144   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2174999  |
---------------------------------
Eval num_timesteps=2190000, episode_reward=24001.11 +/- 26.54
Episode length: 19.00 +/- 4.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2190000  |
| train/             |          |
|    actor_loss      | -8.47    |
|    critic_loss     | 0.0105   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2179999  |
---------------------------------
Eval num_timesteps=2195000, episode_reward=24011.29 +/- 25.17
Episode length: 19.20 +/- 5.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2195000  |
| train/             |          |
|    actor_loss      | -8.3     |
|    critic_loss     | 0.00986  |
|    learning_rate   | 0.0003   |
|    n_updates       | 2184999  |
---------------------------------
Eval num_timesteps=2200000, episode_reward=24012.93 +/- 20.02
Episode length: 22.60 +/- 2.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2200000  |
| train/             |          |
|    actor_loss      | -8.3     |
|    critic_loss     | 0.011    |
|    learning_rate   | 0.0003   |
|    n_updates       | 2189999  |
---------------------------------
Eval num_timesteps=2205000, episode_reward=24005.55 +/- 25.96
Episode length: 20.00 +/- 5.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2205000  |
| train/             |          |
|    actor_loss      | -8.35    |
|    critic_loss     | 0.0265   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2194999  |
---------------------------------
Eval num_timesteps=2210000, episode_reward=24016.58 +/- 11.21
Episode length: 18.20 +/- 3.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2210000  |
| train/             |          |
|    actor_loss      | -8.51    |
|    critic_loss     | 0.0104   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2199999  |
---------------------------------
Eval num_timesteps=2215000, episode_reward=23993.50 +/- 18.98
Episode length: 23.80 +/- 1.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2215000  |
| train/             |          |
|    actor_loss      | -8.24    |
|    critic_loss     | 0.0114   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2204999  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 123      |
|    ep_rew_mean     | 2.32e+04 |
| time/              |          |
|    episodes        | 13000    |
|    fps             | 192      |
|    time_elapsed    | 11527    |
|    total_timesteps | 2216580  |
| train/             |          |
|    actor_loss      | -8.17    |
|    critic_loss     | 0.011    |
|    learning_rate   | 0.0003   |
|    n_updates       | 2206579  |
---------------------------------
Eval num_timesteps=2220000, episode_reward=23997.98 +/- 17.40
Episode length: 18.80 +/- 4.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2220000  |
| train/             |          |
|    actor_loss      | -8.34    |
|    critic_loss     | 0.0138   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2209999  |
---------------------------------
Eval num_timesteps=2225000, episode_reward=24007.30 +/- 6.94
Episode length: 21.40 +/- 1.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2225000  |
| train/             |          |
|    actor_loss      | -8.37    |
|    critic_loss     | 0.0126   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2214999  |
---------------------------------
Eval num_timesteps=2230000, episode_reward=23990.03 +/- 26.51
Episode length: 22.80 +/- 3.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2230000  |
| train/             |          |
|    actor_loss      | -8.41    |
|    critic_loss     | 0.0111   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2219999  |
---------------------------------
Eval num_timesteps=2235000, episode_reward=24008.14 +/- 16.45
Episode length: 21.00 +/- 2.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2235000  |
| train/             |          |
|    actor_loss      | -8.44    |
|    critic_loss     | 0.0102   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2224999  |
---------------------------------
Eval num_timesteps=2240000, episode_reward=24295.35 +/- 612.01
Episode length: 20.20 +/- 3.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.2     |
|    mean_reward     | 2.43e+04 |
| time/              |          |
|    total_timesteps | 2240000  |
| train/             |          |
|    actor_loss      | -8.33    |
|    critic_loss     | 0.0171   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2229999  |
---------------------------------
Eval num_timesteps=2245000, episode_reward=23983.48 +/- 12.94
Episode length: 25.00 +/- 3.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2245000  |
| train/             |          |
|    actor_loss      | -8.28    |
|    critic_loss     | 0.0178   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2234999  |
---------------------------------
Eval num_timesteps=2250000, episode_reward=24017.50 +/- 20.70
Episode length: 20.00 +/- 4.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2250000  |
| train/             |          |
|    actor_loss      | -8.41    |
|    critic_loss     | 0.0157   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2239999  |
---------------------------------
Eval num_timesteps=2255000, episode_reward=24000.59 +/- 14.35
Episode length: 23.40 +/- 3.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2255000  |
| train/             |          |
|    actor_loss      | -8.4     |
|    critic_loss     | 0.0327   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2244999  |
---------------------------------
Eval num_timesteps=2260000, episode_reward=23991.56 +/- 15.57
Episode length: 25.60 +/- 3.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2260000  |
| train/             |          |
|    actor_loss      | -8.25    |
|    critic_loss     | 0.0112   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2249999  |
---------------------------------
Eval num_timesteps=2265000, episode_reward=24004.68 +/- 24.63
Episode length: 20.60 +/- 6.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2265000  |
| train/             |          |
|    actor_loss      | -8.24    |
|    critic_loss     | 0.0163   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2254999  |
---------------------------------
Eval num_timesteps=2270000, episode_reward=23999.57 +/- 18.88
Episode length: 21.60 +/- 4.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2270000  |
| train/             |          |
|    actor_loss      | -8.2     |
|    critic_loss     | 0.0126   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2259999  |
---------------------------------
Eval num_timesteps=2275000, episode_reward=24014.09 +/- 23.14
Episode length: 17.60 +/- 5.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2275000  |
| train/             |          |
|    actor_loss      | -8.21    |
|    critic_loss     | 0.0104   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2264999  |
---------------------------------
Eval num_timesteps=2280000, episode_reward=24022.78 +/- 5.92
Episode length: 19.00 +/- 1.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2280000  |
| train/             |          |
|    actor_loss      | -8.15    |
|    critic_loss     | 0.0181   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2269999  |
---------------------------------
Eval num_timesteps=2285000, episode_reward=24019.14 +/- 15.89
Episode length: 20.40 +/- 4.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2285000  |
| train/             |          |
|    actor_loss      | -8.08    |
|    critic_loss     | 0.0145   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2274999  |
---------------------------------
Eval num_timesteps=2290000, episode_reward=24002.57 +/- 21.49
Episode length: 19.20 +/- 4.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2290000  |
| train/             |          |
|    actor_loss      | -8.35    |
|    critic_loss     | 0.00915  |
|    learning_rate   | 0.0003   |
|    n_updates       | 2279999  |
---------------------------------
Eval num_timesteps=2295000, episode_reward=24002.20 +/- 16.71
Episode length: 23.20 +/- 3.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2295000  |
| train/             |          |
|    actor_loss      | -8.38    |
|    critic_loss     | 0.0116   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2284999  |
---------------------------------
Eval num_timesteps=2300000, episode_reward=23991.19 +/- 24.76
Episode length: 22.80 +/- 4.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2300000  |
| train/             |          |
|    actor_loss      | -8.46    |
|    critic_loss     | 0.0122   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2289999  |
---------------------------------
Eval num_timesteps=2305000, episode_reward=24008.15 +/- 20.38
Episode length: 22.00 +/- 3.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2305000  |
| train/             |          |
|    actor_loss      | -8.23    |
|    critic_loss     | 0.0188   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2294999  |
---------------------------------
Eval num_timesteps=2310000, episode_reward=24001.36 +/- 33.45
Episode length: 22.60 +/- 3.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2310000  |
| train/             |          |
|    actor_loss      | -8.26    |
|    critic_loss     | 0.0116   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2299999  |
---------------------------------
Eval num_timesteps=2315000, episode_reward=24006.42 +/- 19.64
Episode length: 17.60 +/- 4.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2315000  |
| train/             |          |
|    actor_loss      | -8.24    |
|    critic_loss     | 0.0103   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2304999  |
---------------------------------
Eval num_timesteps=2320000, episode_reward=24006.66 +/- 14.82
Episode length: 21.20 +/- 3.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2320000  |
| train/             |          |
|    actor_loss      | -8.14    |
|    critic_loss     | 0.0139   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2309999  |
---------------------------------
Eval num_timesteps=2325000, episode_reward=24304.94 +/- 593.17
Episode length: 21.60 +/- 1.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.6     |
|    mean_reward     | 2.43e+04 |
| time/              |          |
|    total_timesteps | 2325000  |
| train/             |          |
|    actor_loss      | -8.39    |
|    critic_loss     | 0.0104   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2314999  |
---------------------------------
Eval num_timesteps=2330000, episode_reward=23993.11 +/- 26.06
Episode length: 21.20 +/- 4.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2330000  |
| train/             |          |
|    actor_loss      | -8.29    |
|    critic_loss     | 0.0141   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2319999  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 106      |
|    ep_rew_mean     | 2.24e+04 |
| time/              |          |
|    episodes        | 14000    |
|    fps             | 192      |
|    time_elapsed    | 12104    |
|    total_timesteps | 2331885  |
| train/             |          |
|    actor_loss      | -8.65    |
|    critic_loss     | 0.0131   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2321884  |
---------------------------------
Eval num_timesteps=2335000, episode_reward=24000.03 +/- 20.26
Episode length: 22.20 +/- 1.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2335000  |
| train/             |          |
|    actor_loss      | -8.25    |
|    critic_loss     | 0.0132   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2324999  |
---------------------------------
Eval num_timesteps=2340000, episode_reward=24009.77 +/- 22.43
Episode length: 19.20 +/- 5.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2340000  |
| train/             |          |
|    actor_loss      | -8.29    |
|    critic_loss     | 0.0101   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2329999  |
---------------------------------
Eval num_timesteps=2345000, episode_reward=23997.20 +/- 27.14
Episode length: 21.40 +/- 3.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2345000  |
| train/             |          |
|    actor_loss      | -8.33    |
|    critic_loss     | 0.00856  |
|    learning_rate   | 0.0003   |
|    n_updates       | 2334999  |
---------------------------------
Eval num_timesteps=2350000, episode_reward=24003.06 +/- 29.42
Episode length: 21.80 +/- 6.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2350000  |
| train/             |          |
|    actor_loss      | -8.3     |
|    critic_loss     | 0.0101   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2339999  |
---------------------------------
Eval num_timesteps=2355000, episode_reward=24019.61 +/- 11.77
Episode length: 21.00 +/- 1.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2355000  |
| train/             |          |
|    actor_loss      | -8.1     |
|    critic_loss     | 0.0139   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2344999  |
---------------------------------
Eval num_timesteps=2360000, episode_reward=23998.98 +/- 23.75
Episode length: 22.00 +/- 5.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2360000  |
| train/             |          |
|    actor_loss      | -8.53    |
|    critic_loss     | 0.0131   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2349999  |
---------------------------------
Eval num_timesteps=2365000, episode_reward=24011.40 +/- 25.33
Episode length: 18.80 +/- 4.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2365000  |
| train/             |          |
|    actor_loss      | -8.35    |
|    critic_loss     | 0.0112   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2354999  |
---------------------------------
Eval num_timesteps=2370000, episode_reward=23991.16 +/- 21.01
Episode length: 21.60 +/- 5.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2370000  |
| train/             |          |
|    actor_loss      | -8.48    |
|    critic_loss     | 0.00917  |
|    learning_rate   | 0.0003   |
|    n_updates       | 2359999  |
---------------------------------
Eval num_timesteps=2375000, episode_reward=24005.16 +/- 20.39
Episode length: 22.80 +/- 2.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2375000  |
| train/             |          |
|    actor_loss      | -8.26    |
|    critic_loss     | 0.0132   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2364999  |
---------------------------------
Eval num_timesteps=2380000, episode_reward=23992.50 +/- 13.95
Episode length: 22.20 +/- 2.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2380000  |
| train/             |          |
|    actor_loss      | -8.57    |
|    critic_loss     | 0.00992  |
|    learning_rate   | 0.0003   |
|    n_updates       | 2369999  |
---------------------------------
Eval num_timesteps=2385000, episode_reward=24003.60 +/- 26.10
Episode length: 20.80 +/- 6.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2385000  |
| train/             |          |
|    actor_loss      | -8.21    |
|    critic_loss     | 0.0134   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2374999  |
---------------------------------
Eval num_timesteps=2390000, episode_reward=24007.64 +/- 16.10
Episode length: 21.00 +/- 3.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2390000  |
| train/             |          |
|    actor_loss      | -8.53    |
|    critic_loss     | 0.00827  |
|    learning_rate   | 0.0003   |
|    n_updates       | 2379999  |
---------------------------------
Eval num_timesteps=2395000, episode_reward=23995.08 +/- 16.03
Episode length: 24.20 +/- 2.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2395000  |
| train/             |          |
|    actor_loss      | -8.37    |
|    critic_loss     | 0.00997  |
|    learning_rate   | 0.0003   |
|    n_updates       | 2384999  |
---------------------------------
Eval num_timesteps=2400000, episode_reward=24005.85 +/- 26.43
Episode length: 19.20 +/- 5.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2400000  |
| train/             |          |
|    actor_loss      | -8.41    |
|    critic_loss     | 0.013    |
|    learning_rate   | 0.0003   |
|    n_updates       | 2389999  |
---------------------------------
Eval num_timesteps=2405000, episode_reward=24009.34 +/- 20.03
Episode length: 22.40 +/- 4.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2405000  |
| train/             |          |
|    actor_loss      | -8.32    |
|    critic_loss     | 0.00967  |
|    learning_rate   | 0.0003   |
|    n_updates       | 2394999  |
---------------------------------
Eval num_timesteps=2410000, episode_reward=24003.60 +/- 19.97
Episode length: 19.80 +/- 3.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2410000  |
| train/             |          |
|    actor_loss      | -8.62    |
|    critic_loss     | 0.0127   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2399999  |
---------------------------------
Eval num_timesteps=2415000, episode_reward=24003.35 +/- 24.94
Episode length: 19.80 +/- 4.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2415000  |
| train/             |          |
|    actor_loss      | -8.28    |
|    critic_loss     | 0.0115   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2404999  |
---------------------------------
Eval num_timesteps=2420000, episode_reward=24010.40 +/- 23.84
Episode length: 19.60 +/- 4.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2420000  |
| train/             |          |
|    actor_loss      | -8.28    |
|    critic_loss     | 0.0306   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2409999  |
---------------------------------
Eval num_timesteps=2425000, episode_reward=23999.59 +/- 24.22
Episode length: 20.60 +/- 4.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2425000  |
| train/             |          |
|    actor_loss      | -8.45    |
|    critic_loss     | 0.0102   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2414999  |
---------------------------------
Eval num_timesteps=2430000, episode_reward=24008.61 +/- 15.18
Episode length: 21.60 +/- 2.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2430000  |
| train/             |          |
|    actor_loss      | -8.4     |
|    critic_loss     | 0.00821  |
|    learning_rate   | 0.0003   |
|    n_updates       | 2419999  |
---------------------------------
Eval num_timesteps=2435000, episode_reward=24013.41 +/- 7.75
Episode length: 21.00 +/- 1.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2435000  |
| train/             |          |
|    actor_loss      | -8.42    |
|    critic_loss     | 0.00999  |
|    learning_rate   | 0.0003   |
|    n_updates       | 2424999  |
---------------------------------
Eval num_timesteps=2440000, episode_reward=23987.11 +/- 22.55
Episode length: 22.60 +/- 1.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2440000  |
| train/             |          |
|    actor_loss      | -8.43    |
|    critic_loss     | 0.00991  |
|    learning_rate   | 0.0003   |
|    n_updates       | 2429999  |
---------------------------------
Eval num_timesteps=2445000, episode_reward=23991.20 +/- 7.37
Episode length: 22.60 +/- 3.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2445000  |
| train/             |          |
|    actor_loss      | -8.19    |
|    critic_loss     | 0.0129   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2434999  |
---------------------------------
Eval num_timesteps=2450000, episode_reward=24015.86 +/- 19.81
Episode length: 21.40 +/- 2.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2450000  |
| train/             |          |
|    actor_loss      | -8.25    |
|    critic_loss     | 0.00902  |
|    learning_rate   | 0.0003   |
|    n_updates       | 2439999  |
---------------------------------
Eval num_timesteps=2455000, episode_reward=23994.18 +/- 14.81
Episode length: 23.40 +/- 3.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2455000  |
| train/             |          |
|    actor_loss      | -8.22    |
|    critic_loss     | 0.0125   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2444999  |
---------------------------------
Eval num_timesteps=2460000, episode_reward=23996.41 +/- 6.71
Episode length: 22.80 +/- 2.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2460000  |
| train/             |          |
|    actor_loss      | -8.16    |
|    critic_loss     | 0.0127   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2449999  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 109      |
|    ep_rew_mean     | 2.3e+04  |
| time/              |          |
|    episodes        | 15000    |
|    fps             | 192      |
|    time_elapsed    | 12751    |
|    total_timesteps | 2460165  |
| train/             |          |
|    actor_loss      | -8.22    |
|    critic_loss     | 0.0133   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2450164  |
---------------------------------
Eval num_timesteps=2465000, episode_reward=23997.28 +/- 15.23
Episode length: 21.40 +/- 2.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2465000  |
| train/             |          |
|    actor_loss      | -8.52    |
|    critic_loss     | 0.00865  |
|    learning_rate   | 0.0003   |
|    n_updates       | 2454999  |
---------------------------------
Eval num_timesteps=2470000, episode_reward=23997.87 +/- 8.00
Episode length: 21.00 +/- 1.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2470000  |
| train/             |          |
|    actor_loss      | -8.47    |
|    critic_loss     | 0.0414   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2459999  |
---------------------------------
Eval num_timesteps=2475000, episode_reward=24011.89 +/- 29.53
Episode length: 19.40 +/- 6.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2475000  |
| train/             |          |
|    actor_loss      | -8.56    |
|    critic_loss     | 0.0104   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2464999  |
---------------------------------
Eval num_timesteps=2480000, episode_reward=24012.77 +/- 22.34
Episode length: 19.20 +/- 3.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2480000  |
| train/             |          |
|    actor_loss      | -8.42    |
|    critic_loss     | 0.00744  |
|    learning_rate   | 0.0003   |
|    n_updates       | 2469999  |
---------------------------------
Eval num_timesteps=2485000, episode_reward=24009.76 +/- 25.17
Episode length: 18.60 +/- 5.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2485000  |
| train/             |          |
|    actor_loss      | -8.51    |
|    critic_loss     | 0.00976  |
|    learning_rate   | 0.0003   |
|    n_updates       | 2474999  |
---------------------------------
Eval num_timesteps=2490000, episode_reward=24008.73 +/- 24.28
Episode length: 20.00 +/- 4.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2490000  |
| train/             |          |
|    actor_loss      | -7.94    |
|    critic_loss     | 0.0109   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2479999  |
---------------------------------
Eval num_timesteps=2495000, episode_reward=24013.50 +/- 10.77
Episode length: 20.40 +/- 3.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2495000  |
| train/             |          |
|    actor_loss      | -8.36    |
|    critic_loss     | 0.0103   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2484999  |
---------------------------------
Eval num_timesteps=2500000, episode_reward=24002.37 +/- 15.24
Episode length: 23.00 +/- 2.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2500000  |
| train/             |          |
|    actor_loss      | -8.4     |
|    critic_loss     | 0.0106   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2489999  |
---------------------------------
Eval num_timesteps=2505000, episode_reward=23978.19 +/- 19.53
Episode length: 24.40 +/- 2.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2505000  |
| train/             |          |
|    actor_loss      | -8.39    |
|    critic_loss     | 0.0108   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2494999  |
---------------------------------
Eval num_timesteps=2510000, episode_reward=24025.81 +/- 17.38
Episode length: 19.40 +/- 3.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2510000  |
| train/             |          |
|    actor_loss      | -8.49    |
|    critic_loss     | 0.00937  |
|    learning_rate   | 0.0003   |
|    n_updates       | 2499999  |
---------------------------------
Eval num_timesteps=2515000, episode_reward=24010.01 +/- 15.61
Episode length: 21.60 +/- 2.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2515000  |
| train/             |          |
|    actor_loss      | -8.26    |
|    critic_loss     | 0.0132   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2504999  |
---------------------------------
Eval num_timesteps=2520000, episode_reward=23996.99 +/- 18.71
Episode length: 22.40 +/- 2.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2520000  |
| train/             |          |
|    actor_loss      | -8.25    |
|    critic_loss     | 0.00945  |
|    learning_rate   | 0.0003   |
|    n_updates       | 2509999  |
---------------------------------
Eval num_timesteps=2525000, episode_reward=24022.55 +/- 21.52
Episode length: 18.60 +/- 4.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2525000  |
| train/             |          |
|    actor_loss      | -8.3     |
|    critic_loss     | 0.0104   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2514999  |
---------------------------------
Eval num_timesteps=2530000, episode_reward=23996.62 +/- 9.37
Episode length: 23.00 +/- 2.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2530000  |
| train/             |          |
|    actor_loss      | -8.23    |
|    critic_loss     | 0.0128   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2519999  |
---------------------------------
Eval num_timesteps=2535000, episode_reward=23995.43 +/- 11.06
Episode length: 21.40 +/- 4.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2535000  |
| train/             |          |
|    actor_loss      | -8.3     |
|    critic_loss     | 0.0104   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2524999  |
---------------------------------
Eval num_timesteps=2540000, episode_reward=24011.66 +/- 22.13
Episode length: 19.80 +/- 3.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2540000  |
| train/             |          |
|    actor_loss      | -8.45    |
|    critic_loss     | 0.0103   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2529999  |
---------------------------------
Eval num_timesteps=2545000, episode_reward=24019.87 +/- 16.18
Episode length: 16.40 +/- 4.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2545000  |
| train/             |          |
|    actor_loss      | -8.41    |
|    critic_loss     | 0.0142   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2534999  |
---------------------------------
Eval num_timesteps=2550000, episode_reward=24012.72 +/- 24.57
Episode length: 20.60 +/- 4.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2550000  |
| train/             |          |
|    actor_loss      | -8.38    |
|    critic_loss     | 0.0248   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2539999  |
---------------------------------
Eval num_timesteps=2555000, episode_reward=23987.81 +/- 9.63
Episode length: 22.20 +/- 2.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2555000  |
| train/             |          |
|    actor_loss      | -8.14    |
|    critic_loss     | 0.0148   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2544999  |
---------------------------------
Eval num_timesteps=2560000, episode_reward=24013.81 +/- 17.55
Episode length: 19.20 +/- 2.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2560000  |
| train/             |          |
|    actor_loss      | -8.05    |
|    critic_loss     | 0.00919  |
|    learning_rate   | 0.0003   |
|    n_updates       | 2549999  |
---------------------------------
Eval num_timesteps=2565000, episode_reward=24002.54 +/- 8.89
Episode length: 19.40 +/- 1.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2565000  |
| train/             |          |
|    actor_loss      | -8.47    |
|    critic_loss     | 0.0111   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2554999  |
---------------------------------
Eval num_timesteps=2570000, episode_reward=24005.97 +/- 7.66
Episode length: 23.00 +/- 2.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2570000  |
| train/             |          |
|    actor_loss      | -8.59    |
|    critic_loss     | 0.00882  |
|    learning_rate   | 0.0003   |
|    n_updates       | 2559999  |
---------------------------------
Eval num_timesteps=2575000, episode_reward=24017.75 +/- 20.14
Episode length: 18.40 +/- 5.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2575000  |
| train/             |          |
|    actor_loss      | -8.43    |
|    critic_loss     | 0.00916  |
|    learning_rate   | 0.0003   |
|    n_updates       | 2564999  |
---------------------------------
Eval num_timesteps=2580000, episode_reward=24007.48 +/- 22.03
Episode length: 21.40 +/- 4.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2580000  |
| train/             |          |
|    actor_loss      | -8.54    |
|    critic_loss     | 0.0236   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2569999  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 130      |
|    ep_rew_mean     | 2.29e+04 |
| time/              |          |
|    episodes        | 16000    |
|    fps             | 193      |
|    time_elapsed    | 13356    |
|    total_timesteps | 2580474  |
| train/             |          |
|    actor_loss      | -8.62    |
|    critic_loss     | 0.0151   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2570473  |
---------------------------------
Eval num_timesteps=2585000, episode_reward=23997.45 +/- 29.58
Episode length: 24.20 +/- 5.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2585000  |
| train/             |          |
|    actor_loss      | -8.43    |
|    critic_loss     | 0.0129   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2574999  |
---------------------------------
Eval num_timesteps=2590000, episode_reward=24007.48 +/- 6.88
Episode length: 22.20 +/- 3.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2590000  |
| train/             |          |
|    actor_loss      | -8.48    |
|    critic_loss     | 0.0192   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2579999  |
---------------------------------
Eval num_timesteps=2595000, episode_reward=24023.04 +/- 13.38
Episode length: 19.40 +/- 3.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2595000  |
| train/             |          |
|    actor_loss      | -8.47    |
|    critic_loss     | 0.00894  |
|    learning_rate   | 0.0003   |
|    n_updates       | 2584999  |
---------------------------------
Eval num_timesteps=2600000, episode_reward=24020.75 +/- 12.17
Episode length: 18.60 +/- 4.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2600000  |
| train/             |          |
|    actor_loss      | -8.42    |
|    critic_loss     | 0.00945  |
|    learning_rate   | 0.0003   |
|    n_updates       | 2589999  |
---------------------------------
Eval num_timesteps=2605000, episode_reward=24008.37 +/- 20.86
Episode length: 21.40 +/- 3.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2605000  |
| train/             |          |
|    actor_loss      | -8.29    |
|    critic_loss     | 0.016    |
|    learning_rate   | 0.0003   |
|    n_updates       | 2594999  |
---------------------------------
Eval num_timesteps=2610000, episode_reward=24005.50 +/- 25.60
Episode length: 22.60 +/- 5.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2610000  |
| train/             |          |
|    actor_loss      | -8.41    |
|    critic_loss     | 0.00934  |
|    learning_rate   | 0.0003   |
|    n_updates       | 2599999  |
---------------------------------
Eval num_timesteps=2615000, episode_reward=24003.67 +/- 18.34
Episode length: 20.40 +/- 4.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2615000  |
| train/             |          |
|    actor_loss      | -8.46    |
|    critic_loss     | 0.0095   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2604999  |
---------------------------------
Eval num_timesteps=2620000, episode_reward=24011.83 +/- 24.73
Episode length: 21.20 +/- 4.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2620000  |
| train/             |          |
|    actor_loss      | -8.38    |
|    critic_loss     | 0.0103   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2609999  |
---------------------------------
Eval num_timesteps=2625000, episode_reward=24020.32 +/- 8.36
Episode length: 19.40 +/- 3.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2625000  |
| train/             |          |
|    actor_loss      | -8.43    |
|    critic_loss     | 0.0118   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2614999  |
---------------------------------
Eval num_timesteps=2630000, episode_reward=24016.44 +/- 16.66
Episode length: 19.40 +/- 5.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2630000  |
| train/             |          |
|    actor_loss      | -8.47    |
|    critic_loss     | 0.0114   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2619999  |
---------------------------------
Eval num_timesteps=2635000, episode_reward=24016.99 +/- 11.00
Episode length: 17.80 +/- 3.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2635000  |
| train/             |          |
|    actor_loss      | -8.54    |
|    critic_loss     | 0.011    |
|    learning_rate   | 0.0003   |
|    n_updates       | 2624999  |
---------------------------------
Eval num_timesteps=2640000, episode_reward=23996.34 +/- 4.94
Episode length: 22.60 +/- 1.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2640000  |
| train/             |          |
|    actor_loss      | -8.53    |
|    critic_loss     | 0.011    |
|    learning_rate   | 0.0003   |
|    n_updates       | 2629999  |
---------------------------------
Eval num_timesteps=2645000, episode_reward=24005.99 +/- 6.03
Episode length: 22.60 +/- 2.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2645000  |
| train/             |          |
|    actor_loss      | -8.33    |
|    critic_loss     | 0.0178   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2634999  |
---------------------------------
Eval num_timesteps=2650000, episode_reward=24014.85 +/- 11.93
Episode length: 22.40 +/- 3.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2650000  |
| train/             |          |
|    actor_loss      | -8.4     |
|    critic_loss     | 0.0107   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2639999  |
---------------------------------
Eval num_timesteps=2655000, episode_reward=24004.57 +/- 13.94
Episode length: 22.40 +/- 3.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2655000  |
| train/             |          |
|    actor_loss      | -8.41    |
|    critic_loss     | 0.0088   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2644999  |
---------------------------------
Eval num_timesteps=2660000, episode_reward=23980.00 +/- 26.82
Episode length: 22.60 +/- 6.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2660000  |
| train/             |          |
|    actor_loss      | -8.68    |
|    critic_loss     | 0.00838  |
|    learning_rate   | 0.0003   |
|    n_updates       | 2649999  |
---------------------------------
Eval num_timesteps=2665000, episode_reward=24012.55 +/- 13.36
Episode length: 20.20 +/- 4.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2665000  |
| train/             |          |
|    actor_loss      | -8.68    |
|    critic_loss     | 0.0122   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2654999  |
---------------------------------
Eval num_timesteps=2670000, episode_reward=24009.75 +/- 18.69
Episode length: 18.60 +/- 2.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2670000  |
| train/             |          |
|    actor_loss      | -8.6     |
|    critic_loss     | 0.0136   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2659999  |
---------------------------------
Eval num_timesteps=2675000, episode_reward=24005.78 +/- 26.38
Episode length: 22.20 +/- 4.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2675000  |
| train/             |          |
|    actor_loss      | -8.52    |
|    critic_loss     | 0.0128   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2664999  |
---------------------------------
Eval num_timesteps=2680000, episode_reward=23997.04 +/- 16.04
Episode length: 23.00 +/- 4.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2680000  |
| train/             |          |
|    actor_loss      | -8.35    |
|    critic_loss     | 0.00828  |
|    learning_rate   | 0.0003   |
|    n_updates       | 2669999  |
---------------------------------
Eval num_timesteps=2685000, episode_reward=24002.31 +/- 17.56
Episode length: 22.20 +/- 2.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2685000  |
| train/             |          |
|    actor_loss      | -8.5     |
|    critic_loss     | 0.0116   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2674999  |
---------------------------------
Eval num_timesteps=2690000, episode_reward=24010.05 +/- 29.82
Episode length: 20.00 +/- 5.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2690000  |
| train/             |          |
|    actor_loss      | -8.44    |
|    critic_loss     | 0.00948  |
|    learning_rate   | 0.0003   |
|    n_updates       | 2679999  |
---------------------------------
Eval num_timesteps=2695000, episode_reward=23989.83 +/- 13.52
Episode length: 25.60 +/- 1.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2695000  |
| train/             |          |
|    actor_loss      | -8.56    |
|    critic_loss     | 0.00773  |
|    learning_rate   | 0.0003   |
|    n_updates       | 2684999  |
---------------------------------
Eval num_timesteps=2700000, episode_reward=23984.54 +/- 10.24
Episode length: 26.40 +/- 0.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2700000  |
| train/             |          |
|    actor_loss      | -8.35    |
|    critic_loss     | 0.0143   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2689999  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 114      |
|    ep_rew_mean     | 2.29e+04 |
| time/              |          |
|    episodes        | 17000    |
|    fps             | 193      |
|    time_elapsed    | 13963    |
|    total_timesteps | 2700833  |
| train/             |          |
|    actor_loss      | -8.31    |
|    critic_loss     | 0.0118   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2690832  |
---------------------------------
Eval num_timesteps=2705000, episode_reward=23997.13 +/- 17.94
Episode length: 22.80 +/- 3.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2705000  |
| train/             |          |
|    actor_loss      | -8.26    |
|    critic_loss     | 0.00889  |
|    learning_rate   | 0.0003   |
|    n_updates       | 2694999  |
---------------------------------
Eval num_timesteps=2710000, episode_reward=23989.86 +/- 19.04
Episode length: 25.00 +/- 4.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2710000  |
| train/             |          |
|    actor_loss      | -8.43    |
|    critic_loss     | 0.0173   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2699999  |
---------------------------------
Eval num_timesteps=2715000, episode_reward=23995.33 +/- 27.31
Episode length: 21.80 +/- 7.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2715000  |
| train/             |          |
|    actor_loss      | -8.47    |
|    critic_loss     | 0.0131   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2704999  |
---------------------------------
Eval num_timesteps=2720000, episode_reward=24005.49 +/- 10.12
Episode length: 23.00 +/- 2.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2720000  |
| train/             |          |
|    actor_loss      | -8.44    |
|    critic_loss     | 0.0126   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2709999  |
---------------------------------
Eval num_timesteps=2725000, episode_reward=24015.58 +/- 12.99
Episode length: 23.00 +/- 2.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2725000  |
| train/             |          |
|    actor_loss      | -8.49    |
|    critic_loss     | 0.00898  |
|    learning_rate   | 0.0003   |
|    n_updates       | 2714999  |
---------------------------------
Eval num_timesteps=2730000, episode_reward=23993.13 +/- 21.47
Episode length: 22.80 +/- 4.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2730000  |
| train/             |          |
|    actor_loss      | -8.36    |
|    critic_loss     | 0.0165   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2719999  |
---------------------------------
Eval num_timesteps=2735000, episode_reward=24018.93 +/- 33.16
Episode length: 17.20 +/- 6.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2735000  |
| train/             |          |
|    actor_loss      | -8.51    |
|    critic_loss     | 0.0103   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2724999  |
---------------------------------
Eval num_timesteps=2740000, episode_reward=23995.79 +/- 22.70
Episode length: 22.60 +/- 3.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2740000  |
| train/             |          |
|    actor_loss      | -8.39    |
|    critic_loss     | 0.00948  |
|    learning_rate   | 0.0003   |
|    n_updates       | 2729999  |
---------------------------------
Eval num_timesteps=2745000, episode_reward=24004.73 +/- 34.11
Episode length: 21.60 +/- 8.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2745000  |
| train/             |          |
|    actor_loss      | -8.48    |
|    critic_loss     | 0.0115   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2734999  |
---------------------------------
Eval num_timesteps=2750000, episode_reward=24019.60 +/- 17.39
Episode length: 18.20 +/- 3.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2750000  |
| train/             |          |
|    actor_loss      | -8.32    |
|    critic_loss     | 0.0084   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2739999  |
---------------------------------
Eval num_timesteps=2755000, episode_reward=24009.54 +/- 13.34
Episode length: 21.60 +/- 3.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2755000  |
| train/             |          |
|    actor_loss      | -8.45    |
|    critic_loss     | 0.00958  |
|    learning_rate   | 0.0003   |
|    n_updates       | 2744999  |
---------------------------------
Eval num_timesteps=2760000, episode_reward=24013.91 +/- 19.07
Episode length: 20.60 +/- 3.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2760000  |
| train/             |          |
|    actor_loss      | -8.38    |
|    critic_loss     | 0.00819  |
|    learning_rate   | 0.0003   |
|    n_updates       | 2749999  |
---------------------------------
Eval num_timesteps=2765000, episode_reward=24005.90 +/- 24.36
Episode length: 20.00 +/- 4.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2765000  |
| train/             |          |
|    actor_loss      | -8.51    |
|    critic_loss     | 0.014    |
|    learning_rate   | 0.0003   |
|    n_updates       | 2754999  |
---------------------------------
Eval num_timesteps=2770000, episode_reward=23979.80 +/- 19.91
Episode length: 24.80 +/- 4.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2770000  |
| train/             |          |
|    actor_loss      | -8.57    |
|    critic_loss     | 0.0154   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2759999  |
---------------------------------
Eval num_timesteps=2775000, episode_reward=24006.65 +/- 18.15
Episode length: 21.80 +/- 2.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2775000  |
| train/             |          |
|    actor_loss      | -8.83    |
|    critic_loss     | 0.0202   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2764999  |
---------------------------------
Eval num_timesteps=2780000, episode_reward=24008.35 +/- 19.18
Episode length: 22.40 +/- 4.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2780000  |
| train/             |          |
|    actor_loss      | -8.62    |
|    critic_loss     | 0.0121   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2769999  |
---------------------------------
Eval num_timesteps=2785000, episode_reward=23998.09 +/- 26.06
Episode length: 24.60 +/- 5.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2785000  |
| train/             |          |
|    actor_loss      | -8.45    |
|    critic_loss     | 0.0105   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2774999  |
---------------------------------
Eval num_timesteps=2790000, episode_reward=24011.30 +/- 12.55
Episode length: 21.20 +/- 3.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2790000  |
| train/             |          |
|    actor_loss      | -8.27    |
|    critic_loss     | 0.0119   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2779999  |
---------------------------------
Eval num_timesteps=2795000, episode_reward=23999.51 +/- 15.76
Episode length: 23.00 +/- 3.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2795000  |
| train/             |          |
|    actor_loss      | -8.52    |
|    critic_loss     | 0.0127   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2784999  |
---------------------------------
Eval num_timesteps=2800000, episode_reward=23999.19 +/- 27.25
Episode length: 23.40 +/- 4.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2800000  |
| train/             |          |
|    actor_loss      | -8.54    |
|    critic_loss     | 0.0114   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2789999  |
---------------------------------
Eval num_timesteps=2805000, episode_reward=24006.36 +/- 23.45
Episode length: 20.40 +/- 4.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2805000  |
| train/             |          |
|    actor_loss      | -8.34    |
|    critic_loss     | 0.0154   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2794999  |
---------------------------------
Eval num_timesteps=2810000, episode_reward=23999.40 +/- 20.32
Episode length: 22.60 +/- 5.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2810000  |
| train/             |          |
|    actor_loss      | -8.5     |
|    critic_loss     | 0.012    |
|    learning_rate   | 0.0003   |
|    n_updates       | 2799999  |
---------------------------------
Eval num_timesteps=2815000, episode_reward=24005.28 +/- 23.61
Episode length: 20.60 +/- 5.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2815000  |
| train/             |          |
|    actor_loss      | -8.18    |
|    critic_loss     | 0.0157   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2804999  |
---------------------------------
Eval num_timesteps=2820000, episode_reward=23995.28 +/- 15.62
Episode length: 22.40 +/- 2.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2820000  |
| train/             |          |
|    actor_loss      | -8.6     |
|    critic_loss     | 0.00809  |
|    learning_rate   | 0.0003   |
|    n_updates       | 2809999  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 130      |
|    ep_rew_mean     | 2.28e+04 |
| time/              |          |
|    episodes        | 18000    |
|    fps             | 193      |
|    time_elapsed    | 14577    |
|    total_timesteps | 2824230  |
| train/             |          |
|    actor_loss      | -8.66    |
|    critic_loss     | 0.0133   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2814229  |
---------------------------------
Eval num_timesteps=2825000, episode_reward=23996.64 +/- 24.84
Episode length: 23.20 +/- 3.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2825000  |
| train/             |          |
|    actor_loss      | -8.48    |
|    critic_loss     | 0.0111   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2814999  |
---------------------------------
Eval num_timesteps=2830000, episode_reward=24000.51 +/- 14.85
Episode length: 22.80 +/- 2.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2830000  |
| train/             |          |
|    actor_loss      | -8.47    |
|    critic_loss     | 0.00822  |
|    learning_rate   | 0.0003   |
|    n_updates       | 2819999  |
---------------------------------
Eval num_timesteps=2835000, episode_reward=24006.35 +/- 24.68
Episode length: 21.80 +/- 5.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2835000  |
| train/             |          |
|    actor_loss      | -8.91    |
|    critic_loss     | 0.00923  |
|    learning_rate   | 0.0003   |
|    n_updates       | 2824999  |
---------------------------------
Eval num_timesteps=2840000, episode_reward=24003.38 +/- 12.53
Episode length: 22.40 +/- 2.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2840000  |
| train/             |          |
|    actor_loss      | -8.51    |
|    critic_loss     | 0.00683  |
|    learning_rate   | 0.0003   |
|    n_updates       | 2829999  |
---------------------------------
Eval num_timesteps=2845000, episode_reward=24012.56 +/- 26.30
Episode length: 19.80 +/- 7.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2845000  |
| train/             |          |
|    actor_loss      | -8.56    |
|    critic_loss     | 0.0086   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2834999  |
---------------------------------
Eval num_timesteps=2850000, episode_reward=24004.53 +/- 17.40
Episode length: 21.60 +/- 3.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2850000  |
| train/             |          |
|    actor_loss      | -8.54    |
|    critic_loss     | 0.0118   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2839999  |
---------------------------------
Eval num_timesteps=2855000, episode_reward=23982.32 +/- 10.70
Episode length: 22.60 +/- 4.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2855000  |
| train/             |          |
|    actor_loss      | -8.51    |
|    critic_loss     | 0.0111   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2844999  |
---------------------------------
Eval num_timesteps=2860000, episode_reward=23993.61 +/- 17.74
Episode length: 22.60 +/- 1.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2860000  |
| train/             |          |
|    actor_loss      | -8.8     |
|    critic_loss     | 0.0118   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2849999  |
---------------------------------
Eval num_timesteps=2865000, episode_reward=24016.59 +/- 7.05
Episode length: 16.40 +/- 1.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2865000  |
| train/             |          |
|    actor_loss      | -8.55    |
|    critic_loss     | 0.0412   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2854999  |
---------------------------------
Eval num_timesteps=2870000, episode_reward=23983.85 +/- 27.89
Episode length: 23.80 +/- 4.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2870000  |
| train/             |          |
|    actor_loss      | -8.44    |
|    critic_loss     | 0.01     |
|    learning_rate   | 0.0003   |
|    n_updates       | 2859999  |
---------------------------------
Eval num_timesteps=2875000, episode_reward=23985.53 +/- 28.55
Episode length: 21.60 +/- 6.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2875000  |
| train/             |          |
|    actor_loss      | -8.58    |
|    critic_loss     | 0.00722  |
|    learning_rate   | 0.0003   |
|    n_updates       | 2864999  |
---------------------------------
Eval num_timesteps=2880000, episode_reward=24016.95 +/- 13.26
Episode length: 16.80 +/- 4.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2880000  |
| train/             |          |
|    actor_loss      | -8.52    |
|    critic_loss     | 0.00985  |
|    learning_rate   | 0.0003   |
|    n_updates       | 2869999  |
---------------------------------
Eval num_timesteps=2885000, episode_reward=24001.26 +/- 17.86
Episode length: 22.80 +/- 4.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2885000  |
| train/             |          |
|    actor_loss      | -8.77    |
|    critic_loss     | 0.0112   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2874999  |
---------------------------------
Eval num_timesteps=2890000, episode_reward=23992.52 +/- 9.53
Episode length: 24.00 +/- 2.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2890000  |
| train/             |          |
|    actor_loss      | -8.5     |
|    critic_loss     | 0.00738  |
|    learning_rate   | 0.0003   |
|    n_updates       | 2879999  |
---------------------------------
Eval num_timesteps=2895000, episode_reward=23996.24 +/- 10.24
Episode length: 23.40 +/- 2.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2895000  |
| train/             |          |
|    actor_loss      | -8.52    |
|    critic_loss     | 0.111    |
|    learning_rate   | 0.0003   |
|    n_updates       | 2884999  |
---------------------------------
Eval num_timesteps=2900000, episode_reward=23984.25 +/- 21.12
Episode length: 22.80 +/- 3.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2900000  |
| train/             |          |
|    actor_loss      | -8.61    |
|    critic_loss     | 0.00748  |
|    learning_rate   | 0.0003   |
|    n_updates       | 2889999  |
---------------------------------
Eval num_timesteps=2905000, episode_reward=23989.28 +/- 13.81
Episode length: 22.40 +/- 2.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2905000  |
| train/             |          |
|    actor_loss      | -8.37    |
|    critic_loss     | 0.0136   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2894999  |
---------------------------------
Eval num_timesteps=2910000, episode_reward=23997.86 +/- 16.49
Episode length: 24.00 +/- 2.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2910000  |
| train/             |          |
|    actor_loss      | -8.67    |
|    critic_loss     | 0.0106   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2899999  |
---------------------------------
Eval num_timesteps=2915000, episode_reward=24011.54 +/- 10.36
Episode length: 18.60 +/- 4.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2915000  |
| train/             |          |
|    actor_loss      | -8.41    |
|    critic_loss     | 0.0134   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2904999  |
---------------------------------
Eval num_timesteps=2920000, episode_reward=24318.21 +/- 604.76
Episode length: 17.00 +/- 3.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | 2.43e+04 |
| time/              |          |
|    total_timesteps | 2920000  |
| train/             |          |
|    actor_loss      | -8.54    |
|    critic_loss     | 0.00878  |
|    learning_rate   | 0.0003   |
|    n_updates       | 2909999  |
---------------------------------
Eval num_timesteps=2925000, episode_reward=24014.44 +/- 11.63
Episode length: 21.80 +/- 3.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2925000  |
| train/             |          |
|    actor_loss      | -8.45    |
|    critic_loss     | 0.0273   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2914999  |
---------------------------------
Eval num_timesteps=2930000, episode_reward=24011.55 +/- 18.53
Episode length: 20.40 +/- 4.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2930000  |
| train/             |          |
|    actor_loss      | -8.58    |
|    critic_loss     | 0.0103   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2919999  |
---------------------------------
Eval num_timesteps=2935000, episode_reward=24013.11 +/- 9.29
Episode length: 18.20 +/- 3.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2935000  |
| train/             |          |
|    actor_loss      | -8.49    |
|    critic_loss     | 0.00744  |
|    learning_rate   | 0.0003   |
|    n_updates       | 2924999  |
---------------------------------
Eval num_timesteps=2940000, episode_reward=23983.48 +/- 16.11
Episode length: 24.00 +/- 2.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2940000  |
| train/             |          |
|    actor_loss      | -8.49    |
|    critic_loss     | 0.00924  |
|    learning_rate   | 0.0003   |
|    n_updates       | 2929999  |
---------------------------------
Eval num_timesteps=2945000, episode_reward=24005.22 +/- 31.25
Episode length: 21.40 +/- 5.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2945000  |
| train/             |          |
|    actor_loss      | -8.44    |
|    critic_loss     | 0.0135   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2934999  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 113      |
|    ep_rew_mean     | 2.3e+04  |
| time/              |          |
|    episodes        | 19000    |
|    fps             | 194      |
|    time_elapsed    | 15185    |
|    total_timesteps | 2946724  |
| train/             |          |
|    actor_loss      | -8.31    |
|    critic_loss     | 0.00945  |
|    learning_rate   | 0.0003   |
|    n_updates       | 2936723  |
---------------------------------
Eval num_timesteps=2950000, episode_reward=24003.24 +/- 14.75
Episode length: 21.80 +/- 3.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2950000  |
| train/             |          |
|    actor_loss      | -8.41    |
|    critic_loss     | 0.0108   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2939999  |
---------------------------------
Eval num_timesteps=2955000, episode_reward=23987.09 +/- 13.59
Episode length: 24.40 +/- 2.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2955000  |
| train/             |          |
|    actor_loss      | -8.45    |
|    critic_loss     | 0.0113   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2944999  |
---------------------------------
Eval num_timesteps=2960000, episode_reward=24001.53 +/- 19.22
Episode length: 21.40 +/- 4.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2960000  |
| train/             |          |
|    actor_loss      | -8.61    |
|    critic_loss     | 0.0153   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2949999  |
---------------------------------
Eval num_timesteps=2965000, episode_reward=24005.09 +/- 15.17
Episode length: 21.20 +/- 6.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2965000  |
| train/             |          |
|    actor_loss      | -8.39    |
|    critic_loss     | 0.015    |
|    learning_rate   | 0.0003   |
|    n_updates       | 2954999  |
---------------------------------
Eval num_timesteps=2970000, episode_reward=24003.48 +/- 28.98
Episode length: 19.40 +/- 5.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2970000  |
| train/             |          |
|    actor_loss      | -8.36    |
|    critic_loss     | 0.0173   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2959999  |
---------------------------------
Eval num_timesteps=2975000, episode_reward=24013.94 +/- 9.30
Episode length: 20.40 +/- 3.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2975000  |
| train/             |          |
|    actor_loss      | -8.46    |
|    critic_loss     | 0.0116   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2964999  |
---------------------------------
Eval num_timesteps=2980000, episode_reward=24324.51 +/- 598.31
Episode length: 18.00 +/- 4.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18       |
|    mean_reward     | 2.43e+04 |
| time/              |          |
|    total_timesteps | 2980000  |
| train/             |          |
|    actor_loss      | -8.44    |
|    critic_loss     | 0.0105   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2969999  |
---------------------------------
Eval num_timesteps=2985000, episode_reward=23997.97 +/- 20.35
Episode length: 21.80 +/- 5.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2985000  |
| train/             |          |
|    actor_loss      | -8.63    |
|    critic_loss     | 0.00957  |
|    learning_rate   | 0.0003   |
|    n_updates       | 2974999  |
---------------------------------
Eval num_timesteps=2990000, episode_reward=24011.31 +/- 5.36
Episode length: 21.20 +/- 4.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2990000  |
| train/             |          |
|    actor_loss      | -8.49    |
|    critic_loss     | 0.00821  |
|    learning_rate   | 0.0003   |
|    n_updates       | 2979999  |
---------------------------------
Eval num_timesteps=2995000, episode_reward=24012.96 +/- 23.07
Episode length: 19.40 +/- 6.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 2995000  |
| train/             |          |
|    actor_loss      | -8.29    |
|    critic_loss     | 0.0149   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2984999  |
---------------------------------
Eval num_timesteps=3000000, episode_reward=24004.83 +/- 16.97
Episode length: 19.80 +/- 3.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3000000  |
| train/             |          |
|    actor_loss      | -8.78    |
|    critic_loss     | 0.0131   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2989999  |
---------------------------------
Eval num_timesteps=3005000, episode_reward=23989.25 +/- 13.83
Episode length: 21.80 +/- 3.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3005000  |
| train/             |          |
|    actor_loss      | -8.29    |
|    critic_loss     | 0.0118   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2994999  |
---------------------------------
Eval num_timesteps=3010000, episode_reward=23997.58 +/- 13.18
Episode length: 22.60 +/- 4.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3010000  |
| train/             |          |
|    actor_loss      | -8.53    |
|    critic_loss     | 0.0188   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2999999  |
---------------------------------
Eval num_timesteps=3015000, episode_reward=23994.04 +/- 28.36
Episode length: 21.40 +/- 4.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3015000  |
| train/             |          |
|    actor_loss      | -8.67    |
|    critic_loss     | 0.0137   |
|    learning_rate   | 0.0003   |
|    n_updates       | 3004999  |
---------------------------------
Eval num_timesteps=3020000, episode_reward=24006.52 +/- 17.14
Episode length: 21.00 +/- 3.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3020000  |
| train/             |          |
|    actor_loss      | -8.49    |
|    critic_loss     | 0.0105   |
|    learning_rate   | 0.0003   |
|    n_updates       | 3009999  |
---------------------------------
Eval num_timesteps=3025000, episode_reward=24012.03 +/- 12.69
Episode length: 17.40 +/- 1.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3025000  |
| train/             |          |
|    actor_loss      | -8.51    |
|    critic_loss     | 0.00892  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3014999  |
---------------------------------
Eval num_timesteps=3030000, episode_reward=24009.56 +/- 10.71
Episode length: 19.00 +/- 2.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3030000  |
| train/             |          |
|    actor_loss      | -8.47    |
|    critic_loss     | 0.0147   |
|    learning_rate   | 0.0003   |
|    n_updates       | 3019999  |
---------------------------------
Eval num_timesteps=3035000, episode_reward=23999.99 +/- 7.66
Episode length: 22.20 +/- 2.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3035000  |
| train/             |          |
|    actor_loss      | -8.72    |
|    critic_loss     | 0.00952  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3024999  |
---------------------------------
Eval num_timesteps=3040000, episode_reward=23995.48 +/- 18.53
Episode length: 24.20 +/- 2.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3040000  |
| train/             |          |
|    actor_loss      | -8.73    |
|    critic_loss     | 0.0138   |
|    learning_rate   | 0.0003   |
|    n_updates       | 3029999  |
---------------------------------
Eval num_timesteps=3045000, episode_reward=23999.84 +/- 7.94
Episode length: 22.40 +/- 3.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3045000  |
| train/             |          |
|    actor_loss      | -8.5     |
|    critic_loss     | 0.00679  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3034999  |
---------------------------------
Eval num_timesteps=3050000, episode_reward=24010.17 +/- 21.53
Episode length: 18.20 +/- 5.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3050000  |
| train/             |          |
|    actor_loss      | -8.76    |
|    critic_loss     | 0.0108   |
|    learning_rate   | 0.0003   |
|    n_updates       | 3039999  |
---------------------------------
Eval num_timesteps=3055000, episode_reward=24009.64 +/- 12.21
Episode length: 22.40 +/- 4.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3055000  |
| train/             |          |
|    actor_loss      | -8.62    |
|    critic_loss     | 0.00931  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3044999  |
---------------------------------
Eval num_timesteps=3060000, episode_reward=24007.89 +/- 19.30
Episode length: 21.00 +/- 4.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3060000  |
| train/             |          |
|    actor_loss      | -8.9     |
|    critic_loss     | 0.0161   |
|    learning_rate   | 0.0003   |
|    n_updates       | 3049999  |
---------------------------------
Eval num_timesteps=3065000, episode_reward=23999.74 +/- 19.02
Episode length: 22.60 +/- 3.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3065000  |
| train/             |          |
|    actor_loss      | -8.56    |
|    critic_loss     | 0.0124   |
|    learning_rate   | 0.0003   |
|    n_updates       | 3054999  |
---------------------------------
Eval num_timesteps=3070000, episode_reward=23998.78 +/- 18.17
Episode length: 19.80 +/- 4.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3070000  |
| train/             |          |
|    actor_loss      | -8.71    |
|    critic_loss     | 0.00896  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3059999  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 113      |
|    ep_rew_mean     | 2.27e+04 |
| time/              |          |
|    episodes        | 20000    |
|    fps             | 194      |
|    time_elapsed    | 15791    |
|    total_timesteps | 3070494  |
| train/             |          |
|    actor_loss      | -8.65    |
|    critic_loss     | 0.00936  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3060493  |
---------------------------------
Eval num_timesteps=3075000, episode_reward=23998.97 +/- 16.39
Episode length: 22.60 +/- 3.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3075000  |
| train/             |          |
|    actor_loss      | -8.81    |
|    critic_loss     | 0.00794  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3064999  |
---------------------------------
Eval num_timesteps=3080000, episode_reward=24008.96 +/- 17.15
Episode length: 19.20 +/- 5.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3080000  |
| train/             |          |
|    actor_loss      | -8.69    |
|    critic_loss     | 0.00928  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3069999  |
---------------------------------
Eval num_timesteps=3085000, episode_reward=24298.72 +/- 597.82
Episode length: 22.20 +/- 1.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.2     |
|    mean_reward     | 2.43e+04 |
| time/              |          |
|    total_timesteps | 3085000  |
| train/             |          |
|    actor_loss      | -8.77    |
|    critic_loss     | 0.00975  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3074999  |
---------------------------------
Eval num_timesteps=3090000, episode_reward=24012.60 +/- 14.12
Episode length: 20.40 +/- 3.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3090000  |
| train/             |          |
|    actor_loss      | -8.61    |
|    critic_loss     | 0.00821  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3079999  |
---------------------------------
Eval num_timesteps=3095000, episode_reward=24006.56 +/- 10.70
Episode length: 21.60 +/- 4.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3095000  |
| train/             |          |
|    actor_loss      | -8.76    |
|    critic_loss     | 0.00977  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3084999  |
---------------------------------
Eval num_timesteps=3100000, episode_reward=24004.04 +/- 19.93
Episode length: 21.60 +/- 5.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3100000  |
| train/             |          |
|    actor_loss      | -8.56    |
|    critic_loss     | 0.00919  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3089999  |
---------------------------------
Eval num_timesteps=3105000, episode_reward=24005.55 +/- 15.65
Episode length: 21.40 +/- 3.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3105000  |
| train/             |          |
|    actor_loss      | -8.73    |
|    critic_loss     | 0.0112   |
|    learning_rate   | 0.0003   |
|    n_updates       | 3094999  |
---------------------------------
Eval num_timesteps=3110000, episode_reward=24008.42 +/- 18.61
Episode length: 19.00 +/- 2.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3110000  |
| train/             |          |
|    actor_loss      | -8.82    |
|    critic_loss     | 0.00914  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3099999  |
---------------------------------
Eval num_timesteps=3115000, episode_reward=24009.27 +/- 18.90
Episode length: 20.60 +/- 3.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3115000  |
| train/             |          |
|    actor_loss      | -8.76    |
|    critic_loss     | 0.0147   |
|    learning_rate   | 0.0003   |
|    n_updates       | 3104999  |
---------------------------------
Eval num_timesteps=3120000, episode_reward=23992.75 +/- 14.09
Episode length: 25.20 +/- 3.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3120000  |
| train/             |          |
|    actor_loss      | -8.67    |
|    critic_loss     | 0.00915  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3109999  |
---------------------------------
Eval num_timesteps=3125000, episode_reward=23997.73 +/- 14.03
Episode length: 21.00 +/- 2.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3125000  |
| train/             |          |
|    actor_loss      | -8.78    |
|    critic_loss     | 0.0124   |
|    learning_rate   | 0.0003   |
|    n_updates       | 3114999  |
---------------------------------
Eval num_timesteps=3130000, episode_reward=24014.50 +/- 13.80
Episode length: 19.80 +/- 3.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3130000  |
| train/             |          |
|    actor_loss      | -8.76    |
|    critic_loss     | 0.0741   |
|    learning_rate   | 0.0003   |
|    n_updates       | 3119999  |
---------------------------------
Eval num_timesteps=3135000, episode_reward=24007.80 +/- 24.59
Episode length: 19.60 +/- 5.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3135000  |
| train/             |          |
|    actor_loss      | -8.67    |
|    critic_loss     | 0.00793  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3124999  |
---------------------------------
Eval num_timesteps=3140000, episode_reward=24002.02 +/- 14.88
Episode length: 22.20 +/- 2.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3140000  |
| train/             |          |
|    actor_loss      | -8.89    |
|    critic_loss     | 0.0115   |
|    learning_rate   | 0.0003   |
|    n_updates       | 3129999  |
---------------------------------
Eval num_timesteps=3145000, episode_reward=23991.54 +/- 8.65
Episode length: 25.40 +/- 1.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3145000  |
| train/             |          |
|    actor_loss      | -8.78    |
|    critic_loss     | 0.00829  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3134999  |
---------------------------------
Eval num_timesteps=3150000, episode_reward=24008.64 +/- 14.18
Episode length: 20.20 +/- 3.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3150000  |
| train/             |          |
|    actor_loss      | -8.61    |
|    critic_loss     | 0.00804  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3139999  |
---------------------------------
Eval num_timesteps=3155000, episode_reward=24000.05 +/- 15.76
Episode length: 20.20 +/- 2.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3155000  |
| train/             |          |
|    actor_loss      | -8.79    |
|    critic_loss     | 0.00836  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3144999  |
---------------------------------
Eval num_timesteps=3160000, episode_reward=23993.18 +/- 19.57
Episode length: 24.80 +/- 4.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3160000  |
| train/             |          |
|    actor_loss      | -8.82    |
|    critic_loss     | 0.00755  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3149999  |
---------------------------------
Eval num_timesteps=3165000, episode_reward=24008.05 +/- 18.10
Episode length: 19.40 +/- 3.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3165000  |
| train/             |          |
|    actor_loss      | -8.82    |
|    critic_loss     | 0.0115   |
|    learning_rate   | 0.0003   |
|    n_updates       | 3154999  |
---------------------------------
Eval num_timesteps=3170000, episode_reward=23986.56 +/- 24.83
Episode length: 21.80 +/- 4.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3170000  |
| train/             |          |
|    actor_loss      | -8.85    |
|    critic_loss     | 0.00874  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3159999  |
---------------------------------
Eval num_timesteps=3175000, episode_reward=24011.04 +/- 6.97
Episode length: 22.20 +/- 3.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3175000  |
| train/             |          |
|    actor_loss      | -8.76    |
|    critic_loss     | 0.00718  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3164999  |
---------------------------------
Eval num_timesteps=3180000, episode_reward=24004.90 +/- 4.67
Episode length: 19.80 +/- 2.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3180000  |
| train/             |          |
|    actor_loss      | -8.83    |
|    critic_loss     | 0.0128   |
|    learning_rate   | 0.0003   |
|    n_updates       | 3169999  |
---------------------------------
Eval num_timesteps=3185000, episode_reward=24012.94 +/- 14.22
Episode length: 20.00 +/- 4.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3185000  |
| train/             |          |
|    actor_loss      | -8.78    |
|    critic_loss     | 0.0111   |
|    learning_rate   | 0.0003   |
|    n_updates       | 3174999  |
---------------------------------
Eval num_timesteps=3190000, episode_reward=24028.37 +/- 24.06
Episode length: 14.60 +/- 5.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3190000  |
| train/             |          |
|    actor_loss      | -8.86    |
|    critic_loss     | 0.00773  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3179999  |
---------------------------------
Eval num_timesteps=3195000, episode_reward=24014.10 +/- 10.67
Episode length: 18.20 +/- 4.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3195000  |
| train/             |          |
|    actor_loss      | -8.93    |
|    critic_loss     | 0.0123   |
|    learning_rate   | 0.0003   |
|    n_updates       | 3184999  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 116      |
|    ep_rew_mean     | 2.25e+04 |
| time/              |          |
|    episodes        | 21000    |
|    fps             | 194      |
|    time_elapsed    | 16411    |
|    total_timesteps | 3195406  |
| train/             |          |
|    actor_loss      | -8.8     |
|    critic_loss     | 0.00849  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3185405  |
---------------------------------
Eval num_timesteps=3200000, episode_reward=24015.19 +/- 16.06
Episode length: 17.80 +/- 3.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3200000  |
| train/             |          |
|    actor_loss      | -8.8     |
|    critic_loss     | 0.00692  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3189999  |
---------------------------------
Eval num_timesteps=3205000, episode_reward=24023.67 +/- 11.84
Episode length: 18.20 +/- 1.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3205000  |
| train/             |          |
|    actor_loss      | -8.95    |
|    critic_loss     | 0.0148   |
|    learning_rate   | 0.0003   |
|    n_updates       | 3194999  |
---------------------------------
Eval num_timesteps=3210000, episode_reward=24005.20 +/- 16.29
Episode length: 22.20 +/- 2.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3210000  |
| train/             |          |
|    actor_loss      | -8.95    |
|    critic_loss     | 0.0108   |
|    learning_rate   | 0.0003   |
|    n_updates       | 3199999  |
---------------------------------
Eval num_timesteps=3215000, episode_reward=24012.15 +/- 14.47
Episode length: 18.80 +/- 3.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3215000  |
| train/             |          |
|    actor_loss      | -8.84    |
|    critic_loss     | 0.00787  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3204999  |
---------------------------------
Eval num_timesteps=3220000, episode_reward=24006.07 +/- 13.21
Episode length: 21.40 +/- 2.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3220000  |
| train/             |          |
|    actor_loss      | -8.85    |
|    critic_loss     | 0.00806  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3209999  |
---------------------------------
Eval num_timesteps=3225000, episode_reward=24008.06 +/- 16.40
Episode length: 19.60 +/- 4.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3225000  |
| train/             |          |
|    actor_loss      | -8.97    |
|    critic_loss     | 0.00851  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3214999  |
---------------------------------
Eval num_timesteps=3230000, episode_reward=24014.13 +/- 9.69
Episode length: 17.60 +/- 3.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3230000  |
| train/             |          |
|    actor_loss      | -8.88    |
|    critic_loss     | 0.0111   |
|    learning_rate   | 0.0003   |
|    n_updates       | 3219999  |
---------------------------------
Eval num_timesteps=3235000, episode_reward=24000.28 +/- 15.83
Episode length: 20.60 +/- 3.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3235000  |
| train/             |          |
|    actor_loss      | -8.95    |
|    critic_loss     | 0.00873  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3224999  |
---------------------------------
Eval num_timesteps=3240000, episode_reward=24011.23 +/- 11.71
Episode length: 19.80 +/- 2.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3240000  |
| train/             |          |
|    actor_loss      | -8.94    |
|    critic_loss     | 0.00781  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3229999  |
---------------------------------
Eval num_timesteps=3245000, episode_reward=24304.59 +/- 600.09
Episode length: 20.00 +/- 2.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20       |
|    mean_reward     | 2.43e+04 |
| time/              |          |
|    total_timesteps | 3245000  |
| train/             |          |
|    actor_loss      | -8.99    |
|    critic_loss     | 0.00607  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3234999  |
---------------------------------
Eval num_timesteps=3250000, episode_reward=23987.20 +/- 4.48
Episode length: 25.60 +/- 1.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3250000  |
| train/             |          |
|    actor_loss      | -8.95    |
|    critic_loss     | 0.00836  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3239999  |
---------------------------------
Eval num_timesteps=3255000, episode_reward=24000.16 +/- 12.52
Episode length: 22.40 +/- 3.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3255000  |
| train/             |          |
|    actor_loss      | -8.97    |
|    critic_loss     | 0.00883  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3244999  |
---------------------------------
Eval num_timesteps=3260000, episode_reward=24002.34 +/- 4.33
Episode length: 21.00 +/- 3.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3260000  |
| train/             |          |
|    actor_loss      | -8.96    |
|    critic_loss     | 0.0214   |
|    learning_rate   | 0.0003   |
|    n_updates       | 3249999  |
---------------------------------
Eval num_timesteps=3265000, episode_reward=24000.87 +/- 12.92
Episode length: 23.40 +/- 3.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3265000  |
| train/             |          |
|    actor_loss      | -8.98    |
|    critic_loss     | 0.00703  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3254999  |
---------------------------------
Eval num_timesteps=3270000, episode_reward=24002.50 +/- 18.44
Episode length: 21.60 +/- 3.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3270000  |
| train/             |          |
|    actor_loss      | -8.96    |
|    critic_loss     | 0.00667  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3259999  |
---------------------------------
Eval num_timesteps=3275000, episode_reward=24016.75 +/- 9.75
Episode length: 19.00 +/- 2.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3275000  |
| train/             |          |
|    actor_loss      | -8.98    |
|    critic_loss     | 0.00789  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3264999  |
---------------------------------
Eval num_timesteps=3280000, episode_reward=24024.11 +/- 23.29
Episode length: 15.80 +/- 2.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3280000  |
| train/             |          |
|    actor_loss      | -8.9     |
|    critic_loss     | 0.00879  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3269999  |
---------------------------------
Eval num_timesteps=3285000, episode_reward=23996.59 +/- 20.08
Episode length: 21.60 +/- 3.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3285000  |
| train/             |          |
|    actor_loss      | -8.92    |
|    critic_loss     | 0.0109   |
|    learning_rate   | 0.0003   |
|    n_updates       | 3274999  |
---------------------------------
Eval num_timesteps=3290000, episode_reward=24318.09 +/- 607.45
Episode length: 16.60 +/- 7.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | 2.43e+04 |
| time/              |          |
|    total_timesteps | 3290000  |
| train/             |          |
|    actor_loss      | -8.92    |
|    critic_loss     | 0.00787  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3279999  |
---------------------------------
Eval num_timesteps=3295000, episode_reward=24022.34 +/- 19.39
Episode length: 16.00 +/- 4.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3295000  |
| train/             |          |
|    actor_loss      | -8.95    |
|    critic_loss     | 0.00806  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3284999  |
---------------------------------
Eval num_timesteps=3300000, episode_reward=24303.06 +/- 593.25
Episode length: 20.60 +/- 3.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.6     |
|    mean_reward     | 2.43e+04 |
| time/              |          |
|    total_timesteps | 3300000  |
| train/             |          |
|    actor_loss      | -9.02    |
|    critic_loss     | 0.0116   |
|    learning_rate   | 0.0003   |
|    n_updates       | 3289999  |
---------------------------------
Eval num_timesteps=3305000, episode_reward=24002.95 +/- 12.97
Episode length: 21.20 +/- 2.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3305000  |
| train/             |          |
|    actor_loss      | -8.96    |
|    critic_loss     | 0.00728  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3294999  |
---------------------------------
Eval num_timesteps=3310000, episode_reward=24009.14 +/- 8.15
Episode length: 19.00 +/- 3.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3310000  |
| train/             |          |
|    actor_loss      | -8.97    |
|    critic_loss     | 0.00783  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3299999  |
---------------------------------
Eval num_timesteps=3315000, episode_reward=23990.74 +/- 5.49
Episode length: 24.60 +/- 1.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3315000  |
| train/             |          |
|    actor_loss      | -8.97    |
|    critic_loss     | 0.011    |
|    learning_rate   | 0.0003   |
|    n_updates       | 3304999  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 131      |
|    ep_rew_mean     | 2.26e+04 |
| time/              |          |
|    episodes        | 22000    |
|    fps             | 194      |
|    time_elapsed    | 17018    |
|    total_timesteps | 3318460  |
| train/             |          |
|    actor_loss      | -8.9     |
|    critic_loss     | 0.00681  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3308459  |
---------------------------------
Eval num_timesteps=3320000, episode_reward=24005.42 +/- 21.86
Episode length: 20.40 +/- 4.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3320000  |
| train/             |          |
|    actor_loss      | -8.97    |
|    critic_loss     | 0.00893  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3309999  |
---------------------------------
Eval num_timesteps=3325000, episode_reward=23995.69 +/- 5.89
Episode length: 24.80 +/- 1.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3325000  |
| train/             |          |
|    actor_loss      | -8.85    |
|    critic_loss     | 0.0152   |
|    learning_rate   | 0.0003   |
|    n_updates       | 3314999  |
---------------------------------
Eval num_timesteps=3330000, episode_reward=24010.61 +/- 22.66
Episode length: 18.20 +/- 5.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3330000  |
| train/             |          |
|    actor_loss      | -8.96    |
|    critic_loss     | 0.00837  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3319999  |
---------------------------------
Eval num_timesteps=3335000, episode_reward=24011.48 +/- 6.75
Episode length: 19.20 +/- 2.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3335000  |
| train/             |          |
|    actor_loss      | -8.9     |
|    critic_loss     | 0.00894  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3324999  |
---------------------------------
Eval num_timesteps=3340000, episode_reward=24003.27 +/- 13.04
Episode length: 20.20 +/- 4.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3340000  |
| train/             |          |
|    actor_loss      | -9       |
|    critic_loss     | 0.118    |
|    learning_rate   | 0.0003   |
|    n_updates       | 3329999  |
---------------------------------
Eval num_timesteps=3345000, episode_reward=24003.87 +/- 16.22
Episode length: 21.60 +/- 5.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3345000  |
| train/             |          |
|    actor_loss      | -8.91    |
|    critic_loss     | 0.00939  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3334999  |
---------------------------------
Eval num_timesteps=3350000, episode_reward=24009.38 +/- 22.14
Episode length: 19.60 +/- 4.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3350000  |
| train/             |          |
|    actor_loss      | -9.02    |
|    critic_loss     | 0.0114   |
|    learning_rate   | 0.0003   |
|    n_updates       | 3339999  |
---------------------------------
Eval num_timesteps=3355000, episode_reward=24004.26 +/- 19.21
Episode length: 21.00 +/- 5.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3355000  |
| train/             |          |
|    actor_loss      | -9       |
|    critic_loss     | 0.00824  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3344999  |
---------------------------------
Eval num_timesteps=3360000, episode_reward=23990.37 +/- 12.03
Episode length: 23.80 +/- 3.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3360000  |
| train/             |          |
|    actor_loss      | -8.96    |
|    critic_loss     | 0.00778  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3349999  |
---------------------------------
Eval num_timesteps=3365000, episode_reward=24013.12 +/- 17.31
Episode length: 18.60 +/- 5.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3365000  |
| train/             |          |
|    actor_loss      | -8.97    |
|    critic_loss     | 0.00954  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3354999  |
---------------------------------
Eval num_timesteps=3370000, episode_reward=24001.76 +/- 15.69
Episode length: 22.60 +/- 3.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3370000  |
| train/             |          |
|    actor_loss      | -8.9     |
|    critic_loss     | 0.0111   |
|    learning_rate   | 0.0003   |
|    n_updates       | 3359999  |
---------------------------------
Eval num_timesteps=3375000, episode_reward=23993.86 +/- 11.75
Episode length: 22.20 +/- 2.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3375000  |
| train/             |          |
|    actor_loss      | -9       |
|    critic_loss     | 0.00604  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3364999  |
---------------------------------
Eval num_timesteps=3380000, episode_reward=23999.65 +/- 9.74
Episode length: 24.60 +/- 2.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3380000  |
| train/             |          |
|    actor_loss      | -8.97    |
|    critic_loss     | 0.00599  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3369999  |
---------------------------------
Eval num_timesteps=3385000, episode_reward=23996.23 +/- 13.87
Episode length: 23.00 +/- 3.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3385000  |
| train/             |          |
|    actor_loss      | -8.94    |
|    critic_loss     | 0.0286   |
|    learning_rate   | 0.0003   |
|    n_updates       | 3374999  |
---------------------------------
Eval num_timesteps=3390000, episode_reward=24012.19 +/- 11.12
Episode length: 20.60 +/- 4.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3390000  |
| train/             |          |
|    actor_loss      | -8.94    |
|    critic_loss     | 0.00722  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3379999  |
---------------------------------
Eval num_timesteps=3395000, episode_reward=24004.80 +/- 14.13
Episode length: 21.00 +/- 5.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3395000  |
| train/             |          |
|    actor_loss      | -9.01    |
|    critic_loss     | 0.00955  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3384999  |
---------------------------------
Eval num_timesteps=3400000, episode_reward=24002.17 +/- 8.26
Episode length: 22.20 +/- 2.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3400000  |
| train/             |          |
|    actor_loss      | -9.03    |
|    critic_loss     | 0.00739  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3389999  |
---------------------------------
Eval num_timesteps=3405000, episode_reward=24004.61 +/- 10.22
Episode length: 20.80 +/- 3.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3405000  |
| train/             |          |
|    actor_loss      | -8.96    |
|    critic_loss     | 0.00721  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3394999  |
---------------------------------
Eval num_timesteps=3410000, episode_reward=24011.59 +/- 8.34
Episode length: 19.40 +/- 4.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3410000  |
| train/             |          |
|    actor_loss      | -8.99    |
|    critic_loss     | 0.00921  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3399999  |
---------------------------------
Eval num_timesteps=3415000, episode_reward=24006.51 +/- 12.87
Episode length: 20.80 +/- 3.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3415000  |
| train/             |          |
|    actor_loss      | -9.02    |
|    critic_loss     | 0.00684  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3404999  |
---------------------------------
Eval num_timesteps=3420000, episode_reward=24012.47 +/- 17.85
Episode length: 18.20 +/- 3.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3420000  |
| train/             |          |
|    actor_loss      | -9.08    |
|    critic_loss     | 0.00846  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3409999  |
---------------------------------
Eval num_timesteps=3425000, episode_reward=24007.93 +/- 17.70
Episode length: 20.20 +/- 5.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3425000  |
| train/             |          |
|    actor_loss      | -9.08    |
|    critic_loss     | 0.00985  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3414999  |
---------------------------------
Eval num_timesteps=3430000, episode_reward=24009.25 +/- 10.85
Episode length: 20.80 +/- 2.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3430000  |
| train/             |          |
|    actor_loss      | -8.96    |
|    critic_loss     | 0.0095   |
|    learning_rate   | 0.0003   |
|    n_updates       | 3419999  |
---------------------------------
Eval num_timesteps=3435000, episode_reward=24323.79 +/- 590.89
Episode length: 17.40 +/- 4.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.4     |
|    mean_reward     | 2.43e+04 |
| time/              |          |
|    total_timesteps | 3435000  |
| train/             |          |
|    actor_loss      | -8.98    |
|    critic_loss     | 0.00652  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3424999  |
---------------------------------
Eval num_timesteps=3440000, episode_reward=24016.07 +/- 15.13
Episode length: 17.60 +/- 4.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3440000  |
| train/             |          |
|    actor_loss      | -9.05    |
|    critic_loss     | 0.00512  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3429999  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 106      |
|    ep_rew_mean     | 2.23e+04 |
| time/              |          |
|    episodes        | 23000    |
|    fps             | 195      |
|    time_elapsed    | 17633    |
|    total_timesteps | 3440976  |
| train/             |          |
|    actor_loss      | -9.05    |
|    critic_loss     | 0.00783  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3430975  |
---------------------------------
Eval num_timesteps=3445000, episode_reward=24008.90 +/- 15.84
Episode length: 20.80 +/- 4.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3445000  |
| train/             |          |
|    actor_loss      | -9.02    |
|    critic_loss     | 0.00848  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3434999  |
---------------------------------
Eval num_timesteps=3450000, episode_reward=24004.49 +/- 23.93
Episode length: 20.40 +/- 5.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3450000  |
| train/             |          |
|    actor_loss      | -9.05    |
|    critic_loss     | 0.00708  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3439999  |
---------------------------------
Eval num_timesteps=3455000, episode_reward=24016.21 +/- 18.42
Episode length: 20.00 +/- 2.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3455000  |
| train/             |          |
|    actor_loss      | -9.04    |
|    critic_loss     | 0.00568  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3444999  |
---------------------------------
Eval num_timesteps=3460000, episode_reward=23999.32 +/- 17.82
Episode length: 21.40 +/- 3.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3460000  |
| train/             |          |
|    actor_loss      | -8.99    |
|    critic_loss     | 0.00603  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3449999  |
---------------------------------
Eval num_timesteps=3465000, episode_reward=24018.16 +/- 8.90
Episode length: 18.60 +/- 1.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3465000  |
| train/             |          |
|    actor_loss      | -9.03    |
|    critic_loss     | 0.00738  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3454999  |
---------------------------------
Eval num_timesteps=3470000, episode_reward=24010.97 +/- 9.96
Episode length: 19.80 +/- 1.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3470000  |
| train/             |          |
|    actor_loss      | -9.13    |
|    critic_loss     | 0.00708  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3459999  |
---------------------------------
Eval num_timesteps=3475000, episode_reward=23987.30 +/- 8.62
Episode length: 24.60 +/- 2.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3475000  |
| train/             |          |
|    actor_loss      | -9       |
|    critic_loss     | 0.00637  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3464999  |
---------------------------------
Eval num_timesteps=3480000, episode_reward=24004.19 +/- 22.95
Episode length: 19.40 +/- 4.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3480000  |
| train/             |          |
|    actor_loss      | -9.02    |
|    critic_loss     | 0.00858  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3469999  |
---------------------------------
Eval num_timesteps=3485000, episode_reward=24004.50 +/- 11.99
Episode length: 20.80 +/- 3.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3485000  |
| train/             |          |
|    actor_loss      | -9.04    |
|    critic_loss     | 0.00923  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3474999  |
---------------------------------
Eval num_timesteps=3490000, episode_reward=24008.49 +/- 7.81
Episode length: 18.20 +/- 2.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3490000  |
| train/             |          |
|    actor_loss      | -8.99    |
|    critic_loss     | 0.0138   |
|    learning_rate   | 0.0003   |
|    n_updates       | 3479999  |
---------------------------------
Eval num_timesteps=3495000, episode_reward=23992.33 +/- 10.94
Episode length: 25.00 +/- 2.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3495000  |
| train/             |          |
|    actor_loss      | -9.1     |
|    critic_loss     | 0.00554  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3484999  |
---------------------------------
Eval num_timesteps=3500000, episode_reward=23991.61 +/- 6.45
Episode length: 23.60 +/- 1.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3500000  |
| train/             |          |
|    actor_loss      | -9.04    |
|    critic_loss     | 0.00929  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3489999  |
---------------------------------
Eval num_timesteps=3505000, episode_reward=24008.32 +/- 9.15
Episode length: 19.80 +/- 2.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3505000  |
| train/             |          |
|    actor_loss      | -9.02    |
|    critic_loss     | 0.00802  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3494999  |
---------------------------------
Eval num_timesteps=3510000, episode_reward=24001.68 +/- 7.09
Episode length: 23.40 +/- 2.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3510000  |
| train/             |          |
|    actor_loss      | -9.11    |
|    critic_loss     | 0.0105   |
|    learning_rate   | 0.0003   |
|    n_updates       | 3499999  |
---------------------------------
Eval num_timesteps=3515000, episode_reward=24016.81 +/- 9.08
Episode length: 18.40 +/- 3.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3515000  |
| train/             |          |
|    actor_loss      | -9.06    |
|    critic_loss     | 0.00757  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3504999  |
---------------------------------
Eval num_timesteps=3520000, episode_reward=24022.15 +/- 10.32
Episode length: 17.80 +/- 1.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3520000  |
| train/             |          |
|    actor_loss      | -9.09    |
|    critic_loss     | 0.0101   |
|    learning_rate   | 0.0003   |
|    n_updates       | 3509999  |
---------------------------------
Eval num_timesteps=3525000, episode_reward=24024.09 +/- 18.74
Episode length: 16.40 +/- 5.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3525000  |
| train/             |          |
|    actor_loss      | -9.02    |
|    critic_loss     | 0.01     |
|    learning_rate   | 0.0003   |
|    n_updates       | 3514999  |
---------------------------------
Eval num_timesteps=3530000, episode_reward=23998.71 +/- 16.85
Episode length: 22.00 +/- 3.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3530000  |
| train/             |          |
|    actor_loss      | -9.01    |
|    critic_loss     | 0.00755  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3519999  |
---------------------------------
Eval num_timesteps=3535000, episode_reward=23992.05 +/- 17.30
Episode length: 23.60 +/- 3.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3535000  |
| train/             |          |
|    actor_loss      | -8.98    |
|    critic_loss     | 0.00712  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3524999  |
---------------------------------
Eval num_timesteps=3540000, episode_reward=24003.85 +/- 7.70
Episode length: 20.80 +/- 2.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3540000  |
| train/             |          |
|    actor_loss      | -8.98    |
|    critic_loss     | 0.0132   |
|    learning_rate   | 0.0003   |
|    n_updates       | 3529999  |
---------------------------------
Eval num_timesteps=3545000, episode_reward=24006.53 +/- 15.96
Episode length: 19.20 +/- 4.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3545000  |
| train/             |          |
|    actor_loss      | -9.14    |
|    critic_loss     | 0.00933  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3534999  |
---------------------------------
Eval num_timesteps=3550000, episode_reward=24002.16 +/- 10.16
Episode length: 22.80 +/- 2.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3550000  |
| train/             |          |
|    actor_loss      | -9       |
|    critic_loss     | 0.00652  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3539999  |
---------------------------------
Eval num_timesteps=3555000, episode_reward=24014.59 +/- 10.18
Episode length: 19.80 +/- 2.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3555000  |
| train/             |          |
|    actor_loss      | -9.04    |
|    critic_loss     | 0.00968  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3544999  |
---------------------------------
Eval num_timesteps=3560000, episode_reward=24006.72 +/- 9.55
Episode length: 19.20 +/- 1.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3560000  |
| train/             |          |
|    actor_loss      | -8.98    |
|    critic_loss     | 0.00819  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3549999  |
---------------------------------
Eval num_timesteps=3565000, episode_reward=24011.19 +/- 18.66
Episode length: 20.00 +/- 3.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3565000  |
| train/             |          |
|    actor_loss      | -9.05    |
|    critic_loss     | 0.0226   |
|    learning_rate   | 0.0003   |
|    n_updates       | 3554999  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 114      |
|    ep_rew_mean     | 2.33e+04 |
| time/              |          |
|    episodes        | 24000    |
|    fps             | 195      |
|    time_elapsed    | 18266    |
|    total_timesteps | 3567277  |
| train/             |          |
|    actor_loss      | -9.02    |
|    critic_loss     | 0.0165   |
|    learning_rate   | 0.0003   |
|    n_updates       | 3557276  |
---------------------------------
Eval num_timesteps=3570000, episode_reward=24015.74 +/- 18.28
Episode length: 19.60 +/- 3.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3570000  |
| train/             |          |
|    actor_loss      | -9.06    |
|    critic_loss     | 0.0065   |
|    learning_rate   | 0.0003   |
|    n_updates       | 3559999  |
---------------------------------
Eval num_timesteps=3575000, episode_reward=24006.00 +/- 23.08
Episode length: 19.80 +/- 5.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3575000  |
| train/             |          |
|    actor_loss      | -9.08    |
|    critic_loss     | 0.00964  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3564999  |
---------------------------------
Eval num_timesteps=3580000, episode_reward=24007.96 +/- 8.19
Episode length: 20.20 +/- 1.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3580000  |
| train/             |          |
|    actor_loss      | -9.06    |
|    critic_loss     | 0.0098   |
|    learning_rate   | 0.0003   |
|    n_updates       | 3569999  |
---------------------------------
Eval num_timesteps=3585000, episode_reward=24015.66 +/- 19.40
Episode length: 17.60 +/- 4.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3585000  |
| train/             |          |
|    actor_loss      | -9.09    |
|    critic_loss     | 0.0166   |
|    learning_rate   | 0.0003   |
|    n_updates       | 3574999  |
---------------------------------
Eval num_timesteps=3590000, episode_reward=23998.73 +/- 22.18
Episode length: 21.80 +/- 5.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3590000  |
| train/             |          |
|    actor_loss      | -9.04    |
|    critic_loss     | 0.00767  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3579999  |
---------------------------------
Eval num_timesteps=3595000, episode_reward=24004.32 +/- 20.24
Episode length: 22.40 +/- 4.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3595000  |
| train/             |          |
|    actor_loss      | -9.04    |
|    critic_loss     | 0.00733  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3584999  |
---------------------------------
Eval num_timesteps=3600000, episode_reward=23992.58 +/- 14.94
Episode length: 23.80 +/- 3.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3600000  |
| train/             |          |
|    actor_loss      | -9.06    |
|    critic_loss     | 0.00818  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3589999  |
---------------------------------
Eval num_timesteps=3605000, episode_reward=24003.85 +/- 13.49
Episode length: 21.60 +/- 2.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3605000  |
| train/             |          |
|    actor_loss      | -9.11    |
|    critic_loss     | 0.0086   |
|    learning_rate   | 0.0003   |
|    n_updates       | 3594999  |
---------------------------------
Eval num_timesteps=3610000, episode_reward=24013.32 +/- 12.74
Episode length: 18.00 +/- 3.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3610000  |
| train/             |          |
|    actor_loss      | -9.06    |
|    critic_loss     | 0.00956  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3599999  |
---------------------------------
Eval num_timesteps=3615000, episode_reward=24001.51 +/- 19.65
Episode length: 20.80 +/- 5.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3615000  |
| train/             |          |
|    actor_loss      | -9.12    |
|    critic_loss     | 0.0107   |
|    learning_rate   | 0.0003   |
|    n_updates       | 3604999  |
---------------------------------
Eval num_timesteps=3620000, episode_reward=23985.50 +/- 13.46
Episode length: 24.20 +/- 3.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3620000  |
| train/             |          |
|    actor_loss      | -9.07    |
|    critic_loss     | 0.0103   |
|    learning_rate   | 0.0003   |
|    n_updates       | 3609999  |
---------------------------------
Eval num_timesteps=3625000, episode_reward=24010.80 +/- 19.26
Episode length: 20.80 +/- 3.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3625000  |
| train/             |          |
|    actor_loss      | -9.05    |
|    critic_loss     | 0.00692  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3614999  |
---------------------------------
Eval num_timesteps=3630000, episode_reward=24011.53 +/- 6.76
Episode length: 20.60 +/- 4.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3630000  |
| train/             |          |
|    actor_loss      | -9.09    |
|    critic_loss     | 0.00617  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3619999  |
---------------------------------
Eval num_timesteps=3635000, episode_reward=24014.13 +/- 8.61
Episode length: 18.80 +/- 1.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3635000  |
| train/             |          |
|    actor_loss      | -9       |
|    critic_loss     | 0.00818  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3624999  |
---------------------------------
Eval num_timesteps=3640000, episode_reward=24009.88 +/- 13.56
Episode length: 19.00 +/- 3.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3640000  |
| train/             |          |
|    actor_loss      | -9.08    |
|    critic_loss     | 0.0213   |
|    learning_rate   | 0.0003   |
|    n_updates       | 3629999  |
---------------------------------
Eval num_timesteps=3645000, episode_reward=24323.47 +/- 606.42
Episode length: 14.40 +/- 6.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.4     |
|    mean_reward     | 2.43e+04 |
| time/              |          |
|    total_timesteps | 3645000  |
| train/             |          |
|    actor_loss      | -9.1     |
|    critic_loss     | 0.0107   |
|    learning_rate   | 0.0003   |
|    n_updates       | 3634999  |
---------------------------------
Eval num_timesteps=3650000, episode_reward=24003.12 +/- 9.33
Episode length: 22.40 +/- 3.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3650000  |
| train/             |          |
|    actor_loss      | -9.13    |
|    critic_loss     | 0.0108   |
|    learning_rate   | 0.0003   |
|    n_updates       | 3639999  |
---------------------------------
Eval num_timesteps=3655000, episode_reward=24018.79 +/- 4.33
Episode length: 20.60 +/- 4.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3655000  |
| train/             |          |
|    actor_loss      | -9.08    |
|    critic_loss     | 0.00994  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3644999  |
---------------------------------
Eval num_timesteps=3660000, episode_reward=24009.11 +/- 12.83
Episode length: 21.20 +/- 3.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3660000  |
| train/             |          |
|    actor_loss      | -9.08    |
|    critic_loss     | 0.00882  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3649999  |
---------------------------------
Eval num_timesteps=3665000, episode_reward=24000.74 +/- 15.23
Episode length: 20.60 +/- 3.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3665000  |
| train/             |          |
|    actor_loss      | -9.03    |
|    critic_loss     | 0.0238   |
|    learning_rate   | 0.0003   |
|    n_updates       | 3654999  |
---------------------------------
Eval num_timesteps=3670000, episode_reward=24008.11 +/- 9.74
Episode length: 20.80 +/- 2.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3670000  |
| train/             |          |
|    actor_loss      | -9.03    |
|    critic_loss     | 0.00808  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3659999  |
---------------------------------
Eval num_timesteps=3675000, episode_reward=24011.54 +/- 16.31
Episode length: 22.00 +/- 2.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3675000  |
| train/             |          |
|    actor_loss      | -9.02    |
|    critic_loss     | 0.00957  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3664999  |
---------------------------------
Eval num_timesteps=3680000, episode_reward=24011.11 +/- 5.08
Episode length: 21.40 +/- 3.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3680000  |
| train/             |          |
|    actor_loss      | -9.01    |
|    critic_loss     | 0.0295   |
|    learning_rate   | 0.0003   |
|    n_updates       | 3669999  |
---------------------------------
Eval num_timesteps=3685000, episode_reward=23996.62 +/- 8.65
Episode length: 22.60 +/- 2.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3685000  |
| train/             |          |
|    actor_loss      | -9.09    |
|    critic_loss     | 0.00472  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3674999  |
---------------------------------
Eval num_timesteps=3690000, episode_reward=24002.27 +/- 9.70
Episode length: 22.20 +/- 2.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3690000  |
| train/             |          |
|    actor_loss      | -9.13    |
|    critic_loss     | 0.00719  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3679999  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 135      |
|    ep_rew_mean     | 2.24e+04 |
| time/              |          |
|    episodes        | 25000    |
|    fps             | 195      |
|    time_elapsed    | 18882    |
|    total_timesteps | 3691023  |
| train/             |          |
|    actor_loss      | -9.01    |
|    critic_loss     | 0.00845  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3681022  |
---------------------------------
Eval num_timesteps=3695000, episode_reward=23999.78 +/- 21.48
Episode length: 22.40 +/- 4.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3695000  |
| train/             |          |
|    actor_loss      | -9.1     |
|    critic_loss     | 0.00763  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3684999  |
---------------------------------
Eval num_timesteps=3700000, episode_reward=24026.88 +/- 25.96
Episode length: 17.00 +/- 6.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3700000  |
| train/             |          |
|    actor_loss      | -9.09    |
|    critic_loss     | 0.00715  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3689999  |
---------------------------------
Eval num_timesteps=3705000, episode_reward=24014.96 +/- 14.36
Episode length: 20.40 +/- 2.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3705000  |
| train/             |          |
|    actor_loss      | -9.1     |
|    critic_loss     | 0.00651  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3694999  |
---------------------------------
Eval num_timesteps=3710000, episode_reward=24327.69 +/- 600.40
Episode length: 15.80 +/- 5.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | 2.43e+04 |
| time/              |          |
|    total_timesteps | 3710000  |
| train/             |          |
|    actor_loss      | -9.08    |
|    critic_loss     | 0.00712  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3699999  |
---------------------------------
Eval num_timesteps=3715000, episode_reward=24005.31 +/- 12.00
Episode length: 22.20 +/- 3.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3715000  |
| train/             |          |
|    actor_loss      | -9.06    |
|    critic_loss     | 0.00856  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3704999  |
---------------------------------
Eval num_timesteps=3720000, episode_reward=24005.87 +/- 14.05
Episode length: 22.40 +/- 2.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3720000  |
| train/             |          |
|    actor_loss      | -9.09    |
|    critic_loss     | 0.00734  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3709999  |
---------------------------------
Eval num_timesteps=3725000, episode_reward=23992.00 +/- 21.06
Episode length: 22.80 +/- 5.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3725000  |
| train/             |          |
|    actor_loss      | -9.1     |
|    critic_loss     | 0.00626  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3714999  |
---------------------------------
Eval num_timesteps=3730000, episode_reward=24003.06 +/- 10.81
Episode length: 20.40 +/- 3.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3730000  |
| train/             |          |
|    actor_loss      | -9       |
|    critic_loss     | 0.0081   |
|    learning_rate   | 0.0003   |
|    n_updates       | 3719999  |
---------------------------------
Eval num_timesteps=3735000, episode_reward=24004.15 +/- 14.18
Episode length: 19.80 +/- 3.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3735000  |
| train/             |          |
|    actor_loss      | -9.09    |
|    critic_loss     | 0.00925  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3724999  |
---------------------------------
Eval num_timesteps=3740000, episode_reward=23988.08 +/- 12.47
Episode length: 23.00 +/- 3.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3740000  |
| train/             |          |
|    actor_loss      | -9.05    |
|    critic_loss     | 0.00717  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3729999  |
---------------------------------
Eval num_timesteps=3745000, episode_reward=24012.25 +/- 13.37
Episode length: 19.60 +/- 3.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3745000  |
| train/             |          |
|    actor_loss      | -9.08    |
|    critic_loss     | 0.0108   |
|    learning_rate   | 0.0003   |
|    n_updates       | 3734999  |
---------------------------------
Eval num_timesteps=3750000, episode_reward=24001.89 +/- 3.21
Episode length: 20.60 +/- 2.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3750000  |
| train/             |          |
|    actor_loss      | -9.07    |
|    critic_loss     | 0.00608  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3739999  |
---------------------------------
Eval num_timesteps=3755000, episode_reward=24013.30 +/- 15.37
Episode length: 19.00 +/- 4.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3755000  |
| train/             |          |
|    actor_loss      | -9.01    |
|    critic_loss     | 0.00759  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3744999  |
---------------------------------
Eval num_timesteps=3760000, episode_reward=23993.78 +/- 10.86
Episode length: 23.40 +/- 2.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3760000  |
| train/             |          |
|    actor_loss      | -9.09    |
|    critic_loss     | 0.00751  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3749999  |
---------------------------------
Eval num_timesteps=3765000, episode_reward=23993.88 +/- 5.23
Episode length: 23.60 +/- 1.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3765000  |
| train/             |          |
|    actor_loss      | -9.04    |
|    critic_loss     | 0.00703  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3754999  |
---------------------------------
Eval num_timesteps=3770000, episode_reward=24002.26 +/- 10.32
Episode length: 20.80 +/- 2.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3770000  |
| train/             |          |
|    actor_loss      | -9.12    |
|    critic_loss     | 0.00854  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3759999  |
---------------------------------
Eval num_timesteps=3775000, episode_reward=24019.21 +/- 6.58
Episode length: 17.80 +/- 3.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3775000  |
| train/             |          |
|    actor_loss      | -9.07    |
|    critic_loss     | 0.00748  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3764999  |
---------------------------------
Eval num_timesteps=3780000, episode_reward=24017.01 +/- 24.50
Episode length: 17.80 +/- 4.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3780000  |
| train/             |          |
|    actor_loss      | -9.13    |
|    critic_loss     | 0.0067   |
|    learning_rate   | 0.0003   |
|    n_updates       | 3769999  |
---------------------------------
Eval num_timesteps=3785000, episode_reward=23998.55 +/- 14.87
Episode length: 24.40 +/- 2.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3785000  |
| train/             |          |
|    actor_loss      | -9.07    |
|    critic_loss     | 0.00878  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3774999  |
---------------------------------
Eval num_timesteps=3790000, episode_reward=23997.57 +/- 10.68
Episode length: 21.60 +/- 2.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3790000  |
| train/             |          |
|    actor_loss      | -9.12    |
|    critic_loss     | 0.0076   |
|    learning_rate   | 0.0003   |
|    n_updates       | 3779999  |
---------------------------------
Eval num_timesteps=3795000, episode_reward=24005.77 +/- 19.77
Episode length: 21.40 +/- 4.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3795000  |
| train/             |          |
|    actor_loss      | -9.07    |
|    critic_loss     | 0.00797  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3784999  |
---------------------------------
Eval num_timesteps=3800000, episode_reward=24003.20 +/- 15.74
Episode length: 23.00 +/- 2.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3800000  |
| train/             |          |
|    actor_loss      | -8.92    |
|    critic_loss     | 0.00617  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3789999  |
---------------------------------
Eval num_timesteps=3805000, episode_reward=24041.23 +/- 26.56
Episode length: 15.00 +/- 6.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3805000  |
| train/             |          |
|    actor_loss      | -9.09    |
|    critic_loss     | 0.126    |
|    learning_rate   | 0.0003   |
|    n_updates       | 3794999  |
---------------------------------
Eval num_timesteps=3810000, episode_reward=24021.44 +/- 14.62
Episode length: 20.20 +/- 4.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3810000  |
| train/             |          |
|    actor_loss      | -9.09    |
|    critic_loss     | 0.00913  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3799999  |
---------------------------------
Eval num_timesteps=3815000, episode_reward=24006.14 +/- 12.09
Episode length: 19.40 +/- 1.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3815000  |
| train/             |          |
|    actor_loss      | -9.01    |
|    critic_loss     | 0.00797  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3804999  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 110      |
|    ep_rew_mean     | 2.33e+04 |
| time/              |          |
|    episodes        | 26000    |
|    fps             | 195      |
|    time_elapsed    | 19510    |
|    total_timesteps | 3816641  |
| train/             |          |
|    actor_loss      | -9.08    |
|    critic_loss     | 0.00819  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3806640  |
---------------------------------
Eval num_timesteps=3820000, episode_reward=23995.43 +/- 8.75
Episode length: 22.40 +/- 1.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3820000  |
| train/             |          |
|    actor_loss      | -9.06    |
|    critic_loss     | 0.00783  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3809999  |
---------------------------------
Eval num_timesteps=3825000, episode_reward=24016.45 +/- 26.30
Episode length: 19.00 +/- 5.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3825000  |
| train/             |          |
|    actor_loss      | -9.08    |
|    critic_loss     | 0.00635  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3814999  |
---------------------------------
Eval num_timesteps=3830000, episode_reward=24000.46 +/- 14.13
Episode length: 24.20 +/- 3.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3830000  |
| train/             |          |
|    actor_loss      | -9       |
|    critic_loss     | 0.00692  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3819999  |
---------------------------------
Eval num_timesteps=3835000, episode_reward=24010.87 +/- 5.15
Episode length: 21.40 +/- 2.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3835000  |
| train/             |          |
|    actor_loss      | -9.12    |
|    critic_loss     | 0.00876  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3824999  |
---------------------------------
Eval num_timesteps=3840000, episode_reward=24014.84 +/- 30.79
Episode length: 21.60 +/- 6.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3840000  |
| train/             |          |
|    actor_loss      | -9.13    |
|    critic_loss     | 0.00853  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3829999  |
---------------------------------
Eval num_timesteps=3845000, episode_reward=24004.89 +/- 15.11
Episode length: 22.80 +/- 3.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3845000  |
| train/             |          |
|    actor_loss      | -9.06    |
|    critic_loss     | 0.00543  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3834999  |
---------------------------------
Eval num_timesteps=3850000, episode_reward=24002.42 +/- 22.78
Episode length: 20.60 +/- 3.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3850000  |
| train/             |          |
|    actor_loss      | -9.04    |
|    critic_loss     | 0.0101   |
|    learning_rate   | 0.0003   |
|    n_updates       | 3839999  |
---------------------------------
Eval num_timesteps=3855000, episode_reward=24003.47 +/- 20.21
Episode length: 23.20 +/- 3.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3855000  |
| train/             |          |
|    actor_loss      | -9.09    |
|    critic_loss     | 0.00769  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3844999  |
---------------------------------
Eval num_timesteps=3860000, episode_reward=24001.75 +/- 9.91
Episode length: 20.40 +/- 3.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3860000  |
| train/             |          |
|    actor_loss      | -9.12    |
|    critic_loss     | 0.0103   |
|    learning_rate   | 0.0003   |
|    n_updates       | 3849999  |
---------------------------------
Eval num_timesteps=3865000, episode_reward=24013.50 +/- 22.18
Episode length: 20.60 +/- 5.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3865000  |
| train/             |          |
|    actor_loss      | -9.02    |
|    critic_loss     | 0.0132   |
|    learning_rate   | 0.0003   |
|    n_updates       | 3854999  |
---------------------------------
Eval num_timesteps=3870000, episode_reward=24016.88 +/- 17.43
Episode length: 18.20 +/- 3.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3870000  |
| train/             |          |
|    actor_loss      | -9.1     |
|    critic_loss     | 0.00706  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3859999  |
---------------------------------
Eval num_timesteps=3875000, episode_reward=24024.71 +/- 16.47
Episode length: 19.60 +/- 3.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3875000  |
| train/             |          |
|    actor_loss      | -9.03    |
|    critic_loss     | 0.00763  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3864999  |
---------------------------------
Eval num_timesteps=3880000, episode_reward=24015.92 +/- 17.20
Episode length: 20.80 +/- 3.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3880000  |
| train/             |          |
|    actor_loss      | -9.08    |
|    critic_loss     | 0.00815  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3869999  |
---------------------------------
Eval num_timesteps=3885000, episode_reward=24013.73 +/- 12.25
Episode length: 22.00 +/- 3.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3885000  |
| train/             |          |
|    actor_loss      | -9.05    |
|    critic_loss     | 0.00836  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3874999  |
---------------------------------
Eval num_timesteps=3890000, episode_reward=24011.80 +/- 17.62
Episode length: 21.60 +/- 4.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3890000  |
| train/             |          |
|    actor_loss      | -9.06    |
|    critic_loss     | 0.007    |
|    learning_rate   | 0.0003   |
|    n_updates       | 3879999  |
---------------------------------
Eval num_timesteps=3895000, episode_reward=24016.10 +/- 20.98
Episode length: 18.60 +/- 5.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3895000  |
| train/             |          |
|    actor_loss      | -9.1     |
|    critic_loss     | 0.00682  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3884999  |
---------------------------------
Eval num_timesteps=3900000, episode_reward=24006.38 +/- 14.21
Episode length: 22.60 +/- 2.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3900000  |
| train/             |          |
|    actor_loss      | -9.04    |
|    critic_loss     | 0.00835  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3889999  |
---------------------------------
Eval num_timesteps=3905000, episode_reward=24013.36 +/- 12.53
Episode length: 22.20 +/- 1.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3905000  |
| train/             |          |
|    actor_loss      | -9.06    |
|    critic_loss     | 0.00721  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3894999  |
---------------------------------
Eval num_timesteps=3910000, episode_reward=24016.28 +/- 22.62
Episode length: 20.60 +/- 4.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3910000  |
| train/             |          |
|    actor_loss      | -9.08    |
|    critic_loss     | 0.00827  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3899999  |
---------------------------------
Eval num_timesteps=3915000, episode_reward=24017.82 +/- 10.40
Episode length: 20.40 +/- 3.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3915000  |
| train/             |          |
|    actor_loss      | -9.09    |
|    critic_loss     | 0.00732  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3904999  |
---------------------------------
Eval num_timesteps=3920000, episode_reward=24046.23 +/- 21.76
Episode length: 16.40 +/- 4.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3920000  |
| train/             |          |
|    actor_loss      | -9.12    |
|    critic_loss     | 0.00718  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3909999  |
---------------------------------
Eval num_timesteps=3925000, episode_reward=24023.40 +/- 11.91
Episode length: 20.40 +/- 3.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3925000  |
| train/             |          |
|    actor_loss      | -9.09    |
|    critic_loss     | 0.00709  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3914999  |
---------------------------------
Eval num_timesteps=3930000, episode_reward=23997.77 +/- 19.50
Episode length: 22.80 +/- 3.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3930000  |
| train/             |          |
|    actor_loss      | -9.08    |
|    critic_loss     | 0.00672  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3919999  |
---------------------------------
Eval num_timesteps=3935000, episode_reward=24020.71 +/- 23.65
Episode length: 19.20 +/- 4.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3935000  |
| train/             |          |
|    actor_loss      | -9.1     |
|    critic_loss     | 0.00752  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3924999  |
---------------------------------
Eval num_timesteps=3940000, episode_reward=24003.68 +/- 13.89
Episode length: 21.40 +/- 4.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3940000  |
| train/             |          |
|    actor_loss      | -9.09    |
|    critic_loss     | 0.00573  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3929999  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 125      |
|    ep_rew_mean     | 2.25e+04 |
| time/              |          |
|    episodes        | 27000    |
|    fps             | 195      |
|    time_elapsed    | 20128    |
|    total_timesteps | 3940609  |
| train/             |          |
|    actor_loss      | -8.97    |
|    critic_loss     | 0.0304   |
|    learning_rate   | 0.0003   |
|    n_updates       | 3930608  |
---------------------------------
Eval num_timesteps=3945000, episode_reward=24002.03 +/- 13.25
Episode length: 23.00 +/- 3.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3945000  |
| train/             |          |
|    actor_loss      | -9.05    |
|    critic_loss     | 0.00752  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3934999  |
---------------------------------
Eval num_timesteps=3950000, episode_reward=24320.97 +/- 607.00
Episode length: 17.40 +/- 6.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.4     |
|    mean_reward     | 2.43e+04 |
| time/              |          |
|    total_timesteps | 3950000  |
| train/             |          |
|    actor_loss      | -9.05    |
|    critic_loss     | 0.00765  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3939999  |
---------------------------------
Eval num_timesteps=3955000, episode_reward=24014.71 +/- 20.28
Episode length: 20.00 +/- 3.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3955000  |
| train/             |          |
|    actor_loss      | -9.09    |
|    critic_loss     | 0.00646  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3944999  |
---------------------------------
Eval num_timesteps=3960000, episode_reward=24021.37 +/- 23.72
Episode length: 19.60 +/- 4.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3960000  |
| train/             |          |
|    actor_loss      | -9.11    |
|    critic_loss     | 0.00547  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3949999  |
---------------------------------
Eval num_timesteps=3965000, episode_reward=24025.20 +/- 19.36
Episode length: 18.00 +/- 4.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3965000  |
| train/             |          |
|    actor_loss      | -9.1     |
|    critic_loss     | 0.0105   |
|    learning_rate   | 0.0003   |
|    n_updates       | 3954999  |
---------------------------------
Eval num_timesteps=3970000, episode_reward=24013.20 +/- 17.85
Episode length: 21.20 +/- 3.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3970000  |
| train/             |          |
|    actor_loss      | -9.1     |
|    critic_loss     | 0.0224   |
|    learning_rate   | 0.0003   |
|    n_updates       | 3959999  |
---------------------------------
Eval num_timesteps=3975000, episode_reward=24013.04 +/- 5.05
Episode length: 22.00 +/- 2.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3975000  |
| train/             |          |
|    actor_loss      | -9.12    |
|    critic_loss     | 0.00641  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3964999  |
---------------------------------
Eval num_timesteps=3980000, episode_reward=24019.57 +/- 10.41
Episode length: 19.60 +/- 4.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3980000  |
| train/             |          |
|    actor_loss      | -9.09    |
|    critic_loss     | 0.00745  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3969999  |
---------------------------------
Eval num_timesteps=3985000, episode_reward=24000.74 +/- 24.12
Episode length: 23.00 +/- 3.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3985000  |
| train/             |          |
|    actor_loss      | -9.05    |
|    critic_loss     | 0.00715  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3974999  |
---------------------------------
Eval num_timesteps=3990000, episode_reward=24020.83 +/- 19.44
Episode length: 20.00 +/- 3.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3990000  |
| train/             |          |
|    actor_loss      | -9       |
|    critic_loss     | 0.0101   |
|    learning_rate   | 0.0003   |
|    n_updates       | 3979999  |
---------------------------------
Eval num_timesteps=3995000, episode_reward=24001.96 +/- 3.97
Episode length: 23.20 +/- 2.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 3995000  |
| train/             |          |
|    actor_loss      | -9.05    |
|    critic_loss     | 0.00645  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3984999  |
---------------------------------
Eval num_timesteps=4000000, episode_reward=24023.31 +/- 19.83
Episode length: 20.20 +/- 3.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4000000  |
| train/             |          |
|    actor_loss      | -9.09    |
|    critic_loss     | 0.0069   |
|    learning_rate   | 0.0003   |
|    n_updates       | 3989999  |
---------------------------------
Eval num_timesteps=4005000, episode_reward=24005.33 +/- 9.37
Episode length: 22.40 +/- 1.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4005000  |
| train/             |          |
|    actor_loss      | -9.06    |
|    critic_loss     | 0.00584  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3994999  |
---------------------------------
Eval num_timesteps=4010000, episode_reward=24001.55 +/- 13.00
Episode length: 20.40 +/- 2.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4010000  |
| train/             |          |
|    actor_loss      | -9.09    |
|    critic_loss     | 0.00802  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3999999  |
---------------------------------
Eval num_timesteps=4015000, episode_reward=24005.52 +/- 20.99
Episode length: 22.40 +/- 2.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4015000  |
| train/             |          |
|    actor_loss      | -9.06    |
|    critic_loss     | 0.00691  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4004999  |
---------------------------------
Eval num_timesteps=4020000, episode_reward=24022.24 +/- 11.32
Episode length: 19.80 +/- 3.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4020000  |
| train/             |          |
|    actor_loss      | -9.05    |
|    critic_loss     | 0.0166   |
|    learning_rate   | 0.0003   |
|    n_updates       | 4009999  |
---------------------------------
Eval num_timesteps=4025000, episode_reward=24026.12 +/- 25.84
Episode length: 17.80 +/- 6.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4025000  |
| train/             |          |
|    actor_loss      | -9.07    |
|    critic_loss     | 0.0185   |
|    learning_rate   | 0.0003   |
|    n_updates       | 4014999  |
---------------------------------
Eval num_timesteps=4030000, episode_reward=24021.04 +/- 15.49
Episode length: 20.00 +/- 3.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4030000  |
| train/             |          |
|    actor_loss      | -8.99    |
|    critic_loss     | 0.00634  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4019999  |
---------------------------------
Eval num_timesteps=4035000, episode_reward=24021.46 +/- 19.40
Episode length: 19.60 +/- 5.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4035000  |
| train/             |          |
|    actor_loss      | -9.04    |
|    critic_loss     | 0.00676  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4024999  |
---------------------------------
Eval num_timesteps=4040000, episode_reward=24014.61 +/- 11.49
Episode length: 21.60 +/- 1.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4040000  |
| train/             |          |
|    actor_loss      | -9.04    |
|    critic_loss     | 0.0105   |
|    learning_rate   | 0.0003   |
|    n_updates       | 4029999  |
---------------------------------
Eval num_timesteps=4045000, episode_reward=24016.57 +/- 8.06
Episode length: 19.00 +/- 0.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4045000  |
| train/             |          |
|    actor_loss      | -9.03    |
|    critic_loss     | 0.00579  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4034999  |
---------------------------------
Eval num_timesteps=4050000, episode_reward=23999.07 +/- 13.80
Episode length: 21.00 +/- 3.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4050000  |
| train/             |          |
|    actor_loss      | -9.05    |
|    critic_loss     | 0.00975  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4039999  |
---------------------------------
Eval num_timesteps=4055000, episode_reward=24000.97 +/- 9.26
Episode length: 21.40 +/- 2.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4055000  |
| train/             |          |
|    actor_loss      | -9.08    |
|    critic_loss     | 0.00805  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4044999  |
---------------------------------
Eval num_timesteps=4060000, episode_reward=23998.98 +/- 8.80
Episode length: 22.60 +/- 1.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4060000  |
| train/             |          |
|    actor_loss      | -9.09    |
|    critic_loss     | 0.00667  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4049999  |
---------------------------------
Eval num_timesteps=4065000, episode_reward=24018.98 +/- 5.45
Episode length: 19.20 +/- 1.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4065000  |
| train/             |          |
|    actor_loss      | -9.1     |
|    critic_loss     | 0.0076   |
|    learning_rate   | 0.0003   |
|    n_updates       | 4054999  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 136      |
|    ep_rew_mean     | 2.26e+04 |
| time/              |          |
|    episodes        | 28000    |
|    fps             | 195      |
|    time_elapsed    | 20760    |
|    total_timesteps | 4066578  |
| train/             |          |
|    actor_loss      | -9.06    |
|    critic_loss     | 0.00617  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4056577  |
---------------------------------
Eval num_timesteps=4070000, episode_reward=24316.41 +/- 615.81
Episode length: 20.00 +/- 5.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20       |
|    mean_reward     | 2.43e+04 |
| time/              |          |
|    total_timesteps | 4070000  |
| train/             |          |
|    actor_loss      | -9.02    |
|    critic_loss     | 0.00691  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4059999  |
---------------------------------
Eval num_timesteps=4075000, episode_reward=24004.08 +/- 35.44
Episode length: 21.40 +/- 6.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4075000  |
| train/             |          |
|    actor_loss      | -9.07    |
|    critic_loss     | 0.00669  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4064999  |
---------------------------------
Eval num_timesteps=4080000, episode_reward=24004.73 +/- 12.84
Episode length: 20.40 +/- 2.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4080000  |
| train/             |          |
|    actor_loss      | -9.08    |
|    critic_loss     | 0.00652  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4069999  |
---------------------------------
Eval num_timesteps=4085000, episode_reward=24011.82 +/- 9.14
Episode length: 19.60 +/- 3.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4085000  |
| train/             |          |
|    actor_loss      | -9.1     |
|    critic_loss     | 0.00562  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4074999  |
---------------------------------
Eval num_timesteps=4090000, episode_reward=24013.34 +/- 22.87
Episode length: 20.20 +/- 4.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4090000  |
| train/             |          |
|    actor_loss      | -9.03    |
|    critic_loss     | 0.0269   |
|    learning_rate   | 0.0003   |
|    n_updates       | 4079999  |
---------------------------------
Eval num_timesteps=4095000, episode_reward=23996.96 +/- 14.48
Episode length: 23.00 +/- 3.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4095000  |
| train/             |          |
|    actor_loss      | -9.03    |
|    critic_loss     | 0.00945  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4084999  |
---------------------------------
Eval num_timesteps=4100000, episode_reward=24013.93 +/- 31.53
Episode length: 19.40 +/- 4.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4100000  |
| train/             |          |
|    actor_loss      | -9.02    |
|    critic_loss     | 0.0215   |
|    learning_rate   | 0.0003   |
|    n_updates       | 4089999  |
---------------------------------
Eval num_timesteps=4105000, episode_reward=24019.40 +/- 16.57
Episode length: 18.20 +/- 4.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4105000  |
| train/             |          |
|    actor_loss      | -9.04    |
|    critic_loss     | 0.0464   |
|    learning_rate   | 0.0003   |
|    n_updates       | 4094999  |
---------------------------------
Eval num_timesteps=4110000, episode_reward=24013.34 +/- 28.82
Episode length: 19.60 +/- 7.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4110000  |
| train/             |          |
|    actor_loss      | -9.13    |
|    critic_loss     | 0.0163   |
|    learning_rate   | 0.0003   |
|    n_updates       | 4099999  |
---------------------------------
Eval num_timesteps=4115000, episode_reward=24017.62 +/- 24.28
Episode length: 18.00 +/- 4.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4115000  |
| train/             |          |
|    actor_loss      | -9.07    |
|    critic_loss     | 0.00659  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4104999  |
---------------------------------
Eval num_timesteps=4120000, episode_reward=24019.82 +/- 21.07
Episode length: 18.00 +/- 5.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4120000  |
| train/             |          |
|    actor_loss      | -9.1     |
|    critic_loss     | 0.0054   |
|    learning_rate   | 0.0003   |
|    n_updates       | 4109999  |
---------------------------------
Eval num_timesteps=4125000, episode_reward=24022.15 +/- 14.17
Episode length: 17.40 +/- 2.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4125000  |
| train/             |          |
|    actor_loss      | -9       |
|    critic_loss     | 0.00775  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4114999  |
---------------------------------
Eval num_timesteps=4130000, episode_reward=24017.70 +/- 21.93
Episode length: 18.20 +/- 5.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4130000  |
| train/             |          |
|    actor_loss      | -9.07    |
|    critic_loss     | 0.00695  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4119999  |
---------------------------------
Eval num_timesteps=4135000, episode_reward=24012.50 +/- 16.13
Episode length: 19.40 +/- 3.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4135000  |
| train/             |          |
|    actor_loss      | -9.09    |
|    critic_loss     | 0.011    |
|    learning_rate   | 0.0003   |
|    n_updates       | 4124999  |
---------------------------------
Eval num_timesteps=4140000, episode_reward=24006.37 +/- 8.71
Episode length: 22.40 +/- 1.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4140000  |
| train/             |          |
|    actor_loss      | -9.09    |
|    critic_loss     | 0.00559  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4129999  |
---------------------------------
Eval num_timesteps=4145000, episode_reward=24004.07 +/- 10.97
Episode length: 21.40 +/- 2.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4145000  |
| train/             |          |
|    actor_loss      | -9.03    |
|    critic_loss     | 0.0101   |
|    learning_rate   | 0.0003   |
|    n_updates       | 4134999  |
---------------------------------
Eval num_timesteps=4150000, episode_reward=24003.14 +/- 21.61
Episode length: 22.20 +/- 3.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4150000  |
| train/             |          |
|    actor_loss      | -9.11    |
|    critic_loss     | 0.00867  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4139999  |
---------------------------------
Eval num_timesteps=4155000, episode_reward=23995.72 +/- 12.22
Episode length: 22.40 +/- 2.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4155000  |
| train/             |          |
|    actor_loss      | -9.05    |
|    critic_loss     | 0.00599  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4144999  |
---------------------------------
Eval num_timesteps=4160000, episode_reward=24006.40 +/- 7.46
Episode length: 22.20 +/- 1.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4160000  |
| train/             |          |
|    actor_loss      | -9.12    |
|    critic_loss     | 0.00753  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4149999  |
---------------------------------
Eval num_timesteps=4165000, episode_reward=23998.94 +/- 13.75
Episode length: 21.80 +/- 3.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4165000  |
| train/             |          |
|    actor_loss      | -9.07    |
|    critic_loss     | 0.0113   |
|    learning_rate   | 0.0003   |
|    n_updates       | 4154999  |
---------------------------------
Eval num_timesteps=4170000, episode_reward=24012.37 +/- 22.17
Episode length: 21.20 +/- 4.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4170000  |
| train/             |          |
|    actor_loss      | -9.1     |
|    critic_loss     | 0.00756  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4159999  |
---------------------------------
Eval num_timesteps=4175000, episode_reward=24009.23 +/- 28.82
Episode length: 19.00 +/- 6.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4175000  |
| train/             |          |
|    actor_loss      | -9.05    |
|    critic_loss     | 0.00625  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4164999  |
---------------------------------
Eval num_timesteps=4180000, episode_reward=24003.36 +/- 22.30
Episode length: 20.80 +/- 3.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4180000  |
| train/             |          |
|    actor_loss      | -9.02    |
|    critic_loss     | 0.00694  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4169999  |
---------------------------------
Eval num_timesteps=4185000, episode_reward=24020.29 +/- 20.25
Episode length: 19.40 +/- 3.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4185000  |
| train/             |          |
|    actor_loss      | -9.09    |
|    critic_loss     | 0.00906  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4174999  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 123      |
|    ep_rew_mean     | 2.26e+04 |
| time/              |          |
|    episodes        | 29000    |
|    fps             | 195      |
|    time_elapsed    | 21369    |
|    total_timesteps | 4188052  |
| train/             |          |
|    actor_loss      | -9.04    |
|    critic_loss     | 0.00803  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4178051  |
---------------------------------
Eval num_timesteps=4190000, episode_reward=24001.02 +/- 7.64
Episode length: 22.20 +/- 2.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4190000  |
| train/             |          |
|    actor_loss      | -9.06    |
|    critic_loss     | 0.00634  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4179999  |
---------------------------------
Eval num_timesteps=4195000, episode_reward=24016.93 +/- 8.99
Episode length: 18.00 +/- 3.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4195000  |
| train/             |          |
|    actor_loss      | -8.97    |
|    critic_loss     | 0.00995  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4184999  |
---------------------------------
Eval num_timesteps=4200000, episode_reward=24030.08 +/- 25.85
Episode length: 16.20 +/- 5.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4200000  |
| train/             |          |
|    actor_loss      | -9.04    |
|    critic_loss     | 0.0142   |
|    learning_rate   | 0.0003   |
|    n_updates       | 4189999  |
---------------------------------
Eval num_timesteps=4205000, episode_reward=24006.68 +/- 26.73
Episode length: 20.40 +/- 6.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4205000  |
| train/             |          |
|    actor_loss      | -9.03    |
|    critic_loss     | 0.00676  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4194999  |
---------------------------------
Eval num_timesteps=4210000, episode_reward=24017.44 +/- 22.01
Episode length: 21.20 +/- 2.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4210000  |
| train/             |          |
|    actor_loss      | -9.01    |
|    critic_loss     | 0.00695  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4199999  |
---------------------------------
Eval num_timesteps=4215000, episode_reward=24007.21 +/- 22.39
Episode length: 21.40 +/- 3.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4215000  |
| train/             |          |
|    actor_loss      | -9.02    |
|    critic_loss     | 0.0101   |
|    learning_rate   | 0.0003   |
|    n_updates       | 4204999  |
---------------------------------
Eval num_timesteps=4220000, episode_reward=24015.30 +/- 14.27
Episode length: 19.80 +/- 3.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4220000  |
| train/             |          |
|    actor_loss      | -9.07    |
|    critic_loss     | 0.00831  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4209999  |
---------------------------------
Eval num_timesteps=4225000, episode_reward=24010.02 +/- 8.63
Episode length: 20.80 +/- 2.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4225000  |
| train/             |          |
|    actor_loss      | -9.1     |
|    critic_loss     | 0.00523  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4214999  |
---------------------------------
Eval num_timesteps=4230000, episode_reward=24016.39 +/- 12.98
Episode length: 21.40 +/- 2.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4230000  |
| train/             |          |
|    actor_loss      | -9.11    |
|    critic_loss     | 0.0109   |
|    learning_rate   | 0.0003   |
|    n_updates       | 4219999  |
---------------------------------
Eval num_timesteps=4235000, episode_reward=24002.41 +/- 11.96
Episode length: 23.00 +/- 1.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4235000  |
| train/             |          |
|    actor_loss      | -9.03    |
|    critic_loss     | 0.0231   |
|    learning_rate   | 0.0003   |
|    n_updates       | 4224999  |
---------------------------------
Eval num_timesteps=4240000, episode_reward=24010.56 +/- 3.40
Episode length: 21.00 +/- 1.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4240000  |
| train/             |          |
|    actor_loss      | -9.02    |
|    critic_loss     | 0.00901  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4229999  |
---------------------------------
Eval num_timesteps=4245000, episode_reward=24020.29 +/- 19.03
Episode length: 18.40 +/- 2.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4245000  |
| train/             |          |
|    actor_loss      | -9.12    |
|    critic_loss     | 0.00967  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4234999  |
---------------------------------
Eval num_timesteps=4250000, episode_reward=24008.56 +/- 7.18
Episode length: 20.60 +/- 1.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4250000  |
| train/             |          |
|    actor_loss      | -9.09    |
|    critic_loss     | 0.00829  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4239999  |
---------------------------------
Eval num_timesteps=4255000, episode_reward=24002.37 +/- 6.75
Episode length: 23.20 +/- 1.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4255000  |
| train/             |          |
|    actor_loss      | -9.04    |
|    critic_loss     | 0.00637  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4244999  |
---------------------------------
Eval num_timesteps=4260000, episode_reward=24009.13 +/- 14.10
Episode length: 19.60 +/- 4.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4260000  |
| train/             |          |
|    actor_loss      | -9.02    |
|    critic_loss     | 0.00915  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4249999  |
---------------------------------
Eval num_timesteps=4265000, episode_reward=24001.59 +/- 11.71
Episode length: 22.60 +/- 2.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4265000  |
| train/             |          |
|    actor_loss      | -9.07    |
|    critic_loss     | 0.00701  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4254999  |
---------------------------------
Eval num_timesteps=4270000, episode_reward=24007.32 +/- 15.46
Episode length: 22.60 +/- 2.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4270000  |
| train/             |          |
|    actor_loss      | -9.06    |
|    critic_loss     | 0.00818  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4259999  |
---------------------------------
Eval num_timesteps=4275000, episode_reward=24029.23 +/- 20.05
Episode length: 17.20 +/- 5.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4275000  |
| train/             |          |
|    actor_loss      | -9.04    |
|    critic_loss     | 0.0731   |
|    learning_rate   | 0.0003   |
|    n_updates       | 4264999  |
---------------------------------
Eval num_timesteps=4280000, episode_reward=24017.45 +/- 12.36
Episode length: 20.80 +/- 2.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4280000  |
| train/             |          |
|    actor_loss      | -9.06    |
|    critic_loss     | 0.00767  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4269999  |
---------------------------------
Eval num_timesteps=4285000, episode_reward=24014.52 +/- 16.72
Episode length: 18.40 +/- 3.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4285000  |
| train/             |          |
|    actor_loss      | -9.03    |
|    critic_loss     | 0.0206   |
|    learning_rate   | 0.0003   |
|    n_updates       | 4274999  |
---------------------------------
Eval num_timesteps=4290000, episode_reward=24008.76 +/- 14.48
Episode length: 21.40 +/- 1.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4290000  |
| train/             |          |
|    actor_loss      | -9.03    |
|    critic_loss     | 0.00892  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4279999  |
---------------------------------
Eval num_timesteps=4295000, episode_reward=24012.63 +/- 17.10
Episode length: 19.80 +/- 3.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4295000  |
| train/             |          |
|    actor_loss      | -9.07    |
|    critic_loss     | 0.00776  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4284999  |
---------------------------------
Eval num_timesteps=4300000, episode_reward=23995.91 +/- 19.56
Episode length: 22.60 +/- 3.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4300000  |
| train/             |          |
|    actor_loss      | -8.96    |
|    critic_loss     | 0.00984  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4289999  |
---------------------------------
Eval num_timesteps=4305000, episode_reward=24318.45 +/- 610.23
Episode length: 16.60 +/- 4.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | 2.43e+04 |
| time/              |          |
|    total_timesteps | 4305000  |
| train/             |          |
|    actor_loss      | -9.03    |
|    critic_loss     | 0.00875  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4294999  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 119      |
|    ep_rew_mean     | 2.24e+04 |
| time/              |          |
|    episodes        | 30000    |
|    fps             | 196      |
|    time_elapsed    | 21970    |
|    total_timesteps | 4307455  |
| train/             |          |
|    actor_loss      | -8.98    |
|    critic_loss     | 0.00838  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4297454  |
---------------------------------
Eval num_timesteps=4310000, episode_reward=24013.18 +/- 24.21
Episode length: 19.60 +/- 4.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4310000  |
| train/             |          |
|    actor_loss      | -9.07    |
|    critic_loss     | 0.00576  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4299999  |
---------------------------------
Eval num_timesteps=4315000, episode_reward=24023.78 +/- 20.21
Episode length: 18.60 +/- 5.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4315000  |
| train/             |          |
|    actor_loss      | -9.08    |
|    critic_loss     | 0.00927  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4304999  |
---------------------------------
Eval num_timesteps=4320000, episode_reward=24018.96 +/- 7.60
Episode length: 19.80 +/- 2.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4320000  |
| train/             |          |
|    actor_loss      | -9.09    |
|    critic_loss     | 0.00761  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4309999  |
---------------------------------
Eval num_timesteps=4325000, episode_reward=24024.60 +/- 22.03
Episode length: 18.60 +/- 5.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4325000  |
| train/             |          |
|    actor_loss      | -9.07    |
|    critic_loss     | 0.0124   |
|    learning_rate   | 0.0003   |
|    n_updates       | 4314999  |
---------------------------------
Eval num_timesteps=4330000, episode_reward=24016.95 +/- 16.48
Episode length: 17.80 +/- 4.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4330000  |
| train/             |          |
|    actor_loss      | -9.05    |
|    critic_loss     | 0.00895  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4319999  |
---------------------------------
Eval num_timesteps=4335000, episode_reward=24031.96 +/- 10.58
Episode length: 18.60 +/- 2.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4335000  |
| train/             |          |
|    actor_loss      | -9.05    |
|    critic_loss     | 0.00672  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4324999  |
---------------------------------
Eval num_timesteps=4340000, episode_reward=24013.80 +/- 13.26
Episode length: 19.40 +/- 2.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4340000  |
| train/             |          |
|    actor_loss      | -9.05    |
|    critic_loss     | 0.00842  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4329999  |
---------------------------------
Eval num_timesteps=4345000, episode_reward=24029.31 +/- 28.24
Episode length: 16.80 +/- 6.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4345000  |
| train/             |          |
|    actor_loss      | -9.03    |
|    critic_loss     | 0.00765  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4334999  |
---------------------------------
Eval num_timesteps=4350000, episode_reward=24018.38 +/- 19.24
Episode length: 19.60 +/- 3.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4350000  |
| train/             |          |
|    actor_loss      | -9.05    |
|    critic_loss     | 0.00785  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4339999  |
---------------------------------
Eval num_timesteps=4355000, episode_reward=24022.33 +/- 22.49
Episode length: 19.00 +/- 6.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4355000  |
| train/             |          |
|    actor_loss      | -9.1     |
|    critic_loss     | 0.0606   |
|    learning_rate   | 0.0003   |
|    n_updates       | 4344999  |
---------------------------------
Eval num_timesteps=4360000, episode_reward=24032.07 +/- 18.91
Episode length: 15.80 +/- 4.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4360000  |
| train/             |          |
|    actor_loss      | -9.07    |
|    critic_loss     | 0.00856  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4349999  |
---------------------------------
Eval num_timesteps=4365000, episode_reward=24007.72 +/- 13.25
Episode length: 20.60 +/- 4.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4365000  |
| train/             |          |
|    actor_loss      | -9.05    |
|    critic_loss     | 0.0119   |
|    learning_rate   | 0.0003   |
|    n_updates       | 4354999  |
---------------------------------
Eval num_timesteps=4370000, episode_reward=24014.24 +/- 18.74
Episode length: 21.60 +/- 2.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4370000  |
| train/             |          |
|    actor_loss      | -9.04    |
|    critic_loss     | 0.00753  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4359999  |
---------------------------------
Eval num_timesteps=4375000, episode_reward=24013.86 +/- 12.95
Episode length: 22.00 +/- 2.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4375000  |
| train/             |          |
|    actor_loss      | -9.02    |
|    critic_loss     | 0.00643  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4364999  |
---------------------------------
Eval num_timesteps=4380000, episode_reward=24021.49 +/- 15.39
Episode length: 18.40 +/- 3.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4380000  |
| train/             |          |
|    actor_loss      | -9       |
|    critic_loss     | 0.00849  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4369999  |
---------------------------------
Eval num_timesteps=4385000, episode_reward=24014.64 +/- 14.85
Episode length: 21.60 +/- 2.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4385000  |
| train/             |          |
|    actor_loss      | -9.05    |
|    critic_loss     | 0.00655  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4374999  |
---------------------------------
Eval num_timesteps=4390000, episode_reward=24012.97 +/- 23.24
Episode length: 19.40 +/- 3.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4390000  |
| train/             |          |
|    actor_loss      | -9.06    |
|    critic_loss     | 0.008    |
|    learning_rate   | 0.0003   |
|    n_updates       | 4379999  |
---------------------------------
Eval num_timesteps=4395000, episode_reward=24310.18 +/- 614.70
Episode length: 20.80 +/- 4.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.8     |
|    mean_reward     | 2.43e+04 |
| time/              |          |
|    total_timesteps | 4395000  |
| train/             |          |
|    actor_loss      | -9.08    |
|    critic_loss     | 0.00662  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4384999  |
---------------------------------
Eval num_timesteps=4400000, episode_reward=24024.95 +/- 18.15
Episode length: 18.40 +/- 3.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4400000  |
| train/             |          |
|    actor_loss      | -9.03    |
|    critic_loss     | 0.0106   |
|    learning_rate   | 0.0003   |
|    n_updates       | 4389999  |
---------------------------------
Eval num_timesteps=4405000, episode_reward=24016.86 +/- 23.01
Episode length: 19.40 +/- 7.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4405000  |
| train/             |          |
|    actor_loss      | -9.03    |
|    critic_loss     | 0.0069   |
|    learning_rate   | 0.0003   |
|    n_updates       | 4394999  |
---------------------------------
Eval num_timesteps=4410000, episode_reward=24022.24 +/- 21.56
Episode length: 18.80 +/- 5.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4410000  |
| train/             |          |
|    actor_loss      | -9.02    |
|    critic_loss     | 0.00618  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4399999  |
---------------------------------
Eval num_timesteps=4415000, episode_reward=23981.84 +/- 24.11
Episode length: 24.60 +/- 2.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4415000  |
| train/             |          |
|    actor_loss      | -8.99    |
|    critic_loss     | 0.00675  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4404999  |
---------------------------------
Eval num_timesteps=4420000, episode_reward=24022.46 +/- 17.83
Episode length: 17.60 +/- 1.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4420000  |
| train/             |          |
|    actor_loss      | -8.99    |
|    critic_loss     | 0.00816  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4409999  |
---------------------------------
Eval num_timesteps=4425000, episode_reward=24010.32 +/- 12.60
Episode length: 22.00 +/- 3.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4425000  |
| train/             |          |
|    actor_loss      | -9.11    |
|    critic_loss     | 0.0116   |
|    learning_rate   | 0.0003   |
|    n_updates       | 4414999  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 128      |
|    ep_rew_mean     | 2.31e+04 |
| time/              |          |
|    episodes        | 31000    |
|    fps             | 196      |
|    time_elapsed    | 22579    |
|    total_timesteps | 4429046  |
| train/             |          |
|    actor_loss      | -8.98    |
|    critic_loss     | 0.0128   |
|    learning_rate   | 0.0003   |
|    n_updates       | 4419045  |
---------------------------------
Eval num_timesteps=4430000, episode_reward=24023.51 +/- 18.45
Episode length: 18.20 +/- 5.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4430000  |
| train/             |          |
|    actor_loss      | -9.04    |
|    critic_loss     | 0.00653  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4419999  |
---------------------------------
Eval num_timesteps=4435000, episode_reward=24014.45 +/- 19.15
Episode length: 21.20 +/- 3.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4435000  |
| train/             |          |
|    actor_loss      | -9.03    |
|    critic_loss     | 0.00782  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4424999  |
---------------------------------
Eval num_timesteps=4440000, episode_reward=24012.87 +/- 11.13
Episode length: 21.00 +/- 2.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4440000  |
| train/             |          |
|    actor_loss      | -9.03    |
|    critic_loss     | 0.00668  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4429999  |
---------------------------------
Eval num_timesteps=4445000, episode_reward=24019.51 +/- 14.82
Episode length: 21.00 +/- 2.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4445000  |
| train/             |          |
|    actor_loss      | -8.98    |
|    critic_loss     | 0.00737  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4434999  |
---------------------------------
Eval num_timesteps=4450000, episode_reward=24301.69 +/- 595.24
Episode length: 23.40 +/- 2.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | 2.43e+04 |
| time/              |          |
|    total_timesteps | 4450000  |
| train/             |          |
|    actor_loss      | -9.01    |
|    critic_loss     | 0.00568  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4439999  |
---------------------------------
Eval num_timesteps=4455000, episode_reward=24014.07 +/- 27.05
Episode length: 19.20 +/- 4.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4455000  |
| train/             |          |
|    actor_loss      | -9.03    |
|    critic_loss     | 0.00684  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4444999  |
---------------------------------
Eval num_timesteps=4460000, episode_reward=24010.73 +/- 22.21
Episode length: 20.40 +/- 3.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4460000  |
| train/             |          |
|    actor_loss      | -9.08    |
|    critic_loss     | 0.00755  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4449999  |
---------------------------------
Eval num_timesteps=4465000, episode_reward=24018.94 +/- 13.75
Episode length: 21.20 +/- 1.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4465000  |
| train/             |          |
|    actor_loss      | -9.08    |
|    critic_loss     | 0.0108   |
|    learning_rate   | 0.0003   |
|    n_updates       | 4454999  |
---------------------------------
Eval num_timesteps=4470000, episode_reward=24011.98 +/- 10.93
Episode length: 21.20 +/- 2.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4470000  |
| train/             |          |
|    actor_loss      | -9.11    |
|    critic_loss     | 0.0162   |
|    learning_rate   | 0.0003   |
|    n_updates       | 4459999  |
---------------------------------
Eval num_timesteps=4475000, episode_reward=24014.34 +/- 12.38
Episode length: 20.80 +/- 2.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4475000  |
| train/             |          |
|    actor_loss      | -9.07    |
|    critic_loss     | 0.0109   |
|    learning_rate   | 0.0003   |
|    n_updates       | 4464999  |
---------------------------------
Eval num_timesteps=4480000, episode_reward=24007.63 +/- 19.77
Episode length: 20.60 +/- 1.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4480000  |
| train/             |          |
|    actor_loss      | -9.02    |
|    critic_loss     | 0.00677  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4469999  |
---------------------------------
Eval num_timesteps=4485000, episode_reward=24012.43 +/- 8.05
Episode length: 22.00 +/- 2.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4485000  |
| train/             |          |
|    actor_loss      | -9.06    |
|    critic_loss     | 0.00664  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4474999  |
---------------------------------
Eval num_timesteps=4490000, episode_reward=24020.53 +/- 19.72
Episode length: 20.00 +/- 3.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4490000  |
| train/             |          |
|    actor_loss      | -9.06    |
|    critic_loss     | 0.0073   |
|    learning_rate   | 0.0003   |
|    n_updates       | 4479999  |
---------------------------------
Eval num_timesteps=4495000, episode_reward=24024.85 +/- 15.32
Episode length: 19.00 +/- 1.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4495000  |
| train/             |          |
|    actor_loss      | -9.09    |
|    critic_loss     | 0.00627  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4484999  |
---------------------------------
Eval num_timesteps=4500000, episode_reward=24018.25 +/- 26.11
Episode length: 18.20 +/- 5.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4500000  |
| train/             |          |
|    actor_loss      | -9.04    |
|    critic_loss     | 0.0098   |
|    learning_rate   | 0.0003   |
|    n_updates       | 4489999  |
---------------------------------
Eval num_timesteps=4505000, episode_reward=24014.48 +/- 6.55
Episode length: 20.80 +/- 2.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4505000  |
| train/             |          |
|    actor_loss      | -9.07    |
|    critic_loss     | 0.00804  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4494999  |
---------------------------------
Eval num_timesteps=4510000, episode_reward=24013.13 +/- 11.95
Episode length: 19.60 +/- 1.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4510000  |
| train/             |          |
|    actor_loss      | -9.07    |
|    critic_loss     | 0.00984  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4499999  |
---------------------------------
Eval num_timesteps=4515000, episode_reward=24012.68 +/- 13.89
Episode length: 20.60 +/- 3.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4515000  |
| train/             |          |
|    actor_loss      | -9.05    |
|    critic_loss     | 0.00618  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4504999  |
---------------------------------
Eval num_timesteps=4520000, episode_reward=24021.57 +/- 13.87
Episode length: 19.20 +/- 3.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4520000  |
| train/             |          |
|    actor_loss      | -9.01    |
|    critic_loss     | 0.00774  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4509999  |
---------------------------------
Eval num_timesteps=4525000, episode_reward=24003.58 +/- 4.40
Episode length: 23.00 +/- 1.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4525000  |
| train/             |          |
|    actor_loss      | -9.12    |
|    critic_loss     | 0.00609  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4514999  |
---------------------------------
Eval num_timesteps=4530000, episode_reward=24011.93 +/- 13.90
Episode length: 19.80 +/- 3.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4530000  |
| train/             |          |
|    actor_loss      | -9.08    |
|    critic_loss     | 0.00528  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4519999  |
---------------------------------
Eval num_timesteps=4535000, episode_reward=24018.52 +/- 25.12
Episode length: 20.40 +/- 2.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4535000  |
| train/             |          |
|    actor_loss      | -9.05    |
|    critic_loss     | 0.00892  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4524999  |
---------------------------------
Eval num_timesteps=4540000, episode_reward=24023.78 +/- 13.01
Episode length: 18.00 +/- 3.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4540000  |
| train/             |          |
|    actor_loss      | -9.07    |
|    critic_loss     | 0.00675  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4529999  |
---------------------------------
Eval num_timesteps=4545000, episode_reward=24016.43 +/- 16.17
Episode length: 18.00 +/- 5.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4545000  |
| train/             |          |
|    actor_loss      | -9.08    |
|    critic_loss     | 0.00668  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4534999  |
---------------------------------
Eval num_timesteps=4550000, episode_reward=24010.75 +/- 19.37
Episode length: 18.40 +/- 2.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4550000  |
| train/             |          |
|    actor_loss      | -9.11    |
|    critic_loss     | 0.00651  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4539999  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 131      |
|    ep_rew_mean     | 2.3e+04  |
| time/              |          |
|    episodes        | 32000    |
|    fps             | 196      |
|    time_elapsed    | 23201    |
|    total_timesteps | 4553435  |
| train/             |          |
|    actor_loss      | -9.08    |
|    critic_loss     | 0.00755  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4543434  |
---------------------------------
Eval num_timesteps=4555000, episode_reward=24004.71 +/- 19.46
Episode length: 22.40 +/- 4.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4555000  |
| train/             |          |
|    actor_loss      | -9.09    |
|    critic_loss     | 0.00662  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4544999  |
---------------------------------
Eval num_timesteps=4560000, episode_reward=24016.36 +/- 5.98
Episode length: 20.00 +/- 1.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4560000  |
| train/             |          |
|    actor_loss      | -9.04    |
|    critic_loss     | 0.00802  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4549999  |
---------------------------------
Eval num_timesteps=4565000, episode_reward=24015.39 +/- 12.59
Episode length: 19.60 +/- 3.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4565000  |
| train/             |          |
|    actor_loss      | -9.02    |
|    critic_loss     | 0.00939  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4554999  |
---------------------------------
Eval num_timesteps=4570000, episode_reward=24002.31 +/- 6.25
Episode length: 23.60 +/- 1.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4570000  |
| train/             |          |
|    actor_loss      | -9.06    |
|    critic_loss     | 0.00636  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4559999  |
---------------------------------
Eval num_timesteps=4575000, episode_reward=24004.70 +/- 12.61
Episode length: 21.80 +/- 3.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4575000  |
| train/             |          |
|    actor_loss      | -9.07    |
|    critic_loss     | 0.00791  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4564999  |
---------------------------------
Eval num_timesteps=4580000, episode_reward=24005.77 +/- 10.88
Episode length: 22.60 +/- 2.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4580000  |
| train/             |          |
|    actor_loss      | -9.1     |
|    critic_loss     | 0.00816  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4569999  |
---------------------------------
Eval num_timesteps=4585000, episode_reward=24016.02 +/- 11.49
Episode length: 20.60 +/- 3.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4585000  |
| train/             |          |
|    actor_loss      | -9.05    |
|    critic_loss     | 0.00938  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4574999  |
---------------------------------
Eval num_timesteps=4590000, episode_reward=24005.99 +/- 11.67
Episode length: 23.20 +/- 2.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4590000  |
| train/             |          |
|    actor_loss      | -9.06    |
|    critic_loss     | 0.00996  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4579999  |
---------------------------------
Eval num_timesteps=4595000, episode_reward=24022.64 +/- 20.53
Episode length: 17.80 +/- 3.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4595000  |
| train/             |          |
|    actor_loss      | -9.01    |
|    critic_loss     | 0.00879  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4584999  |
---------------------------------
Eval num_timesteps=4600000, episode_reward=24001.66 +/- 2.71
Episode length: 23.00 +/- 0.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4600000  |
| train/             |          |
|    actor_loss      | -9.07    |
|    critic_loss     | 0.00925  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4589999  |
---------------------------------
Eval num_timesteps=4605000, episode_reward=24020.36 +/- 15.49
Episode length: 20.20 +/- 3.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4605000  |
| train/             |          |
|    actor_loss      | -9.13    |
|    critic_loss     | 0.0228   |
|    learning_rate   | 0.0003   |
|    n_updates       | 4594999  |
---------------------------------
Eval num_timesteps=4610000, episode_reward=23999.00 +/- 12.86
Episode length: 22.60 +/- 3.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4610000  |
| train/             |          |
|    actor_loss      | -9.07    |
|    critic_loss     | 0.0112   |
|    learning_rate   | 0.0003   |
|    n_updates       | 4599999  |
---------------------------------
Eval num_timesteps=4615000, episode_reward=24013.76 +/- 16.67
Episode length: 20.80 +/- 3.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4615000  |
| train/             |          |
|    actor_loss      | -9.04    |
|    critic_loss     | 0.00736  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4604999  |
---------------------------------
Eval num_timesteps=4620000, episode_reward=24025.10 +/- 16.58
Episode length: 19.00 +/- 3.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4620000  |
| train/             |          |
|    actor_loss      | -9.11    |
|    critic_loss     | 0.00818  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4609999  |
---------------------------------
Eval num_timesteps=4625000, episode_reward=24016.94 +/- 14.72
Episode length: 19.40 +/- 4.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4625000  |
| train/             |          |
|    actor_loss      | -9.07    |
|    critic_loss     | 0.00755  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4614999  |
---------------------------------
Eval num_timesteps=4630000, episode_reward=24018.27 +/- 5.72
Episode length: 19.00 +/- 2.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4630000  |
| train/             |          |
|    actor_loss      | -9.03    |
|    critic_loss     | 0.00738  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4619999  |
---------------------------------
Eval num_timesteps=4635000, episode_reward=24028.51 +/- 20.50
Episode length: 17.40 +/- 6.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4635000  |
| train/             |          |
|    actor_loss      | -9.11    |
|    critic_loss     | 0.00993  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4624999  |
---------------------------------
Eval num_timesteps=4640000, episode_reward=24310.67 +/- 584.75
Episode length: 21.00 +/- 3.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21       |
|    mean_reward     | 2.43e+04 |
| time/              |          |
|    total_timesteps | 4640000  |
| train/             |          |
|    actor_loss      | -9.03    |
|    critic_loss     | 0.00829  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4629999  |
---------------------------------
Eval num_timesteps=4645000, episode_reward=24003.09 +/- 10.53
Episode length: 23.00 +/- 1.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4645000  |
| train/             |          |
|    actor_loss      | -9.08    |
|    critic_loss     | 0.00617  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4634999  |
---------------------------------
Eval num_timesteps=4650000, episode_reward=24023.09 +/- 25.41
Episode length: 18.20 +/- 5.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4650000  |
| train/             |          |
|    actor_loss      | -9.1     |
|    critic_loss     | 0.00768  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4639999  |
---------------------------------
Eval num_timesteps=4655000, episode_reward=24028.71 +/- 17.18
Episode length: 16.00 +/- 3.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4655000  |
| train/             |          |
|    actor_loss      | -8.98    |
|    critic_loss     | 0.00569  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4644999  |
---------------------------------
Eval num_timesteps=4660000, episode_reward=24020.16 +/- 22.48
Episode length: 18.80 +/- 4.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4660000  |
| train/             |          |
|    actor_loss      | -9.06    |
|    critic_loss     | 0.00662  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4649999  |
---------------------------------
Eval num_timesteps=4665000, episode_reward=24004.67 +/- 6.15
Episode length: 21.40 +/- 2.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4665000  |
| train/             |          |
|    actor_loss      | -9.07    |
|    critic_loss     | 0.00719  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4654999  |
---------------------------------
Eval num_timesteps=4670000, episode_reward=24018.84 +/- 24.06
Episode length: 18.60 +/- 5.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4670000  |
| train/             |          |
|    actor_loss      | -9.13    |
|    critic_loss     | 0.0202   |
|    learning_rate   | 0.0003   |
|    n_updates       | 4659999  |
---------------------------------
Eval num_timesteps=4675000, episode_reward=24016.41 +/- 7.62
Episode length: 20.40 +/- 1.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4675000  |
| train/             |          |
|    actor_loss      | -9.07    |
|    critic_loss     | 0.00598  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4664999  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 130      |
|    ep_rew_mean     | 2.24e+04 |
| time/              |          |
|    episodes        | 33000    |
|    fps             | 196      |
|    time_elapsed    | 23827    |
|    total_timesteps | 4679088  |
| train/             |          |
|    actor_loss      | -9.13    |
|    critic_loss     | 0.00927  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4669087  |
---------------------------------
Eval num_timesteps=4680000, episode_reward=24007.44 +/- 19.11
Episode length: 20.60 +/- 3.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4680000  |
| train/             |          |
|    actor_loss      | -9.04    |
|    critic_loss     | 0.00805  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4669999  |
---------------------------------
Eval num_timesteps=4685000, episode_reward=24001.95 +/- 11.33
Episode length: 22.80 +/- 1.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4685000  |
| train/             |          |
|    actor_loss      | -9.03    |
|    critic_loss     | 0.00596  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4674999  |
---------------------------------
Eval num_timesteps=4690000, episode_reward=24013.63 +/- 13.80
Episode length: 18.80 +/- 2.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4690000  |
| train/             |          |
|    actor_loss      | -9.07    |
|    critic_loss     | 0.00761  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4679999  |
---------------------------------
Eval num_timesteps=4695000, episode_reward=24014.22 +/- 18.69
Episode length: 18.60 +/- 4.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4695000  |
| train/             |          |
|    actor_loss      | -9.05    |
|    critic_loss     | 0.0099   |
|    learning_rate   | 0.0003   |
|    n_updates       | 4684999  |
---------------------------------
Eval num_timesteps=4700000, episode_reward=24019.37 +/- 9.47
Episode length: 18.80 +/- 2.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4700000  |
| train/             |          |
|    actor_loss      | -9.08    |
|    critic_loss     | 0.00921  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4689999  |
---------------------------------
Eval num_timesteps=4705000, episode_reward=24010.65 +/- 11.05
Episode length: 19.00 +/- 3.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4705000  |
| train/             |          |
|    actor_loss      | -9.03    |
|    critic_loss     | 0.00989  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4694999  |
---------------------------------
Eval num_timesteps=4710000, episode_reward=24005.18 +/- 8.83
Episode length: 21.40 +/- 2.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4710000  |
| train/             |          |
|    actor_loss      | -9.09    |
|    critic_loss     | 0.0168   |
|    learning_rate   | 0.0003   |
|    n_updates       | 4699999  |
---------------------------------
Eval num_timesteps=4715000, episode_reward=24000.41 +/- 4.45
Episode length: 22.20 +/- 0.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4715000  |
| train/             |          |
|    actor_loss      | -9.1     |
|    critic_loss     | 0.0147   |
|    learning_rate   | 0.0003   |
|    n_updates       | 4704999  |
---------------------------------
Eval num_timesteps=4720000, episode_reward=24011.10 +/- 8.73
Episode length: 19.80 +/- 2.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4720000  |
| train/             |          |
|    actor_loss      | -8.99    |
|    critic_loss     | 0.00752  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4709999  |
---------------------------------
Eval num_timesteps=4725000, episode_reward=24007.85 +/- 18.22
Episode length: 21.00 +/- 4.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4725000  |
| train/             |          |
|    actor_loss      | -9.01    |
|    critic_loss     | 0.00544  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4714999  |
---------------------------------
Eval num_timesteps=4730000, episode_reward=24307.43 +/- 611.15
Episode length: 21.20 +/- 2.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.2     |
|    mean_reward     | 2.43e+04 |
| time/              |          |
|    total_timesteps | 4730000  |
| train/             |          |
|    actor_loss      | -9.05    |
|    critic_loss     | 0.00702  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4719999  |
---------------------------------
Eval num_timesteps=4735000, episode_reward=24027.31 +/- 20.47
Episode length: 15.60 +/- 4.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4735000  |
| train/             |          |
|    actor_loss      | -9.05    |
|    critic_loss     | 0.00601  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4724999  |
---------------------------------
Eval num_timesteps=4740000, episode_reward=24016.37 +/- 5.68
Episode length: 19.20 +/- 1.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4740000  |
| train/             |          |
|    actor_loss      | -9.04    |
|    critic_loss     | 0.0203   |
|    learning_rate   | 0.0003   |
|    n_updates       | 4729999  |
---------------------------------
Eval num_timesteps=4745000, episode_reward=24301.77 +/- 586.98
Episode length: 22.60 +/- 1.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | 2.43e+04 |
| time/              |          |
|    total_timesteps | 4745000  |
| train/             |          |
|    actor_loss      | -9.02    |
|    critic_loss     | 0.00746  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4734999  |
---------------------------------
Eval num_timesteps=4750000, episode_reward=24010.05 +/- 2.82
Episode length: 21.40 +/- 1.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4750000  |
| train/             |          |
|    actor_loss      | -9.05    |
|    critic_loss     | 0.00733  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4739999  |
---------------------------------
Eval num_timesteps=4755000, episode_reward=24015.95 +/- 27.42
Episode length: 19.60 +/- 5.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4755000  |
| train/             |          |
|    actor_loss      | -9.05    |
|    critic_loss     | 0.0126   |
|    learning_rate   | 0.0003   |
|    n_updates       | 4744999  |
---------------------------------
Eval num_timesteps=4760000, episode_reward=24015.89 +/- 18.74
Episode length: 18.00 +/- 3.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4760000  |
| train/             |          |
|    actor_loss      | -9.04    |
|    critic_loss     | 0.00754  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4749999  |
---------------------------------
Eval num_timesteps=4765000, episode_reward=24006.75 +/- 11.32
Episode length: 22.20 +/- 1.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4765000  |
| train/             |          |
|    actor_loss      | -9.07    |
|    critic_loss     | 0.00678  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4754999  |
---------------------------------
Eval num_timesteps=4770000, episode_reward=24005.30 +/- 22.83
Episode length: 20.60 +/- 4.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4770000  |
| train/             |          |
|    actor_loss      | -9.03    |
|    critic_loss     | 0.0138   |
|    learning_rate   | 0.0003   |
|    n_updates       | 4759999  |
---------------------------------
Eval num_timesteps=4775000, episode_reward=24321.65 +/- 584.56
Episode length: 18.40 +/- 7.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.4     |
|    mean_reward     | 2.43e+04 |
| time/              |          |
|    total_timesteps | 4775000  |
| train/             |          |
|    actor_loss      | -9.1     |
|    critic_loss     | 0.00819  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4764999  |
---------------------------------
Eval num_timesteps=4780000, episode_reward=24294.63 +/- 593.16
Episode length: 23.40 +/- 2.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | 2.43e+04 |
| time/              |          |
|    total_timesteps | 4780000  |
| train/             |          |
|    actor_loss      | -9.04    |
|    critic_loss     | 0.00772  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4769999  |
---------------------------------
Eval num_timesteps=4785000, episode_reward=24019.13 +/- 19.67
Episode length: 19.20 +/- 3.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4785000  |
| train/             |          |
|    actor_loss      | -9.04    |
|    critic_loss     | 0.00815  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4774999  |
---------------------------------
Eval num_timesteps=4790000, episode_reward=24009.43 +/- 9.06
Episode length: 21.80 +/- 1.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4790000  |
| train/             |          |
|    actor_loss      | -9.07    |
|    critic_loss     | 0.0119   |
|    learning_rate   | 0.0003   |
|    n_updates       | 4779999  |
---------------------------------
Eval num_timesteps=4795000, episode_reward=24009.04 +/- 16.13
Episode length: 19.80 +/- 3.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4795000  |
| train/             |          |
|    actor_loss      | -9.03    |
|    critic_loss     | 0.00874  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4784999  |
---------------------------------
Eval num_timesteps=4800000, episode_reward=24018.74 +/- 19.19
Episode length: 17.80 +/- 4.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4800000  |
| train/             |          |
|    actor_loss      | -9.02    |
|    critic_loss     | 0.00637  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4789999  |
---------------------------------
Eval num_timesteps=4805000, episode_reward=24010.31 +/- 17.15
Episode length: 21.00 +/- 3.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4805000  |
| train/             |          |
|    actor_loss      | -9.06    |
|    critic_loss     | 0.00793  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4794999  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 113      |
|    ep_rew_mean     | 2.33e+04 |
| time/              |          |
|    episodes        | 34000    |
|    fps             | 196      |
|    time_elapsed    | 24455    |
|    total_timesteps | 4806884  |
| train/             |          |
|    actor_loss      | -9.03    |
|    critic_loss     | 0.00662  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4796883  |
---------------------------------
Eval num_timesteps=4810000, episode_reward=24016.70 +/- 14.71
Episode length: 20.60 +/- 2.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4810000  |
| train/             |          |
|    actor_loss      | -8.97    |
|    critic_loss     | 0.025    |
|    learning_rate   | 0.0003   |
|    n_updates       | 4799999  |
---------------------------------
Eval num_timesteps=4815000, episode_reward=24307.18 +/- 597.09
Episode length: 22.00 +/- 2.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22       |
|    mean_reward     | 2.43e+04 |
| time/              |          |
|    total_timesteps | 4815000  |
| train/             |          |
|    actor_loss      | -9.1     |
|    critic_loss     | 0.0071   |
|    learning_rate   | 0.0003   |
|    n_updates       | 4804999  |
---------------------------------
Eval num_timesteps=4820000, episode_reward=24018.60 +/- 11.40
Episode length: 19.20 +/- 1.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4820000  |
| train/             |          |
|    actor_loss      | -9.01    |
|    critic_loss     | 0.00699  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4809999  |
---------------------------------
Eval num_timesteps=4825000, episode_reward=24311.41 +/- 604.92
Episode length: 20.20 +/- 2.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.2     |
|    mean_reward     | 2.43e+04 |
| time/              |          |
|    total_timesteps | 4825000  |
| train/             |          |
|    actor_loss      | -9.06    |
|    critic_loss     | 0.00693  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4814999  |
---------------------------------
Eval num_timesteps=4830000, episode_reward=24007.27 +/- 13.33
Episode length: 21.80 +/- 2.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4830000  |
| train/             |          |
|    actor_loss      | -9.06    |
|    critic_loss     | 0.00914  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4819999  |
---------------------------------
Eval num_timesteps=4835000, episode_reward=24014.83 +/- 16.52
Episode length: 20.80 +/- 3.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4835000  |
| train/             |          |
|    actor_loss      | -9.09    |
|    critic_loss     | 0.00871  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4824999  |
---------------------------------
Eval num_timesteps=4840000, episode_reward=24033.46 +/- 18.23
Episode length: 15.80 +/- 3.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4840000  |
| train/             |          |
|    actor_loss      | -9.04    |
|    critic_loss     | 0.00797  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4829999  |
---------------------------------
Eval num_timesteps=4845000, episode_reward=24016.47 +/- 16.48
Episode length: 21.20 +/- 3.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4845000  |
| train/             |          |
|    actor_loss      | -9.04    |
|    critic_loss     | 0.00696  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4834999  |
---------------------------------
Eval num_timesteps=4850000, episode_reward=24028.03 +/- 15.93
Episode length: 19.20 +/- 3.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4850000  |
| train/             |          |
|    actor_loss      | -9.06    |
|    critic_loss     | 0.00963  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4839999  |
---------------------------------
Eval num_timesteps=4855000, episode_reward=24013.26 +/- 9.37
Episode length: 19.40 +/- 2.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4855000  |
| train/             |          |
|    actor_loss      | -9.05    |
|    critic_loss     | 0.0135   |
|    learning_rate   | 0.0003   |
|    n_updates       | 4844999  |
---------------------------------
Eval num_timesteps=4860000, episode_reward=24008.58 +/- 13.43
Episode length: 20.60 +/- 2.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4860000  |
| train/             |          |
|    actor_loss      | -8.98    |
|    critic_loss     | 0.00665  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4849999  |
---------------------------------
Eval num_timesteps=4865000, episode_reward=24008.91 +/- 6.31
Episode length: 21.20 +/- 1.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4865000  |
| train/             |          |
|    actor_loss      | -9.04    |
|    critic_loss     | 0.00711  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4854999  |
---------------------------------
Eval num_timesteps=4870000, episode_reward=24013.60 +/- 7.68
Episode length: 20.00 +/- 1.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4870000  |
| train/             |          |
|    actor_loss      | -9.08    |
|    critic_loss     | 0.00608  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4859999  |
---------------------------------
Eval num_timesteps=4875000, episode_reward=24016.88 +/- 21.27
Episode length: 18.20 +/- 2.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4875000  |
| train/             |          |
|    actor_loss      | -9.03    |
|    critic_loss     | 0.0078   |
|    learning_rate   | 0.0003   |
|    n_updates       | 4864999  |
---------------------------------
Eval num_timesteps=4880000, episode_reward=24018.87 +/- 15.77
Episode length: 16.00 +/- 3.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4880000  |
| train/             |          |
|    actor_loss      | -9.08    |
|    critic_loss     | 0.00697  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4869999  |
---------------------------------
Eval num_timesteps=4885000, episode_reward=24019.83 +/- 22.20
Episode length: 17.20 +/- 4.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4885000  |
| train/             |          |
|    actor_loss      | -9.06    |
|    critic_loss     | 0.00698  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4874999  |
---------------------------------
Eval num_timesteps=4890000, episode_reward=24018.74 +/- 25.76
Episode length: 18.00 +/- 6.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4890000  |
| train/             |          |
|    actor_loss      | -9.08    |
|    critic_loss     | 0.00495  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4879999  |
---------------------------------
Eval num_timesteps=4895000, episode_reward=24008.82 +/- 9.19
Episode length: 21.00 +/- 0.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4895000  |
| train/             |          |
|    actor_loss      | -9.09    |
|    critic_loss     | 0.00631  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4884999  |
---------------------------------
Eval num_timesteps=4900000, episode_reward=24030.85 +/- 31.29
Episode length: 15.40 +/- 6.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4900000  |
| train/             |          |
|    actor_loss      | -9.02    |
|    critic_loss     | 0.00829  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4889999  |
---------------------------------
Eval num_timesteps=4905000, episode_reward=24002.65 +/- 21.35
Episode length: 20.40 +/- 4.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4905000  |
| train/             |          |
|    actor_loss      | -9.05    |
|    critic_loss     | 0.00988  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4894999  |
---------------------------------
Eval num_timesteps=4910000, episode_reward=24308.76 +/- 590.68
Episode length: 20.40 +/- 2.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.4     |
|    mean_reward     | 2.43e+04 |
| time/              |          |
|    total_timesteps | 4910000  |
| train/             |          |
|    actor_loss      | -9.06    |
|    critic_loss     | 0.0143   |
|    learning_rate   | 0.0003   |
|    n_updates       | 4899999  |
---------------------------------
Eval num_timesteps=4915000, episode_reward=24021.33 +/- 22.62
Episode length: 17.60 +/- 4.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4915000  |
| train/             |          |
|    actor_loss      | -9.02    |
|    critic_loss     | 0.0104   |
|    learning_rate   | 0.0003   |
|    n_updates       | 4904999  |
---------------------------------
Eval num_timesteps=4920000, episode_reward=24014.58 +/- 19.77
Episode length: 19.00 +/- 3.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4920000  |
| train/             |          |
|    actor_loss      | -9.12    |
|    critic_loss     | 0.00864  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4909999  |
---------------------------------
Eval num_timesteps=4925000, episode_reward=24003.20 +/- 7.65
Episode length: 20.80 +/- 1.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4925000  |
| train/             |          |
|    actor_loss      | -9.05    |
|    critic_loss     | 0.0102   |
|    learning_rate   | 0.0003   |
|    n_updates       | 4914999  |
---------------------------------
Eval num_timesteps=4930000, episode_reward=24003.32 +/- 15.98
Episode length: 20.80 +/- 4.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.8     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4930000  |
| train/             |          |
|    actor_loss      | -9.07    |
|    critic_loss     | 0.00673  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4919999  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 131      |
|    ep_rew_mean     | 2.23e+04 |
| time/              |          |
|    episodes        | 35000    |
|    fps             | 196      |
|    time_elapsed    | 25068    |
|    total_timesteps | 4932681  |
| train/             |          |
|    actor_loss      | -9.04    |
|    critic_loss     | 0.0115   |
|    learning_rate   | 0.0003   |
|    n_updates       | 4922680  |
---------------------------------
Eval num_timesteps=4935000, episode_reward=24322.94 +/- 608.75
Episode length: 18.40 +/- 2.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.4     |
|    mean_reward     | 2.43e+04 |
| time/              |          |
|    total_timesteps | 4935000  |
| train/             |          |
|    actor_loss      | -8.99    |
|    critic_loss     | 0.00953  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4924999  |
---------------------------------
Eval num_timesteps=4940000, episode_reward=24019.26 +/- 21.84
Episode length: 21.20 +/- 3.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4940000  |
| train/             |          |
|    actor_loss      | -9.08    |
|    critic_loss     | 0.00787  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4929999  |
---------------------------------
Eval num_timesteps=4945000, episode_reward=24010.55 +/- 19.98
Episode length: 22.20 +/- 5.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.2     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4945000  |
| train/             |          |
|    actor_loss      | -9.01    |
|    critic_loss     | 0.00822  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4934999  |
---------------------------------
Eval num_timesteps=4950000, episode_reward=24003.94 +/- 15.75
Episode length: 21.60 +/- 1.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4950000  |
| train/             |          |
|    actor_loss      | -9.02    |
|    critic_loss     | 0.00931  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4939999  |
---------------------------------
Eval num_timesteps=4955000, episode_reward=24008.57 +/- 22.67
Episode length: 19.00 +/- 4.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4955000  |
| train/             |          |
|    actor_loss      | -9.04    |
|    critic_loss     | 0.006    |
|    learning_rate   | 0.0003   |
|    n_updates       | 4944999  |
---------------------------------
Eval num_timesteps=4960000, episode_reward=24018.70 +/- 21.77
Episode length: 20.40 +/- 2.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.4     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4960000  |
| train/             |          |
|    actor_loss      | -9.04    |
|    critic_loss     | 0.00823  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4949999  |
---------------------------------
Eval num_timesteps=4965000, episode_reward=24013.20 +/- 16.64
Episode length: 19.60 +/- 2.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4965000  |
| train/             |          |
|    actor_loss      | -9.03    |
|    critic_loss     | 0.0144   |
|    learning_rate   | 0.0003   |
|    n_updates       | 4954999  |
---------------------------------
Eval num_timesteps=4970000, episode_reward=23993.28 +/- 19.70
Episode length: 21.60 +/- 3.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4970000  |
| train/             |          |
|    actor_loss      | -9.01    |
|    critic_loss     | 0.0079   |
|    learning_rate   | 0.0003   |
|    n_updates       | 4959999  |
---------------------------------
Eval num_timesteps=4975000, episode_reward=24008.35 +/- 31.78
Episode length: 22.00 +/- 3.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4975000  |
| train/             |          |
|    actor_loss      | -9.07    |
|    critic_loss     | 0.006    |
|    learning_rate   | 0.0003   |
|    n_updates       | 4964999  |
---------------------------------
Eval num_timesteps=4980000, episode_reward=24313.50 +/- 591.06
Episode length: 20.20 +/- 2.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.2     |
|    mean_reward     | 2.43e+04 |
| time/              |          |
|    total_timesteps | 4980000  |
| train/             |          |
|    actor_loss      | -9.02    |
|    critic_loss     | 0.00911  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4969999  |
---------------------------------
Eval num_timesteps=4985000, episode_reward=24023.37 +/- 22.12
Episode length: 18.60 +/- 4.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4985000  |
| train/             |          |
|    actor_loss      | -9.04    |
|    critic_loss     | 0.019    |
|    learning_rate   | 0.0003   |
|    n_updates       | 4974999  |
---------------------------------
Eval num_timesteps=4990000, episode_reward=24014.82 +/- 16.72
Episode length: 19.60 +/- 4.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4990000  |
| train/             |          |
|    actor_loss      | -9.06    |
|    critic_loss     | 0.00535  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4979999  |
---------------------------------
Eval num_timesteps=4995000, episode_reward=24030.38 +/- 24.48
Episode length: 15.60 +/- 4.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 4995000  |
| train/             |          |
|    actor_loss      | -9       |
|    critic_loss     | 0.00739  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4984999  |
---------------------------------
Eval num_timesteps=5000000, episode_reward=23997.63 +/- 16.63
Episode length: 21.00 +/- 2.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21       |
|    mean_reward     | 2.4e+04  |
| time/              |          |
|    total_timesteps | 5000000  |
| train/             |          |
|    actor_loss      | -9.02    |
|    critic_loss     | 0.00882  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4989999  |
---------------------------------
训练完成，耗时: 25404.29秒
