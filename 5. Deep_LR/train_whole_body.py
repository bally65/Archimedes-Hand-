import gymnasium as gym
import numpy as np
from stable_baselines3 import TD3
from stable_baselines3.common.noise import NormalActionNoise
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.vec_env import VecNormalize
import os
from whole_body_env import WholeBodyEnv

def train_whole_body_v3():
    print("ğŸš€ [Version 3.0] å•Ÿå‹•å…·å‚™åœ°å½¢é æ¸¬èƒ½åŠ›çš„å…¨ç³»çµ±å”åŒè¨“ç·´...")
    
    # å‰µå»ºç’°å¢ƒ
    env = make_vec_env(lambda: WholeBodyEnv(), n_envs=1)
    env = VecNormalize(env, norm_obs=True, norm_reward=True)
    
    n_actions = env.action_space.shape[-1]
    # ä½¿ç”¨æ›´æœ‰å±¤æ¬¡çš„å‹•ä½œå™ªè²ï¼Œä¿ƒé€²æ¢ç´¢
    action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.15 * np.ones(n_actions))
    
    # æå‡æ¨¡å‹è¦æ¨¡ä»¥è™•ç†é«˜åº¦åœ–æ•¸æ“š
    model = TD3(
        "MlpPolicy",
        env,
        action_noise=action_noise,
        verbose=1,
        device="auto",
        learning_rate=1e-4, # Lower LR for better convergence with complex input
        batch_size=128,
        buffer_size=1000000,
        learning_starts=2000,
        policy_kwargs=dict(net_arch=dict(pi=[512, 512, 256], qf=[512, 512, 256]))
    )
    
    os.makedirs("./models/whole_body_v3", exist_ok=True)
    
    print("â³ æ­£åœ¨é€²è¡Œ 1,500,000 æ­¥çš„æ·±åº¦å¼·åŒ–å­¸ç¿’è¨“ç·´...")
    model.learn(total_timesteps=1500000, log_interval=100)
    
    model.save("./models/whole_body_v3/td3_terrain_aware_final")
    env.save("./models/whole_body_v3/vec_normalize_v3.pkl")
    print("âœ… åœ°å½¢é æ¸¬æ¨¡å‹è¨“ç·´å®Œæˆä¸¦å­˜æª”ï¼")

if __name__ == "__main__":
    train_whole_body_v3()
